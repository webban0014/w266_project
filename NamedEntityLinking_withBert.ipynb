{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Linking with BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using BERT embeddings to aid in the task of End-to-End Named Entity Linking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 - Read in the annotations and training dataset from AIDA-YAGO2 as well as the Wiki Entity Name/Id dataset\n",
    "\n",
    "These tab-seperated files will be used to train our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_file = 'C:\\\\Users\\\\andre\\\\Documents\\\\Berkeley\\\\w266\\\\w266_Project\\\\data\\\\AIDA-YAGO2-annotations.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train mentions:  25288\n",
      "validation mentions:  6349\n",
      "test mentions:  6078\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "\n",
    "train_annotations = []\n",
    "val_annotations = []\n",
    "test_annotations = []\n",
    "\n",
    "with open(annotations_file, encoding='utf-8') as tsvfile:\n",
    "    reader = csv.reader(tsvfile, delimiter='\\t')\n",
    "    i = 0\n",
    "    for row in reader:\n",
    "        i += 1\n",
    "        train_annotations.append(row)\n",
    "        if row == ['-DOCSTART- (947testa CRICKET)']: # first validation document\n",
    "            #print('validation data starts at index:', i-1)\n",
    "            val_index = i-1\n",
    "        if row == ['-DOCSTART- (1163testb SOCCER)']: # first test document\n",
    "            #print('test data starts at index:', i-1)\n",
    "            test_index = i-1\n",
    "    \n",
    "    val_annotations = train_annotations[val_index: test_index]\n",
    "    test_annotations = train_annotations[test_index:]\n",
    "    train_annotations = train_annotations[:val_index]\n",
    "    \n",
    "print('train mentions: ', len(train_annotations))\n",
    "print('validation mentions: ', len(val_annotations))\n",
    "print('test mentions: ', len(test_annotations))            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = 'C:\\\\Users\\\\andre\\\\Documents\\\\Berkeley\\\\w266\\\\w266_Project\\\\data\\\\basic_data\\\\test_datasets\\\\AIDA\\\\aida_train_unquoted.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['-DOCSTART- (1 EU)'],\n",
       " ['EU', 'B', 'EU', '--NME--'],\n",
       " ['rejects'],\n",
       " ['German',\n",
       "  'B',\n",
       "  'German',\n",
       "  'Germany',\n",
       "  'http://en.wikipedia.org/wiki/Germany',\n",
       "  '11867',\n",
       "  '/m/0345h'],\n",
       " ['call'],\n",
       " ['to'],\n",
       " ['boycott'],\n",
       " ['British',\n",
       "  'B',\n",
       "  'British',\n",
       "  'United_Kingdom',\n",
       "  'http://en.wikipedia.org/wiki/United_Kingdom',\n",
       "  '31717',\n",
       "  '/m/07ssc'],\n",
       " ['lamb'],\n",
       " ['.'],\n",
       " [],\n",
       " ['Peter', 'B', 'Peter Blackburn', '--NME--'],\n",
       " ['Blackburn', 'I', 'Peter Blackburn', '--NME--'],\n",
       " [],\n",
       " ['BRUSSELS',\n",
       "  'B',\n",
       "  'BRUSSELS',\n",
       "  'Brussels',\n",
       "  'http://en.wikipedia.org/wiki/Brussels',\n",
       "  '3708',\n",
       "  '/m/0177z'],\n",
       " ['1996-08-22'],\n",
       " [],\n",
       " ['The'],\n",
       " ['European',\n",
       "  'B',\n",
       "  'European Commission',\n",
       "  'European_Commission',\n",
       "  'http://en.wikipedia.org/wiki/European_Commission',\n",
       "  '9974',\n",
       "  '/m/02q9k'],\n",
       " ['Commission',\n",
       "  'I',\n",
       "  'European Commission',\n",
       "  'European_Commission',\n",
       "  'http://en.wikipedia.org/wiki/European_Commission',\n",
       "  '9974',\n",
       "  '/m/02q9k'],\n",
       " ['said'],\n",
       " ['on'],\n",
       " ['Thursday'],\n",
       " ['it'],\n",
       " ['disagreed'],\n",
       " ['with'],\n",
       " ['German',\n",
       "  'B',\n",
       "  'German',\n",
       "  'Germany',\n",
       "  'http://en.wikipedia.org/wiki/Germany',\n",
       "  '11867',\n",
       "  '/m/0345h'],\n",
       " ['advice'],\n",
       " ['to'],\n",
       " ['consumers'],\n",
       " ['to'],\n",
       " ['shun'],\n",
       " ['British',\n",
       "  'B',\n",
       "  'British',\n",
       "  'United_Kingdom',\n",
       "  'http://en.wikipedia.org/wiki/United_Kingdom',\n",
       "  '31717',\n",
       "  '/m/07ssc'],\n",
       " ['lamb'],\n",
       " ['until'],\n",
       " ['scientists'],\n",
       " ['determine'],\n",
       " ['whether'],\n",
       " ['mad'],\n",
       " ['cow'],\n",
       " ['disease'],\n",
       " ['can'],\n",
       " ['be'],\n",
       " ['transmitted'],\n",
       " ['to'],\n",
       " ['sheep'],\n",
       " ['.'],\n",
       " [],\n",
       " ['Germany',\n",
       "  'B',\n",
       "  'Germany',\n",
       "  'Germany',\n",
       "  'http://en.wikipedia.org/wiki/Germany',\n",
       "  '11867',\n",
       "  '/m/0345h'],\n",
       " [\"'s\"],\n",
       " ['representative'],\n",
       " ['to'],\n",
       " ['the'],\n",
       " ['European',\n",
       "  'B',\n",
       "  'European Union',\n",
       "  'European_Union',\n",
       "  'http://en.wikipedia.org/wiki/European_Union',\n",
       "  '9317',\n",
       "  '/m/02jxk'],\n",
       " ['Union',\n",
       "  'I',\n",
       "  'European Union',\n",
       "  'European_Union',\n",
       "  'http://en.wikipedia.org/wiki/European_Union',\n",
       "  '9317',\n",
       "  '/m/02jxk'],\n",
       " [\"'s\"],\n",
       " ['veterinary'],\n",
       " ['committee'],\n",
       " ['Werner', 'B', 'Werner Zwingmann', '--NME--'],\n",
       " ['Zwingmann', 'I', 'Werner Zwingmann', '--NME--'],\n",
       " ['said'],\n",
       " ['on'],\n",
       " ['Wednesday'],\n",
       " ['consumers'],\n",
       " ['should'],\n",
       " ['buy'],\n",
       " ['sheepmeat'],\n",
       " ['from'],\n",
       " ['countries'],\n",
       " ['other'],\n",
       " ['than'],\n",
       " ['Britain',\n",
       "  'B',\n",
       "  'Britain',\n",
       "  'United_Kingdom',\n",
       "  'http://en.wikipedia.org/wiki/United_Kingdom',\n",
       "  '31717',\n",
       "  '/m/07ssc'],\n",
       " ['until'],\n",
       " ['the'],\n",
       " ['scientific'],\n",
       " ['advice'],\n",
       " ['was'],\n",
       " ['clearer'],\n",
       " ['.'],\n",
       " [],\n",
       " [],\n",
       " ['We'],\n",
       " ['do'],\n",
       " [\"n't\"],\n",
       " ['support'],\n",
       " ['any'],\n",
       " ['such'],\n",
       " ['recommendation'],\n",
       " ['because'],\n",
       " ['we'],\n",
       " ['do'],\n",
       " [\"n't\"],\n",
       " ['see'],\n",
       " ['any'],\n",
       " ['grounds'],\n",
       " ['for'],\n",
       " ['it'],\n",
       " [','],\n",
       " [],\n",
       " ['the'],\n",
       " ['Commission',\n",
       "  'B',\n",
       "  'Commission',\n",
       "  'European_Commission',\n",
       "  'http://en.wikipedia.org/wiki/European_Commission',\n",
       "  '9974',\n",
       "  '/m/02q9k'],\n",
       " [\"'s\"],\n",
       " ['chief'],\n",
       " ['spokesman'],\n",
       " ['Nikolaus', 'B', 'Nikolaus van der Pas', '--NME--'],\n",
       " ['van', 'I', 'Nikolaus van der Pas', '--NME--'],\n",
       " ['der', 'I', 'Nikolaus van der Pas', '--NME--'],\n",
       " ['Pas', 'I', 'Nikolaus van der Pas', '--NME--'],\n",
       " ['told'],\n",
       " ['a'],\n",
       " ['news'],\n",
       " ['briefing'],\n",
       " ['.'],\n",
       " [],\n",
       " ['He'],\n",
       " ['said'],\n",
       " ['further'],\n",
       " ['scientific'],\n",
       " ['study'],\n",
       " ['was'],\n",
       " ['required'],\n",
       " ['and'],\n",
       " ['if'],\n",
       " ['it'],\n",
       " ['was'],\n",
       " ['found'],\n",
       " ['that'],\n",
       " ['action'],\n",
       " ['was'],\n",
       " ['needed'],\n",
       " ['it'],\n",
       " ['should'],\n",
       " ['be'],\n",
       " ['taken'],\n",
       " ['by'],\n",
       " ['the'],\n",
       " ['European',\n",
       "  'B',\n",
       "  'European Union',\n",
       "  'European_Union',\n",
       "  'http://en.wikipedia.org/wiki/European_Union',\n",
       "  '9317',\n",
       "  '/m/02jxk'],\n",
       " ['Union',\n",
       "  'I',\n",
       "  'European Union',\n",
       "  'European_Union',\n",
       "  'http://en.wikipedia.org/wiki/European_Union',\n",
       "  '9317',\n",
       "  '/m/02jxk'],\n",
       " ['.'],\n",
       " [],\n",
       " ['He'],\n",
       " ['said'],\n",
       " ['a'],\n",
       " ['proposal'],\n",
       " ['last'],\n",
       " ['month'],\n",
       " ['by'],\n",
       " ['EU', 'B', 'EU', '--NME--'],\n",
       " ['Farm'],\n",
       " ['Commissioner'],\n",
       " ['Franz',\n",
       "  'B',\n",
       "  'Franz Fischler',\n",
       "  'Franz_Fischler',\n",
       "  'http://en.wikipedia.org/wiki/Franz_Fischler',\n",
       "  '626779',\n",
       "  '/m/02y4y1'],\n",
       " ['Fischler',\n",
       "  'I',\n",
       "  'Franz Fischler',\n",
       "  'Franz_Fischler',\n",
       "  'http://en.wikipedia.org/wiki/Franz_Fischler',\n",
       "  '626779',\n",
       "  '/m/02y4y1'],\n",
       " ['to'],\n",
       " ['ban'],\n",
       " ['sheep'],\n",
       " ['brains'],\n",
       " [','],\n",
       " ['spleens'],\n",
       " ['and'],\n",
       " ['spinal'],\n",
       " ['cords'],\n",
       " ['from'],\n",
       " ['the'],\n",
       " ['human'],\n",
       " ['and'],\n",
       " ['animal'],\n",
       " ['food'],\n",
       " ['chains'],\n",
       " ['was'],\n",
       " ['a'],\n",
       " ['highly'],\n",
       " ['specific'],\n",
       " ['and'],\n",
       " ['precautionary'],\n",
       " ['move'],\n",
       " ['to'],\n",
       " ['protect'],\n",
       " ['human'],\n",
       " ['health'],\n",
       " ['.'],\n",
       " [],\n",
       " ['Fischler', 'B', 'Fischler', '--NME--'],\n",
       " ['proposed'],\n",
       " ['EU-wide', 'B', 'EU-wide', '--NME--'],\n",
       " ['measures'],\n",
       " ['after'],\n",
       " ['reports'],\n",
       " ['from'],\n",
       " ['Britain',\n",
       "  'B',\n",
       "  'Britain',\n",
       "  'United_Kingdom',\n",
       "  'http://en.wikipedia.org/wiki/United_Kingdom',\n",
       "  '31717',\n",
       "  '/m/07ssc'],\n",
       " ['and'],\n",
       " ['France',\n",
       "  'B',\n",
       "  'France',\n",
       "  'France',\n",
       "  'http://en.wikipedia.org/wiki/France',\n",
       "  '5843419',\n",
       "  '/m/0f8l9c'],\n",
       " ['that'],\n",
       " ['under'],\n",
       " ['laboratory'],\n",
       " ['conditions'],\n",
       " ['sheep'],\n",
       " ['could'],\n",
       " ['contract'],\n",
       " ['Bovine', 'B', 'Bovine Spongiform Encephalopathy', '--NME--'],\n",
       " ['Spongiform', 'I', 'Bovine Spongiform Encephalopathy', '--NME--'],\n",
       " ['Encephalopathy', 'I', 'Bovine Spongiform Encephalopathy', '--NME--'],\n",
       " ['('],\n",
       " ['BSE',\n",
       "  'B',\n",
       "  'BSE',\n",
       "  'Bovine_spongiform_encephalopathy',\n",
       "  'http://en.wikipedia.org/wiki/Bovine_spongiform_encephalopathy',\n",
       "  '19344418',\n",
       "  '/m/0jy66'],\n",
       " [')'],\n",
       " ['--'],\n",
       " ['mad'],\n",
       " ['cow'],\n",
       " ['disease'],\n",
       " ['.'],\n",
       " [],\n",
       " ['But'],\n",
       " ['Fischler', 'B', 'Fischler', '--NME--'],\n",
       " ['agreed'],\n",
       " ['to'],\n",
       " ['review'],\n",
       " ['his'],\n",
       " ['proposal'],\n",
       " ['after'],\n",
       " ['the'],\n",
       " ['EU', 'B', 'EU', '--NME--'],\n",
       " [\"'s\"],\n",
       " ['standing'],\n",
       " ['veterinary'],\n",
       " ['committee'],\n",
       " [','],\n",
       " ['mational'],\n",
       " ['animal'],\n",
       " ['health'],\n",
       " ['officials'],\n",
       " [','],\n",
       " ['questioned'],\n",
       " ['if'],\n",
       " ['such'],\n",
       " ['action'],\n",
       " ['was'],\n",
       " ['justified'],\n",
       " ['as'],\n",
       " ['there'],\n",
       " ['was'],\n",
       " ['only'],\n",
       " ['a'],\n",
       " ['slight'],\n",
       " ['risk'],\n",
       " ['to'],\n",
       " ['human'],\n",
       " ['health'],\n",
       " ['.'],\n",
       " [],\n",
       " ['Spanish',\n",
       "  'B',\n",
       "  'Spanish',\n",
       "  'Spain',\n",
       "  'http://en.wikipedia.org/wiki/Spain',\n",
       "  '26667',\n",
       "  '/m/06mkj'],\n",
       " ['Farm'],\n",
       " ['Minister'],\n",
       " ['Loyola',\n",
       "  'B',\n",
       "  'Loyola de Palacio',\n",
       "  'Loyola_de_Palacio',\n",
       "  'http://en.wikipedia.org/wiki/Loyola_de_Palacio',\n",
       "  '6394317',\n",
       "  '/m/0g3rwc'],\n",
       " ['de',\n",
       "  'I',\n",
       "  'Loyola de Palacio',\n",
       "  'Loyola_de_Palacio',\n",
       "  'http://en.wikipedia.org/wiki/Loyola_de_Palacio',\n",
       "  '6394317',\n",
       "  '/m/0g3rwc'],\n",
       " ['Palacio',\n",
       "  'I',\n",
       "  'Loyola de Palacio',\n",
       "  'Loyola_de_Palacio',\n",
       "  'http://en.wikipedia.org/wiki/Loyola_de_Palacio',\n",
       "  '6394317',\n",
       "  '/m/0g3rwc'],\n",
       " ['had'],\n",
       " ['earlier'],\n",
       " ['accused'],\n",
       " ['Fischler', 'B', 'Fischler', '--NME--'],\n",
       " ['at'],\n",
       " ['an'],\n",
       " ['EU', 'B', 'EU', '--NME--'],\n",
       " ['farm'],\n",
       " ['ministers'],\n",
       " [\"'\"],\n",
       " ['meeting'],\n",
       " ['of'],\n",
       " ['causing'],\n",
       " ['unjustified'],\n",
       " ['alarm'],\n",
       " ['through'],\n",
       " [],\n",
       " ['dangerous'],\n",
       " ['generalisation'],\n",
       " ['.'],\n",
       " [],\n",
       " [],\n",
       " ['.'],\n",
       " [],\n",
       " ['Only'],\n",
       " ['France',\n",
       "  'B',\n",
       "  'France',\n",
       "  'France',\n",
       "  'http://en.wikipedia.org/wiki/France',\n",
       "  '5843419',\n",
       "  '/m/0f8l9c'],\n",
       " ['and'],\n",
       " ['Britain',\n",
       "  'B',\n",
       "  'Britain',\n",
       "  'United_Kingdom',\n",
       "  'http://en.wikipedia.org/wiki/United_Kingdom',\n",
       "  '31717',\n",
       "  '/m/07ssc'],\n",
       " ['backed'],\n",
       " ['Fischler', 'B', 'Fischler', '--NME--'],\n",
       " [\"'s\"],\n",
       " ['proposal'],\n",
       " ['.'],\n",
       " [],\n",
       " ['The'],\n",
       " ['EU', 'B', 'EU', '--NME--'],\n",
       " [\"'s\"],\n",
       " ['scientific'],\n",
       " ['veterinary'],\n",
       " ['and'],\n",
       " ['multidisciplinary'],\n",
       " ['committees'],\n",
       " ['are'],\n",
       " ['due'],\n",
       " ['to'],\n",
       " ['re-examine'],\n",
       " ['the'],\n",
       " ['issue'],\n",
       " ['early'],\n",
       " ['next'],\n",
       " ['month'],\n",
       " ['and'],\n",
       " ['make'],\n",
       " ['recommendations'],\n",
       " ['to'],\n",
       " ['the'],\n",
       " ['senior'],\n",
       " ['veterinary'],\n",
       " ['officials'],\n",
       " ['.'],\n",
       " [],\n",
       " ['Sheep'],\n",
       " ['have'],\n",
       " ['long'],\n",
       " ['been'],\n",
       " ['known'],\n",
       " ['to'],\n",
       " ['contract'],\n",
       " ['scrapie'],\n",
       " [','],\n",
       " ['a'],\n",
       " ['brain-wasting'],\n",
       " ['disease'],\n",
       " ['similar'],\n",
       " ['to'],\n",
       " ['BSE',\n",
       "  'B',\n",
       "  'BSE',\n",
       "  'Bovine_spongiform_encephalopathy',\n",
       "  'http://en.wikipedia.org/wiki/Bovine_spongiform_encephalopathy',\n",
       "  '19344418',\n",
       "  '/m/0jy66'],\n",
       " ['which'],\n",
       " ['is'],\n",
       " ['believed'],\n",
       " ['to'],\n",
       " ['have'],\n",
       " ['been'],\n",
       " ['transferred'],\n",
       " ['to'],\n",
       " ['cattle'],\n",
       " ['through'],\n",
       " ['feed'],\n",
       " ['containing'],\n",
       " ['animal'],\n",
       " ['waste'],\n",
       " ['.'],\n",
       " [],\n",
       " ['British',\n",
       "  'B',\n",
       "  'British',\n",
       "  'United_Kingdom',\n",
       "  'http://en.wikipedia.org/wiki/United_Kingdom',\n",
       "  '31717',\n",
       "  '/m/07ssc'],\n",
       " ['farmers'],\n",
       " ['denied'],\n",
       " ['on'],\n",
       " ['Thursday'],\n",
       " ['there'],\n",
       " ['was'],\n",
       " ['any'],\n",
       " ['danger'],\n",
       " ['to'],\n",
       " ['human'],\n",
       " ['health'],\n",
       " ['from'],\n",
       " ['their'],\n",
       " ['sheep'],\n",
       " [','],\n",
       " ['but'],\n",
       " ['expressed'],\n",
       " ['concern'],\n",
       " ['that'],\n",
       " ['German',\n",
       "  'B',\n",
       "  'German',\n",
       "  'Germany',\n",
       "  'http://en.wikipedia.org/wiki/Germany',\n",
       "  '11867',\n",
       "  '/m/0345h'],\n",
       " ['government'],\n",
       " ['advice'],\n",
       " ['to'],\n",
       " ['consumers'],\n",
       " ['to'],\n",
       " ['avoid'],\n",
       " ['British',\n",
       "  'B',\n",
       "  'British',\n",
       "  'United_Kingdom',\n",
       "  'http://en.wikipedia.org/wiki/United_Kingdom',\n",
       "  '31717',\n",
       "  '/m/07ssc'],\n",
       " ['lamb'],\n",
       " ['might'],\n",
       " ['influence'],\n",
       " ['consumers'],\n",
       " ['across'],\n",
       " ['Europe',\n",
       "  'B',\n",
       "  'Europe',\n",
       "  'Europe',\n",
       "  'http://en.wikipedia.org/wiki/Europe',\n",
       "  '9239',\n",
       "  '/m/02j9z'],\n",
       " ['.'],\n",
       " [],\n",
       " [],\n",
       " ['What'],\n",
       " ['we'],\n",
       " ['have'],\n",
       " ['to'],\n",
       " ['be'],\n",
       " ['extremely'],\n",
       " ['careful'],\n",
       " ['of'],\n",
       " ['is'],\n",
       " ['how'],\n",
       " ['other'],\n",
       " ['countries'],\n",
       " ['are'],\n",
       " ['going'],\n",
       " ['to'],\n",
       " ['take'],\n",
       " ['Germany',\n",
       "  'B',\n",
       "  'Germany',\n",
       "  'Germany',\n",
       "  'http://en.wikipedia.org/wiki/Germany',\n",
       "  '11867',\n",
       "  '/m/0345h'],\n",
       " [\"'s\"],\n",
       " ['lead'],\n",
       " [','],\n",
       " [],\n",
       " ['Welsh', 'B', \"Welsh National Farmers ' Union\", '--NME--'],\n",
       " ['National', 'I', \"Welsh National Farmers ' Union\", '--NME--'],\n",
       " ['Farmers', 'I', \"Welsh National Farmers ' Union\", '--NME--'],\n",
       " [\"'\", 'I', \"Welsh National Farmers ' Union\", '--NME--'],\n",
       " ['Union', 'I', \"Welsh National Farmers ' Union\", '--NME--'],\n",
       " ['('],\n",
       " ['NFU', 'B', 'NFU', '--NME--'],\n",
       " [')'],\n",
       " ['chairman'],\n",
       " ['John', 'B', 'John Lloyd Jones', '--NME--'],\n",
       " ['Lloyd', 'I', 'John Lloyd Jones', '--NME--'],\n",
       " ['Jones', 'I', 'John Lloyd Jones', '--NME--'],\n",
       " ['said'],\n",
       " ['on'],\n",
       " ['BBC', 'B', 'BBC radio', '--NME--'],\n",
       " ['radio', 'I', 'BBC radio', '--NME--'],\n",
       " ['.'],\n",
       " [],\n",
       " ['Bonn',\n",
       "  'B',\n",
       "  'Bonn',\n",
       "  'Bonn',\n",
       "  'http://en.wikipedia.org/wiki/Bonn',\n",
       "  '3295',\n",
       "  '/m/0150n'],\n",
       " ['has'],\n",
       " ['led'],\n",
       " ['efforts'],\n",
       " ['to'],\n",
       " ['protect'],\n",
       " ['public'],\n",
       " ['health'],\n",
       " ['after'],\n",
       " ['consumer'],\n",
       " ['confidence'],\n",
       " ['collapsed'],\n",
       " ['in'],\n",
       " ['March'],\n",
       " ['after'],\n",
       " ['a'],\n",
       " ['British',\n",
       "  'B',\n",
       "  'British',\n",
       "  'United_Kingdom',\n",
       "  'http://en.wikipedia.org/wiki/United_Kingdom',\n",
       "  '31717',\n",
       "  '/m/07ssc'],\n",
       " ['report'],\n",
       " ['suggested'],\n",
       " ['humans'],\n",
       " ['could'],\n",
       " ['contract'],\n",
       " ['an'],\n",
       " ['illness'],\n",
       " ['similar'],\n",
       " ['to'],\n",
       " ['mad'],\n",
       " ['cow'],\n",
       " ['disease'],\n",
       " ['by'],\n",
       " ['eating'],\n",
       " ['contaminated'],\n",
       " ['beef'],\n",
       " ['.'],\n",
       " [],\n",
       " ['Germany',\n",
       "  'B',\n",
       "  'Germany',\n",
       "  'Germany',\n",
       "  'http://en.wikipedia.org/wiki/Germany',\n",
       "  '11867',\n",
       "  '/m/0345h'],\n",
       " ['imported'],\n",
       " ['47,600'],\n",
       " ['sheep'],\n",
       " ['from'],\n",
       " ['Britain',\n",
       "  'B',\n",
       "  'Britain',\n",
       "  'United_Kingdom',\n",
       "  'http://en.wikipedia.org/wiki/United_Kingdom',\n",
       "  '31717',\n",
       "  '/m/07ssc'],\n",
       " ['last'],\n",
       " ['year'],\n",
       " [','],\n",
       " ['nearly'],\n",
       " ['half'],\n",
       " ['of'],\n",
       " ['total'],\n",
       " ['imports'],\n",
       " ['.'],\n",
       " [],\n",
       " ['It'],\n",
       " ['brought'],\n",
       " ['in'],\n",
       " ['4,275'],\n",
       " ['tonnes'],\n",
       " ['of'],\n",
       " ['British',\n",
       "  'B',\n",
       "  'British',\n",
       "  'United_Kingdom',\n",
       "  'http://en.wikipedia.org/wiki/United_Kingdom',\n",
       "  '31717',\n",
       "  '/m/07ssc'],\n",
       " ['mutton'],\n",
       " [','],\n",
       " ['some'],\n",
       " ['10'],\n",
       " ['percent'],\n",
       " ['of'],\n",
       " ['overall'],\n",
       " ['imports'],\n",
       " ['.'],\n",
       " []]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_raw = []\n",
    "\n",
    "with open(train_file, encoding='utf-8') as train_tsvfile:\n",
    "    train_reader = csv.reader(train_tsvfile, delimiter='\\t')\n",
    "    for row in train_reader:\n",
    "        train_data_raw.append(row)\n",
    "    \n",
    "train_data_raw[:490] # just show up to the second document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create raw validation and test datasets\n",
    "test_file = 'C:\\\\Users\\\\andre\\\\Documents\\\\Berkeley\\\\w266\\\\w266_project\\\\data\\\\basic_data\\\\test_datasets\\\\AIDA\\\\testa_testb_aggregate_unquoted.txt'\n",
    "\n",
    "validation_data_raw = []\n",
    "test_data_raw = []\n",
    "\n",
    "with open(test_file, encoding='utf-8') as test_tsvfile:\n",
    "    test_reader = csv.reader(test_tsvfile, delimiter='\\t')\n",
    "    for row in test_reader:\n",
    "        validation_data_raw.append(row)\n",
    "\n",
    "for i in range(len(validation_data_raw)):\n",
    "    if validation_data_raw[i] == ['-DOCSTART- (1163testb SOCCER)']: # first test document\n",
    "        test_index = i\n",
    "        \n",
    "test_data_raw = validation_data_raw[test_index:]\n",
    "validation_data_raw = validation_data_raw[:test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikiName_id_mapFile = 'C:\\\\Users\\\\andre\\\\Documents\\\\Berkeley\\\\w266\\\\w266_project\\\\data\\\\basic_data\\\\wiki_name_id_map.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'United Kingdom'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(wikiName_id_mapFile, encoding='utf-8') as wikiMap_tsvfile:\n",
    "    wikiMap_reader = csv.reader(wikiMap_tsvfile, delimiter='\\t')\n",
    "    wikiName_id_map_dict = {int(row[1]): row[0] for row in wikiMap_reader if row[0][0].isalpha() or row[0][0].isnumeric()}\n",
    "\n",
    "wikiName_id_map_dict[31717]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2 - Pre-process the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules that will be used downstream\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import time\n",
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "from random import shuffle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.python.keras.layers import Lambda\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.backend import sparse_categorical_crossentropy\n",
    "from tensorflow.keras.layers import Dense, TimeDistributed\n",
    "\n",
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_bert_path = 'C:\\\\Users\\\\andre\\\\Documents\\\\Berkeley\\\\w266\\\\w266_project\\\\bert'\n",
    "# local path where the bert module has been cloned from git\n",
    "\n",
    "# make sure that the paths are accessible within the notebook\n",
    "sys.path.insert(0,local_bert_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\andre\\Documents\\Berkeley\\w266\\w266_project\\bert\\optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import optimization\n",
    "import run_classifier\n",
    "import tokenization\n",
    "import run_classifier_with_tfhub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the bert model that is cased\n",
    "bert_url = \"https://tfhub.dev/google/bert_cased_L-12_H-768_A-12/1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a BERT tokenizer so that words can be tokenized for BERT input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\andre\\Documents\\Berkeley\\w266\\w266_project\\bert\\tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\andre\\Documents\\Berkeley\\w266\\w266_project\\bert\\tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "graph = tf.get_default_graph()\n",
    "\n",
    "bert_module = hub.Module(bert_url)\n",
    "tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
    "\n",
    "vocab_file, do_lower_case = tf.Session().run([tokenization_info[\"vocab_file\"],\n",
    "                                            tokenization_info[\"do_lower_case\"]])\n",
    "\n",
    "tokenizer = tokenization.FullTokenizer(\n",
    "    vocab_file = vocab_file, do_lower_case = do_lower_case\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'token', '##izer', ',', 'is', 'it', 'even', 'working', '?']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make sure the tokenizer works\n",
    "tokenizer_test = tokenizer.tokenize('This tokenizer, is it even working?')\n",
    "tokenizer_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1188, 22559, 17260, 117, 1110, 1122, 1256, 1684, 136]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_ids_test = tokenizer.convert_tokens_to_ids(tokenizer_test)\n",
    "tokenizer_ids_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'token', '##izer', ',', 'is', 'it', 'even', 'working', '?']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(tokenizer_ids_test)\n",
    "# seems legit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the BERT tokenizer can be used to create tokens from the input text that can be used by the BERT model (more on that later).\n",
    "\n",
    "To simplify things, create a function that creates dictionary objects for document text, NER tags, entity Ids, and wikipedia URLs given row-by-row data (from training or test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "218505"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createUtilityDicts(data_rows):\n",
    "\n",
    "    data_text_dict = {}\n",
    "    data_nerTag_dict = {}\n",
    "    data_entityId_dict = {}\n",
    "    data_wikiUrl_dict = {}\n",
    "\n",
    "    # set the first DOCSTART of the train data as the dict key\n",
    "    dict_key = data_rows[0][0]\n",
    "    # create empty lists for dict values\n",
    "    text_dict_values = []\n",
    "    nerTag_dict_values = []\n",
    "    entityId_dict_values = []\n",
    "    wikiUrl_dict_values = []\n",
    "\n",
    "    for row in data_rows[1:]:\n",
    "\n",
    "        if row == []: #signaling end of sentence\n",
    "            text_dict_values.append('[SEP]')\n",
    "            nerTag_dict_values.append('[nerSEP]')\n",
    "            entityId_dict_values.append('[entSEP]')\n",
    "            wikiUrl_dict_values.append('[urlSEP]')\n",
    "\n",
    "        elif len(row[0]) > 9 and row[0][:10] == '-DOCSTART-': # signaling a new document\n",
    "            data_text_dict.update({dict_key : text_dict_values})\n",
    "            data_nerTag_dict.update({dict_key : nerTag_dict_values})\n",
    "            data_entityId_dict.update({dict_key : entityId_dict_values})\n",
    "            data_wikiUrl_dict.update({dict_key : wikiUrl_dict_values})\n",
    "\n",
    "            # reset key and value objects\n",
    "            dict_key = row[0]\n",
    "            text_dict_values = []\n",
    "            nerTag_dict_values = []\n",
    "            entityId_dict_values = []\n",
    "            wikiUrl_dict_values = []\n",
    "\n",
    "        elif len(row) > 1: # i.e. there is an entity mention\n",
    "            for i in range(len(row)): # skip the possibly variable number of wiki names\n",
    "                if row[i][:4] == 'http' or row[i] == '--NME--':\n",
    "                    wikiUrl = row[i]                \n",
    "                    if row[i][:4] == 'http': # only for cases where there is a wiki url\n",
    "                        entityId = row[i-1] # append the wiki entity name, which precedes the url\n",
    "                    else:\n",
    "                        entityId = '--NME--'\n",
    "            \n",
    "            for i in range(len(tokenizer.tokenize(row[0]))): \n",
    "                text_dict_values.append(tokenizer.tokenize(row[0])[i]) # append all tokenized strings\n",
    "                nerTag_dict_values.append(row[1]) # append same NER tag for each token\n",
    "                wikiUrl_dict_values.append(wikiUrl) # append same Wiki URL for each token\n",
    "                entityId_dict_values.append(entityId) # append same enitity Id for each token\n",
    "\n",
    "        else: # i.e. there is no entity mention\n",
    "            for i in range(len(tokenizer.tokenize(row[0]))): # append all tokenized strings\n",
    "                text_dict_values.append(tokenizer.tokenize(row[0])[i])\n",
    "                if i == 0:\n",
    "                    nerTag_dict_values.append('O')\n",
    "                else:\n",
    "                    nerTag_dict_values.append('nerX')\n",
    "                entityId_dict_values.append(None)\n",
    "                wikiUrl_dict_values.append(None)\n",
    "\n",
    "    return data_text_dict, data_nerTag_dict, data_entityId_dict, data_wikiUrl_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_dict, train_nerTag_dict, train_entityId_dict, train_wikiUrl_dict = createUtilityDicts(train_data_raw)\n",
    "\n",
    "val_text_dict, val_nerTag_dict, val_entityId_dict, val_wikiUrl_dict = createUtilityDicts(validation_data_raw)\n",
    "\n",
    "test_text_dict, test_nerTag_dict, test_entityId_dict, test_wikiUrl_dict = createUtilityDicts(test_data_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check some descriptive statistics on the sentence lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentenceLengths = []\n",
    "\n",
    "for sentences in train_text_dict.values():\n",
    "    i = 0\n",
    "    for token in sentences:\n",
    "        if token != '[SEP]':\n",
    "            i += 1\n",
    "        else:\n",
    "            sentenceLengths.append(i)\n",
    "            i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence mean 16.784703253041968\n",
      "max sentence length 171\n"
     ]
    }
   ],
   "source": [
    "print('sentence mean', np.mean(sentenceLengths))\n",
    "print('max sentence length', max(sentenceLengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.389e+03, 9.000e+02, 9.300e+02, 1.160e+03, 1.899e+03, 1.486e+03,\n",
       "        1.186e+03, 8.170e+02, 7.220e+02, 5.450e+02, 5.910e+02, 5.220e+02,\n",
       "        5.010e+02, 4.660e+02, 4.350e+02, 3.930e+02, 3.750e+02, 3.210e+02,\n",
       "        2.780e+02, 2.520e+02, 1.620e+02, 1.610e+02, 1.460e+02, 1.040e+02,\n",
       "        1.060e+02, 6.400e+01, 5.500e+01, 4.500e+01, 2.200e+01, 1.500e+01,\n",
       "        1.600e+01, 1.100e+01, 6.000e+00, 3.000e+00, 7.000e+00, 4.000e+00,\n",
       "        3.000e+00, 3.000e+00, 1.000e+00, 0.000e+00, 2.000e+00, 0.000e+00,\n",
       "        1.000e+00, 1.000e+00, 1.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "        0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "        0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "        0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "        0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "        0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "        0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "        1.000e+00]),\n",
       " array([  0.        ,   2.01176471,   4.02352941,   6.03529412,\n",
       "          8.04705882,  10.05882353,  12.07058824,  14.08235294,\n",
       "         16.09411765,  18.10588235,  20.11764706,  22.12941176,\n",
       "         24.14117647,  26.15294118,  28.16470588,  30.17647059,\n",
       "         32.18823529,  34.2       ,  36.21176471,  38.22352941,\n",
       "         40.23529412,  42.24705882,  44.25882353,  46.27058824,\n",
       "         48.28235294,  50.29411765,  52.30588235,  54.31764706,\n",
       "         56.32941176,  58.34117647,  60.35294118,  62.36470588,\n",
       "         64.37647059,  66.38823529,  68.4       ,  70.41176471,\n",
       "         72.42352941,  74.43529412,  76.44705882,  78.45882353,\n",
       "         80.47058824,  82.48235294,  84.49411765,  86.50588235,\n",
       "         88.51764706,  90.52941176,  92.54117647,  94.55294118,\n",
       "         96.56470588,  98.57647059, 100.58823529, 102.6       ,\n",
       "        104.61176471, 106.62352941, 108.63529412, 110.64705882,\n",
       "        112.65882353, 114.67058824, 116.68235294, 118.69411765,\n",
       "        120.70588235, 122.71764706, 124.72941176, 126.74117647,\n",
       "        128.75294118, 130.76470588, 132.77647059, 134.78823529,\n",
       "        136.8       , 138.81176471, 140.82352941, 142.83529412,\n",
       "        144.84705882, 146.85882353, 148.87058824, 150.88235294,\n",
       "        152.89411765, 154.90588235, 156.91764706, 158.92941176,\n",
       "        160.94117647, 162.95294118, 164.96470588, 166.97647059,\n",
       "        168.98823529, 171.        ]),\n",
       " <a list of 85 Patch objects>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(sentenceLengths, bins=85) #bins=170)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 - Generate input for the BERT model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because BERT has a limit of 512 tokens per input, I will try to split documents up into the component sentences, using the max sentence length of 50-75 for training. As can be seen by the histogram above, almost all sentences will fall into this range.\n",
    "\n",
    "Additionally, for each sentence we will need to add [CLS] and [PAD] tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will need **Sentences, NER Tokens,** and **entity ids**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createBertTextObjects(text_dict, nerTag_dict, entId_dict, max_sentence_length):\n",
    "    \"\"\"\n",
    "    Add [CLS] and [PAD] tokens to text, NER tag, and entity Id data \n",
    "    [PAD] tokens are added until max_sentence_length\n",
    "    returns bert_sentences, bert_ner_tags, bert_entities, bert_mask_ids\n",
    "    *Concepts borrowed from BERT_NER_v1\n",
    "    \"\"\"\n",
    "    max_sentence_length -= 1 # to account for Python's zero-based indexing\n",
    "    \n",
    "    bert_sentences = []\n",
    "    bert_ner_tags = []\n",
    "    bert_entities = []\n",
    "    bert_mask_ids = []\n",
    "    \n",
    "    # start with [CLS] token\n",
    "    sentence = ['[CLS]'] \n",
    "    ner_tags = ['[nerCLS]']\n",
    "    entity_ids = ['[entCLS]']\n",
    "    \n",
    "    for item in text_dict:\n",
    "    \n",
    "        for i in range(len(text_dict[item])):\n",
    "\n",
    "            if text_dict[item][i] == '[SEP]':\n",
    "                sentence_length = min(max_sentence_length, len(sentence))\n",
    "\n",
    "                # truncate sequences that are longer than the max_sentence_length\n",
    "                sentence = sentence[:max_sentence_length-1]\n",
    "                ner_tags = ner_tags[:max_sentence_length-1]\n",
    "                entity_ids = entity_ids[:max_sentence_length-1]\n",
    "\n",
    "                # mask all [PAD] items\n",
    "                bert_mask_ids.append([1] * (sentence_length + 1) + \n",
    "                                     [0] * (max_sentence_length - sentence_length))\n",
    "\n",
    "                # insert final [SEP] and [PAD] tokens\n",
    "                sentence += [text_dict[item][i]] + ['[PAD]'] * (max_sentence_length - len(sentence))\n",
    "                ner_tags += [nerTag_dict[item][i]] + ['[nerPAD]'] * (max_sentence_length - len(ner_tags))\n",
    "                entity_ids += [entId_dict[item][i]] + ['[entPAD]'] * (max_sentence_length - len(entity_ids))\n",
    "\n",
    "                bert_sentences.append(sentence)\n",
    "                bert_ner_tags.append(ner_tags)\n",
    "                bert_entities.append(entity_ids)\n",
    "\n",
    "                # restart with [CLS] token\n",
    "                sentence = ['[CLS]'] \n",
    "                ner_tags = ['[nerCLS]']\n",
    "                entity_ids = ['[entCLS]']\n",
    "            else:    \n",
    "                sentence += [text_dict[item][i]]\n",
    "                ner_tags += [nerTag_dict[item][i]]\n",
    "                entity_ids += [entId_dict[item][i]]\n",
    "            \n",
    "    bert_mask_ids = np.asarray(bert_mask_ids)\n",
    "            \n",
    "    return bert_sentences, bert_ner_tags, bert_entities, bert_mask_ids\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sentence_length = 50\n",
    "\n",
    "bert_sentences, bert_ner_tags, bert_entities, bert_mask_ids = createBertTextObjects(train_text_dict,\n",
    "                                                                       train_nerTag_dict,\n",
    "                                                                       train_entityId_dict,\n",
    "                                                                       max_sentence_length = max_sentence_length)\n",
    "\n",
    "val_sentences, val_ner_tags, val_entities, val_mask_ids = createBertTextObjects(val_text_dict,\n",
    "                                                                       val_nerTag_dict,\n",
    "                                                                       val_entityId_dict,\n",
    "                                                                       max_sentence_length = max_sentence_length)\n",
    "\n",
    "test_sentences, test_ner_tags, test_entities, test_mask_ids = createBertTextObjects(test_text_dict,\n",
    "                                                                       test_nerTag_dict,\n",
    "                                                                       test_entityId_dict,\n",
    "                                                                       max_sentence_length = max_sentence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our BERT inputs as text, we need to modify all inputs to be numerical values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approach 1 - NER tags as categorical codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createBertNumObjects1(bert_sentences, bert_ner_tags):\n",
    "    \"\"\"\n",
    "    Converts BERT text objects created from the createBertTextObjects output into numerical arrays\n",
    "    \"\"\"\n",
    "    \n",
    "    bert_sentence_ids = []\n",
    "    \n",
    "    # convert sentence text to token ids using tokenizer\n",
    "    for sentence in bert_sentences:\n",
    "        bert_sentence_ids.append(tokenizer.convert_tokens_to_ids(sentence))\n",
    "        \n",
    "    bert_sentence_ids = np.asarray(bert_sentence_ids, dtype=np.int32)\n",
    "        \n",
    "    # convert NER tag text to categorical codes\n",
    "    nerTag_categories = pd.DataFrame(np.array(bert_ner_tags).reshape(-1))\n",
    "    nerTag_categories.columns = ['text']\n",
    "    nerTag_categories.text = pd.Categorical(nerTag_categories.text)\n",
    "    nerTag_categories['cat'] = nerTag_categories.text.cat.codes\n",
    "    nerTag_categories['sym'] = nerTag_categories.cat \n",
    "    \n",
    "    nerDistribution = (nerTag_categories.groupby(['text','cat']).agg({'sym':'count'}).reset_index()\n",
    "                   .rename(columns={'sym':'occurences'}))\n",
    "    #print(nerDistribution)\n",
    "    \n",
    "    bert_nerTag_ids = np.array(nerTag_categories.cat, dtype=np.int32).reshape(len(bert_ner_tags), len(bert_ner_tags[0]))\n",
    "    \n",
    "    # create a sequence id array (which will just be all zero values)\n",
    "    bert_seq_ids = []\n",
    "\n",
    "    for i in range(len(bert_sentence_ids)):\n",
    "        seq_list = ([0] * len(bert_sentence_ids[i]))\n",
    "        bert_seq_ids.append(seq_list)\n",
    "\n",
    "    bert_seq_ids = np.asarray(bert_seq_ids)\n",
    "    \n",
    "    return bert_sentence_ids, bert_nerTag_ids, bert_seq_ids\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approach 2 - Binary tags (i.e. is entity or is not entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createBertNumObjects2(bert_sentences, bert_ner_tags):\n",
    "    \"\"\"\n",
    "    Converts BERT text objects created from the createBertTextObjects output into numerical arrays\n",
    "    \"\"\"\n",
    "    \n",
    "    bert_sentence_ids = []\n",
    "    \n",
    "    # convert sentence text to token ids using tokenizer\n",
    "    for sentence in bert_sentences:\n",
    "        bert_sentence_ids.append(tokenizer.convert_tokens_to_ids(sentence))\n",
    "        \n",
    "    bert_sentence_ids = np.asarray(bert_sentence_ids, dtype=np.int32)\n",
    "        \n",
    "    # convert NER tag text to binary 'is-entity' boolean\n",
    "    bert_nerTag_ids = []\n",
    "    for item in bert_ner_tags:\n",
    "        sentenceTags = []\n",
    "        for tag in item:\n",
    "            if tag in ['B','I']: # i.e. tag corresponds with an entity\n",
    "                sentenceTags.append([1])\n",
    "            else:\n",
    "                sentenceTags.append([0])\n",
    "        bert_nerTag_ids.append(sentenceTags)\n",
    "    \n",
    "    bert_nerTag_ids = np.asarray(bert_nerTag_ids)\n",
    "    \n",
    "    # create a sequence id array (which will just be all zero values)\n",
    "    bert_seq_ids = []\n",
    "\n",
    "    for i in range(len(bert_sentence_ids)):\n",
    "        seq_list = ([0] * len(bert_sentence_ids[i]))\n",
    "        bert_seq_ids.append(seq_list)\n",
    "\n",
    "    bert_seq_ids = np.asarray(bert_seq_ids)\n",
    "    \n",
    "    return bert_sentence_ids, bert_nerTag_ids, bert_seq_ids\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approach 3 - codes for only relevant tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createBertNumObjects3(bert_sentences, bert_ner_tags):\n",
    "    \"\"\"\n",
    "    Converts BERT text objects created from the createBertTextObjects output into numerical arrays\n",
    "    \"\"\"\n",
    "    \n",
    "    bert_sentence_ids = []\n",
    "    \n",
    "    # convert sentence text to token ids using tokenizer\n",
    "    for sentence in bert_sentences:\n",
    "        bert_sentence_ids.append(tokenizer.convert_tokens_to_ids(sentence))\n",
    "        \n",
    "    bert_sentence_ids = np.asarray(bert_sentence_ids, dtype=np.int32)\n",
    "        \n",
    "    # convert NER tag text to binary 'is-entity' boolean\n",
    "    bert_nerTag_ids = []\n",
    "    for item in bert_ner_tags:\n",
    "        sentenceTags = []\n",
    "        for tag in item:\n",
    "            if tag == 'B': # i.e. tag corresponds with an entity\n",
    "                sentenceTags.append([1])\n",
    "            elif tag == 'I':\n",
    "                sentenceTags.append([2])\n",
    "            else:\n",
    "                sentenceTags.append([0])\n",
    "        bert_nerTag_ids.append(sentenceTags)\n",
    "    \n",
    "    bert_nerTag_ids = np.asarray(bert_nerTag_ids)\n",
    "    \n",
    "    # create a sequence id array (which will just be all zero values)\n",
    "    bert_seq_ids = []\n",
    "\n",
    "    for i in range(len(bert_sentence_ids)):\n",
    "        seq_list = ([0] * len(bert_sentence_ids[i]))\n",
    "        bert_seq_ids.append(seq_list)\n",
    "\n",
    "    bert_seq_ids = np.asarray(bert_seq_ids)\n",
    "    \n",
    "    return bert_sentence_ids, bert_nerTag_ids, bert_seq_ids\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bert_sentence_ids, bert_nerTag_ids, bert_seq_ids = createBertNumObjects3(bert_sentences, bert_ner_tags)\n",
    "\n",
    "val_sentence_ids, val_nerTag_ids, val_seq_ids = createBertNumObjects3(val_sentences, val_ner_tags)\n",
    "\n",
    "test_sentence_ids, test_nerTag_ids, test_seq_ids = createBertNumObjects3(test_sentences, test_ner_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16108, 50)\n",
      "(16108, 50, 1)\n",
      "(16108, 50)\n"
     ]
    }
   ],
   "source": [
    "print(bert_sentence_ids.shape)\n",
    "print(bert_nerTag_ids.shape) # extra dim of 1 for labels\n",
    "print(bert_seq_ids.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the BERT model with custom accuracy function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.reset_default_graph()\n",
    "\n",
    "def initialize_vars(sess):\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.tables_initializer())\n",
    "    \n",
    "def clear_and_set_sess(sess):\n",
    "    K.clear_session()\n",
    "    K.set_session(sess)\n",
    "\n",
    "#sess = tf.Session()\n",
    "#graph = tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will use a custom accuracy function to show how accurate the model performs with regards to 'B' and 'I' tags only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_entity_accuracy(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate accuracy of predicting 'B' and 'I' lables only\n",
    "    \n",
    "    y_true: actual NER tag codes with the 'O' value (0) masked\n",
    "    y_pred: predicted NER tag codes with the 'O' value (0) masked \n",
    "    \n",
    "    returns accuracy measure\n",
    "    \"\"\"\n",
    "\n",
    "    #get labels and predictions\n",
    "    \n",
    "    y_label = tf.reshape(tf.layers.Flatten()(tf.cast(y_true, tf.int32)),[-1])\n",
    "    mask = (y_label > 0) # evaluates to True for all non 'O'/0 labels\n",
    "    y_label_masked = tf.boolean_mask(y_label, mask)  # mask the labels\n",
    "    \n",
    "    y_flat_pred = tf.reshape(tf.layers.Flatten()(tf.cast(y_pred, tf.float64)),[-1,3])\n",
    "    y_max_pred = tf.cast(tf.math.argmax(input = y_flat_pred, axis=1), tf.int32) # take the max predicted value\n",
    "    y_pred_masked = tf.boolean_mask(y_max_pred, mask) # mask the predictions as well\n",
    "    \n",
    "    return tf.reduce_mean(tf.cast(tf.equal(y_pred_masked, y_label_masked), dtype=tf.float64))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure this works the way we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x00000246DEFAD198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x00000246DEFAD198>>: AttributeError: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x00000246DEFAD198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x00000246DEFAD198>>: AttributeError: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x00000246DEFAD198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x00000246DEFAD198>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:From C:\\Users\\andre\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\andre\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x00000246DEFAD198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x00000246DEFAD198>>: AttributeError: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x00000246DEFAD198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x00000246DEFAD198>>: AttributeError: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x00000246DEFAD198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x00000246DEFAD198>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "y_true = tf.constant([1,0,1,0])\n",
    "y_pred = tf.constant([[0.9,0.05,0.05],[0.9,0.05,0.05],[0.05,0.9,0.05],[0.05,0.9,0.05]])\n",
    "\n",
    "print(is_entity_accuracy(y_true, y_pred).eval(session=sess))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that it is working, we can create the BERT layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code borrowed from: https://towardsdatascience.com/bert-in-keras-with-tensorflow-hub-76bcbc9417b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Create BERT layer, following https://towardsdatascience.com/bert-in-keras-with-tensorflow-hub-76bcbc9417b\n",
    "    init:  initialize layer. Specify various parameters regarding output types and dimensions. Very important is\n",
    "           to set the number of trainable layers.\n",
    "    build: build the layer based on parameters\n",
    "    call:  call the BERT layer within a model\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        n_fine_tune_layers=10,\n",
    "        pooling=\"sequence\",\n",
    "        bert_url = bert_url,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.n_fine_tune_layers = n_fine_tune_layers\n",
    "        self.trainable = True\n",
    "        self.output_size = 768\n",
    "        self.pooling = pooling\n",
    "        self.bert_url = bert_url\n",
    "\n",
    "        super(BertLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.bert = hub.Module(\n",
    "            self.bert_url, trainable=self.trainable, name=f\"{self.name}_module\"\n",
    "        )\n",
    "\n",
    "        # Remove unused layers\n",
    "        trainable_vars = self.bert.variables\n",
    "        trainable_vars = [\n",
    "                var\n",
    "                for var in trainable_vars\n",
    "                if not \"/cls/\" in var.name and not \"/pooler/\" in var.name\n",
    "            ]\n",
    "        trainable_layers = []\n",
    "\n",
    "\n",
    "        # Select how many layers to fine tune\n",
    "        for i in range(self.n_fine_tune_layers):\n",
    "            trainable_layers.append(f\"encoder/layer_{str(11 - i)}\")\n",
    "\n",
    "        # Update trainable vars to contain only the specified layers\n",
    "        trainable_vars = [\n",
    "            var\n",
    "            for var in trainable_vars\n",
    "            if any([l in var.name for l in trainable_layers])\n",
    "        ]\n",
    "\n",
    "        # Add to trainable weights\n",
    "        for var in trainable_vars:\n",
    "            self._trainable_weights.append(var)\n",
    "\n",
    "        for var in self.bert.variables:\n",
    "            if var not in self._trainable_weights:\n",
    "                self._non_trainable_weights.append(var)\n",
    "\n",
    "        super(BertLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        inputs = [K.cast(x, dtype=\"int32\") for x in inputs]\n",
    "        input_ids, input_mask, segment_ids = inputs\n",
    "        bert_inputs = dict(\n",
    "            input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids\n",
    "        )\n",
    "        result = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\n",
    "                \"sequence_output\"\n",
    "            ]\n",
    "\n",
    "        mul_mask = lambda x, m: x * tf.expand_dims(m, axis=-1)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary metrics for the Tensor board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variable_summaries(var):\n",
    "    \"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean', mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        tf.summary.scalar('stddev', stddev)\n",
    "        tf.summary.scalar('max', tf.reduce_max(var))\n",
    "        tf.summary.scalar('min', tf.reduce_min(var))\n",
    "        tf.summary.histogram('histogram', var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And a custom adam optimizer that can be adjusted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_adam = tf.keras.optimizers.Adam(lr=0.0005, beta_1=0.915, beta_2=0.999, epsilon=None, decay=0.05, amsgrad=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes for modeling\n",
    "\n",
    "### 1. Sentence input -> BERT -> NER hidden layers -> predict NER tags / is-entity, is-not-entity\n",
    "### 2. Create BERT entity embeddings -> Take first N tokens from wiki summary, aggregate (average) the word embeddings to get a single vector\n",
    "### 3. Use some similarity function to generate N possible Entity Candidates -> Dense layer(N, softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.experimental.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method BertLayer.call of <__main__.BertLayer object at 0x0000029F920912E8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BertLayer.call of <__main__.BertLayer object at 0x0000029F920912E8>>: AttributeError: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method BertLayer.call of <__main__.BertLayer object at 0x0000029F920912E8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BertLayer.call of <__main__.BertLayer object at 0x0000029F920912E8>>: AttributeError: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method BertLayer.call of <__main__.BertLayer object at 0x0000029F920912E8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BertLayer.call of <__main__.BertLayer object at 0x0000029F920912E8>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\andre\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\andre\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x000002A0C81BA7B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x000002A0C81BA7B8>>: AttributeError: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x000002A0C81BA7B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x000002A0C81BA7B8>>: AttributeError: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x000002A0C81BA7B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x000002A0C81BA7B8>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x000002A0C81BA7B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x000002A0C81BA7B8>>: AttributeError: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x000002A0C81BA7B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x000002A0C81BA7B8>>: AttributeError: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x000002A0C81BA7B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x000002A0C81BA7B8>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "bert_token_ids (InputLayer)     [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert_input_masks (InputLayer)   [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert_seq_ids (InputLayer)       [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert_layer (BertLayer)          (None, None, 768)    108931396   bert_token_ids[0][0]             \n",
      "                                                                 bert_input_masks[0][0]           \n",
      "                                                                 bert_seq_ids[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 128)    98432       bert_layer[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, None, 128)    0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, None, 128)    16512       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, None, 128)    0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "entity_identification (Dense)   (None, None, 3)      387         dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 109,046,727\n",
      "Trainable params: 28,466,819\n",
      "Non-trainable params: 80,579,908\n",
      "__________________________________________________________________________________________________\n",
      "Train on 16108 samples, validate on 3846 samples\n",
      "Epoch 1/8\n",
      "16108/16108 [==============================] - 275s 17ms/sample - loss: 0.0290 - acc: 0.9905 - is_entity_accuracy: 0.9157 - val_loss: 0.0179 - val_acc: 0.9941 - val_is_entity_accuracy: 0.9429\n",
      "Epoch 2/8\n",
      "16108/16108 [==============================] - 290s 18ms/sample - loss: 0.0127 - acc: 0.9960 - is_entity_accuracy: 0.9674 - val_loss: 0.0156 - val_acc: 0.9952 - val_is_entity_accuracy: 0.9608\n",
      "Epoch 3/8\n",
      "16108/16108 [==============================] - 296s 18ms/sample - loss: 0.0096 - acc: 0.9971 - is_entity_accuracy: 0.9765 - val_loss: 0.0153 - val_acc: 0.9954 - val_is_entity_accuracy: 0.9636\n",
      "Epoch 4/8\n",
      "16108/16108 [==============================] - 299s 19ms/sample - loss: 0.0077 - acc: 0.9977 - is_entity_accuracy: 0.9822 - val_loss: 0.0157 - val_acc: 0.9955 - val_is_entity_accuracy: 0.9601\n",
      "Epoch 5/8\n",
      "16108/16108 [==============================] - 300s 19ms/sample - loss: 0.0064 - acc: 0.9981 - is_entity_accuracy: 0.9863 - val_loss: 0.0158 - val_acc: 0.9957 - val_is_entity_accuracy: 0.9652\n",
      "Epoch 6/8\n",
      "16108/16108 [==============================] - 310s 19ms/sample - loss: 0.0054 - acc: 0.9984 - is_entity_accuracy: 0.9878 - val_loss: 0.0166 - val_acc: 0.9958 - val_is_entity_accuracy: 0.9634\n",
      "Epoch 7/8\n",
      "16108/16108 [==============================] - 314s 20ms/sample - loss: 0.0046 - acc: 0.9987 - is_entity_accuracy: 0.9906 - val_loss: 0.0167 - val_acc: 0.9957 - val_is_entity_accuracy: 0.9663\n",
      "Epoch 8/8\n",
      "16108/16108 [==============================] - 317s 20ms/sample - loss: 0.0041 - acc: 0.9989 - is_entity_accuracy: 0.9919 - val_loss: 0.0178 - val_acc: 0.9958 - val_is_entity_accuracy: 0.9644\n"
     ]
    }
   ],
   "source": [
    "# Build model --> sentence inputs -> Bert layer -> Dense layer -> softmax is-entity prediction\n",
    "\n",
    "in_id = tf.keras.layers.Input(shape=(max_sentence_length,), name=\"bert_token_ids\")\n",
    "in_mask = tf.keras.layers.Input(shape=(max_sentence_length,), name=\"bert_input_masks\")\n",
    "in_segment = tf.keras.layers.Input(shape=(max_sentence_length,), name=\"bert_seq_ids\")\n",
    "\n",
    "bert_inputs = [in_id, in_mask, in_segment]\n",
    "\n",
    "# Instantiate the custom Bert Layer defined above\n",
    "bert_output = BertLayer(n_fine_tune_layers=4)(bert_inputs)\n",
    "\n",
    "# Build the rest of the classifier \n",
    "dense = tf.keras.layers.Dense(128, activation='relu', name='dense_1')(bert_output)\n",
    "dense = tf.keras.layers.Dropout(rate=0.3, name='dropout_1')(dense)\n",
    "dense = tf.keras.layers.Dense(128, activation='relu', name='dense_2')(dense)\n",
    "dense = tf.keras.layers.Dropout(rate=0.1, name='dropout_2')(dense)\n",
    "\n",
    "# prediction layer\n",
    "ner_pred = tf.keras.layers.Dense(3, activation='softmax', name='entity_identification')(dense)\n",
    "# take the maximum predicted value\n",
    "#pred = tf.math.argmax(input = tf.reshape(tf.keras.layers.Flatten()(tf.cast(pred, tf.float64)),[-1, 3]), axis=1)\n",
    "\n",
    "\n",
    "initialize_vars(sess)\n",
    "\n",
    "ner_model = tf.keras.models.Model(inputs=bert_inputs, outputs=ner_pred)\n",
    "#ner_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy',\n",
    "#                                                                                is_entity_accuracy])\n",
    "ner_model.compile(loss='sparse_categorical_crossentropy', optimizer=custom_adam, metrics=['accuracy',\n",
    "                                                                                is_entity_accuracy])\n",
    "ner_model.summary()\n",
    "\n",
    "\n",
    "with graph.as_default():\n",
    "    K.set_session(sess)\n",
    "    \n",
    "    ner_model.fit(\n",
    "        [bert_sentence_ids, bert_mask_ids, bert_seq_ids], \n",
    "        bert_nerTag_ids,\n",
    "        validation_data=([val_sentence_ids, val_mask_ids, val_seq_ids], \n",
    "                         val_nerTag_ids),\n",
    "        epochs=8,\n",
    "        batch_size=16\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_data_and_lables(sentence_ids, mask_ids, seq_ids, nerTag_ids):\n",
    "    if sentence_ids.shape[0] == mask_ids.shape[0] == seq_ids.shape[0] == nerTag_ids.shape[0]:\n",
    "        shuffle_index = np.random.permutation(sentence_ids.shape[0])\n",
    "        \n",
    "        sentence_ids_shuf = sentence_ids[shuffle_index]\n",
    "        mask_ids_shuf = mask_ids[shuffle_index]\n",
    "        seq_ids_shuf = seq_ids[shuffle_index]\n",
    "        nerTag_ids_shuf = nerTag_ids[shuffle_index]\n",
    "        \n",
    "        return sentence_ids_shuf, mask_ids_shuf, seq_ids_shuf, nerTag_ids_shuf\n",
    "        \n",
    "    else:\n",
    "        print('Incompatible shapes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_test = [val_sentence_ids, val_mask_ids, val_seq_ids]\n",
    "\n",
    "result = ner_model.predict(\n",
    "    bert_test, \n",
    "    batch_size=16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_entity_preds = []\n",
    "\n",
    "# for each sentence in result\n",
    "for i in range(len(result)):\n",
    "    # for each token in the sentence\n",
    "    sentence_preds = []\n",
    "    for j in range(len(result[i])):\n",
    "        # append the index of the max predicted value\n",
    "        index_value = np.where(result[i][j] == np.amax(result[i][j]))[0][0]\n",
    "        sentence_preds.append(index_value)\n",
    "    is_entity_preds.append(sentence_preds) \n",
    "    \n",
    "is_entity_preds = np.asarray(is_entity_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('[CLS]', 0, array([0])),\n",
       " ('West', 1, array([1])),\n",
       " ('Indian', 2, array([2])),\n",
       " ('all', 0, array([0])),\n",
       " ('-', 0, array([0])),\n",
       " ('round', 0, array([0])),\n",
       " ('##er', 0, array([0])),\n",
       " ('Phil', 1, array([1])),\n",
       " ('Simmons', 2, array([2])),\n",
       " ('took', 0, array([0])),\n",
       " ('four', 0, array([0])),\n",
       " ('for', 0, array([0])),\n",
       " ('38', 0, array([0])),\n",
       " ('on', 0, array([0])),\n",
       " ('Friday', 0, array([0])),\n",
       " ('as', 0, array([0])),\n",
       " ('Leicestershire', 1, array([1])),\n",
       " ('beat', 0, array([0])),\n",
       " ('Somerset', 1, array([1])),\n",
       " ('by', 0, array([0])),\n",
       " ('an', 0, array([0])),\n",
       " ('innings', 0, array([0])),\n",
       " ('and', 0, array([0])),\n",
       " ('39', 0, array([0])),\n",
       " ('runs', 0, array([0])),\n",
       " ('in', 0, array([0])),\n",
       " ('two', 0, array([0])),\n",
       " ('days', 0, array([0])),\n",
       " ('to', 0, array([0])),\n",
       " ('take', 0, array([0])),\n",
       " ('over', 0, array([0])),\n",
       " ('at', 0, array([0])),\n",
       " ('the', 0, array([0])),\n",
       " ('head', 0, array([0])),\n",
       " ('of', 0, array([0])),\n",
       " ('the', 0, array([0])),\n",
       " ('county', 0, array([0])),\n",
       " ('championship', 0, array([0])),\n",
       " ('.', 0, array([0])),\n",
       " ('[SEP]', 0, array([0])),\n",
       " ('[PAD]', 0, array([0])),\n",
       " ('[PAD]', 0, array([0])),\n",
       " ('[PAD]', 0, array([0])),\n",
       " ('[PAD]', 0, array([0])),\n",
       " ('[PAD]', 0, array([0])),\n",
       " ('[PAD]', 0, array([0])),\n",
       " ('[PAD]', 0, array([0])),\n",
       " ('[PAD]', 0, array([0])),\n",
       " ('[PAD]', 0, array([0])),\n",
       " ('[PAD]', 0, array([0]))]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "view_result = list(zip(val_sentences[2],is_entity_preds[2],val_nerTag_ids[2]))\n",
    "view_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4 - Create entity embeddings for Wikipedia entities\n",
    "\n",
    "### This will help the task of Candidate Generation for the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the wikiName_id_map_dict created earlier, scrape wikipedia articles for text descriptions of the entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brussels (French: Bruxelles [bysl] (listen) or [byksl]; Dutch: Brussel [brsl] (listen)), officially the Brussels-Capital Region (French: Rgion de Bruxelles-Capitale; Dutch: Brussels Hoofdstedelijk Gewest), is a region of Belgium comprising 19 municipalities, including the City of Brussels, which is the capital of Belgium. The Brussels-Capital Region is located in the central portion of the country and is a part of both the French Community of Belgium and the Flemish Community, but is separate from t\n"
     ]
    }
   ],
   "source": [
    "print(wikipedia.WikipediaPage(title = 'Brussels').summary[:512])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_summary_ids_filepath = 'C:\\\\Users\\\\andre\\\\Documents\\\\Berkeley\\\\w266\\\\w266_Project\\\\w266_Project\\\\data\\\\entity_summary_ids.npy'\n",
    "entity_mask_ids_filepath = 'C:\\\\Users\\\\andre\\\\Documents\\\\Berkeley\\\\w266\\\\w266_Project\\\\w266_Project\\\\data\\\\entity_mask_ids.npy'\n",
    "entity_seq_ids_filepath = 'C:\\\\Users\\\\andre\\\\Documents\\\\Berkeley\\\\w266\\\\w266_Project\\\\w266_Project\\\\data\\\\entity_seq_ids.npy'\n",
    "output_entities_filepath = 'C:\\\\Users\\\\andre\\\\Documents\\\\Berkeley\\\\w266\\\\w266_Project\\\\w266_Project\\\\output_entities.data'\n",
    "entities_ext_filepath = 'C:\\\\Users\\\\andre\\\\Documents\\\\Berkeley\\\\w266\\\\w266_Project\\\\w266_Project\\\\entity_ext.data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For loading pre-created entity objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below cell can be run to open files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_summary_ids = np.load(entity_summary_ids_filepath)\n",
    "entity_mask_ids = np.load(entity_mask_ids_filepath)\n",
    "entity_seq_ids = np.load(entity_seq_ids_filepath)\n",
    "\n",
    "with open(output_entities_filepath, 'rb') as file_in:\n",
    "    output_entities = pickle.load(file_in)\n",
    "\n",
    "with open(entities_ext_filepath, 'rb') as file_in:\n",
    "    entities_ext = pickle.load(file_in)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55890, 300)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_summary_ids.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Once run, skip to \"Encode the entities as vectors\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For loading entity and summaries the first time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "wikiId_summaryTokens_dict = {}\n",
    "i = 0\n",
    "for wikiId in wikiName_id_map_dict:\n",
    "\n",
    "    if i > 100:\n",
    "        break # control\n",
    "    i += 1\n",
    "    try:\n",
    "        summary = wikipedia.summary(wikiName_id_map_dict[wikiId])\n",
    "    except:\n",
    "        summary = None\n",
    "        \n",
    "    if summary:\n",
    "        dict_value = tokenizer.tokenize(summary)[:512] # max_length\n",
    "        dict_value = tokenizer.convert_tokens_to_ids(dict_value)\n",
    "        wikiId_summaryTokens_dict.update({wikiId: dict_value})\n",
    "\n",
    "timing = time.time() - start_time\n",
    "print('100 Wikipedia page summaries executed in',round(timing,2),'seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that the summary extraction takes so long, I'm going to cheat a bit to encode the entities as BERT 768 dimensional pooled vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = []\n",
    "\n",
    "for item in train_data_raw:\n",
    "    if len(item) > 1 and item[1][:7] != '--NME--': # skip -DOCSTART-, --NME--, and non-entities\n",
    "        for i in range(len(item)):\n",
    "            if item[i][:4] == 'http': # index of the url\n",
    "                stop_index = i\n",
    "                entities.extend(item[2:stop_index])\n",
    "        \n",
    "for item in validation_data_raw:\n",
    "    if len(item) > 1 and item[1][:7] != '--NME--': # skip -DOCSTART-, --NME--, and non-entities    \n",
    "        for i in range(len(item)):\n",
    "            if item[i][:4] == 'http': # index of the url\n",
    "                stop_index = i\n",
    "                entities.extend(item[2:stop_index])\n",
    "        \n",
    "for item in test_data_raw:\n",
    "    if len(item) > 1 and item[1][:7] != '--NME--': # skip -DOCSTART-, --NME--, and non-entities\n",
    "        for i in range(len(item)):\n",
    "            if item[i][:4] == 'http': # index of the url\n",
    "                stop_index = i\n",
    "                entities.extend(item[2:stop_index])\n",
    "        \n",
    "entities = list(set(entities)) # unique entities only\n",
    "\n",
    "# replace unicode characters\n",
    "for i in range(len(entities)):\n",
    "    if '\\\\u' in entities[i]:\n",
    "        entities[i] = entities[i].encode().decode('unicode_escape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's add some noise to our entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "entities_ext = []\n",
    "start_time = time.time()\n",
    "i = 1\n",
    "\n",
    "for entity in entities:\n",
    "    ent_start_time = time.time()\n",
    "    print(entity, ' - ', str(round((i/len(entities)*100),3))+'%')\n",
    "    entities_ext.extend(wikipedia.search(entity,results=5))\n",
    "    timing = time.time() - ent_start_time\n",
    "    print('\\t', round(timing,2),'seconds')\n",
    "    i += 1\n",
    "\n",
    "timing = time.time() - start_time\n",
    "\n",
    "print('\\n\\n')\n",
    "print('Created noise for',len(entities),'entities. Executed in', round((timing/60),2),'minutes \\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createBertEntityObjects(entity_list, num_entities = 10, max_length=512):\n",
    "    \"\"\"\n",
    "    Given an list of entities, output numerical bert token ids, mask ids, and sequence ids\n",
    "    Last output is the text list of each entity that is contained in the vectors\n",
    "    Used for input into the bert entity vectors\n",
    "    Starts at 10 entities for output\n",
    "    \"\"\"\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    max_length -= 2 # account for CLS and SEP tokens\n",
    "    \n",
    "    entity_summary_ids = []\n",
    "    entity_mask_ids = []\n",
    "    entity_seq_ids = []\n",
    "    output_entities = []\n",
    "    \n",
    "    i = 1\n",
    "\n",
    "    for item in entity_list[:num_entities]:\n",
    "        # give run time information as this step can take a long time\n",
    "        item_start_time = time.time()\n",
    "        print(item, ' - ', str(round((i/num_entities*100),3))+'%')\n",
    "        try:\n",
    "            summary = tokenizer.tokenize(wikipedia.summary(item))[:max_length] \n",
    "        except:\n",
    "            summary = None\n",
    "\n",
    "        if summary:\n",
    "            num_pads = max_length - len(summary) \n",
    "            masks = ([1] * (len(summary)+2)) + ([0] * num_pads) # no need to account for CLS and SEP pads\n",
    "            summary = ['[CLS]'] + summary + ['[SEP]'] + (['[PAD]'] * num_pads)\n",
    "            summary = tokenizer.convert_tokens_to_ids(summary)\n",
    "\n",
    "            output_entities.append(item)\n",
    "            entity_summary_ids.append(np.asarray(summary))\n",
    "            entity_mask_ids.append(np.asarray(masks))\n",
    "            entity_seq_ids.append(np.asarray([1] * (max_length + 2))) # no need to account for CLS and SEP pads\n",
    "        \n",
    "        timing = time.time() - item_start_time\n",
    "        print('\\t', round(timing,2),'seconds')\n",
    "        i += 1\n",
    "\n",
    "    entity_summary_ids = np.asarray(entity_summary_ids)\n",
    "    entity_mask_ids = np.asarray(entity_mask_ids)\n",
    "    entity_seq_ids = np.asarray(entity_seq_ids)\n",
    "\n",
    "    timing = time.time() - start_time\n",
    "    print('\\n\\n')\n",
    "    \n",
    "    print(len(entity_summary_ids), 'Wikipedia page summaries executed in',round((timing/60),2),'minutes')\n",
    "    print('entity_summary_ids shape:', entity_summary_ids.shape)\n",
    "    print('entity_mask_ids shape:', entity_mask_ids.shape)\n",
    "    print('entity_seq_ids shape:', entity_seq_ids.shape)\n",
    "    print('Number of text entities:',len(output_entities))\n",
    "    \n",
    "    return entity_summary_ids, entity_mask_ids, entity_seq_ids, output_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58846"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(entities_ext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Running this with all extended entities takes ~22 hours**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The Championship (TV programme)', 'Duma']"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities_ext[1040:1042]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'Championship']"
      ]
     },
     "execution_count": 493,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(wikipedia.summary('The Championship (TV programme)'))[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A duma () was a Russian assembly with advisory or legislative functions. The term comes from the Russian verb  (dumat) meaning \"to think\" or \"to consider\". The first formally constituted duma was the State Duma introduced into the Russian Empire by Tsar Nicholas II in 1905 after the revolt of people against him demanding for the elected assembly. The Tsar dismissed the first duma within 75 days and re-elected second duma within three months. It was dissolved in 1917 during the Russian Revolution. Since 1993, the State Duma is the lower legislative house of the Russian Federation.'"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikipedia.summary('Duma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Championship (TV programme)  -  50.0%\n",
      "\t 0.01 seconds\n",
      "Duma  -  100.0%\n",
      "\t 0.01 seconds\n",
      "\n",
      "\n",
      "\n",
      "2 Wikipedia page summaries executed in 0.0 minutes\n",
      "entity_summary_ids shape: (2, 50)\n",
      "entity_mask_ids shape: (2, 50)\n",
      "entity_seq_ids shape: (2, 50)\n",
      "Number of text entities: 2\n"
     ]
    }
   ],
   "source": [
    "num_entities = 2 #len(entities_ext)\n",
    "max_length = 50\n",
    "\n",
    "entity_summary_ids, entity_mask_ids, entity_seq_ids, output_entities = createBertEntityObjects(entities_ext[1040:1042], \n",
    "                                                                              num_entities = num_entities,\n",
    "                                                                              max_length = max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_summary = []\n",
    "\n",
    "for token in tokenizer.convert_ids_to_tokens(entity_summary_ids[1]):\n",
    "    if token not in stopwords:\n",
    "        test_summary.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'A', 'du', '##ma', '[SEP]']"
      ]
     },
     "execution_count": 501,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(entity_summary_ids[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1993 World Championships in Athletics  Women's 10,000 metres  -  2.0%\n",
      "\t 2.13 seconds\n",
      "Luis Garca (footballer, born 1978)  -  4.0%\n",
      "\t 1.66 seconds\n",
      "Luis Garca  -  6.0%\n",
      "\t 1.79 seconds\n",
      "Luis Garca Postigo  -  8.0%\n",
      "\t 1.57 seconds\n",
      "Luis Garca (footballer, born 1972)  -  10.0%\n",
      "\t 1.77 seconds\n",
      "Luis Garca Meza  -  12.0%\n",
      "\t 1.61 seconds\n",
      "Vicente Solano Lima  -  14.0%\n",
      "\t 1.77 seconds\n",
      "March 1973 Argentine general election  -  16.0%\n",
      "\t 1.77 seconds\n",
      "Isabel Martnez de Pern  -  18.0%\n",
      "\t 1.72 seconds\n",
      "Hctor Jos Cmpora  -  20.0%\n",
      "\t 1.69 seconds\n",
      "Ral Alberto Lastiri  -  22.0%\n",
      "\t 1.65 seconds\n",
      "Sale Sharks  -  24.0%\n",
      "\t 1.66 seconds\n",
      "Faf de Klerk  -  26.0%\n",
      "\t 1.64 seconds\n",
      "Jean-Luc du Preez  -  28.0%\n",
      "\t 1.77 seconds\n",
      "Tom Curry (rugby union)  -  30.0%\n",
      "\t 1.63 seconds\n",
      "Lood de Jager  -  32.0%\n",
      "\t 1.72 seconds\n",
      "Maccabi Petah Tikva F.C.  -  34.0%\n",
      "\t 1.7 seconds\n",
      "Petah Tikva  -  36.0%\n",
      "\t 1.95 seconds\n",
      "Dia Saba  -  38.0%\n",
      "\t 1.64 seconds\n",
      "Guy Luzon  -  40.0%\n",
      "\t 1.6 seconds\n",
      "Maccabi Petah Tikva (handball)  -  42.0%\n",
      "\t 1.93 seconds\n",
      "Mississippi River  -  44.0%\n",
      "\t 1.68 seconds\n",
      "Mississippi River System  -  46.0%\n",
      "\t 1.77 seconds\n",
      "Mississippi  -  48.0%\n",
      "\t 1.78 seconds\n",
      "Mississippi River Delta  -  50.0%\n",
      "\t 1.84 seconds\n",
      "Mississippi River floods of 2019  -  52.0%\n",
      "\t 1.64 seconds\n",
      "Zimbabwe Open  -  54.0%\n",
      "\t 1.84 seconds\n",
      "Zimbabwe Open University  -  56.0%\n",
      "\t 1.67 seconds\n",
      "List of universities in Zimbabwe  -  58.0%\n",
      "\t 1.7 seconds\n",
      "Zimbabwe  -  60.0%\n",
      "\t 1.99 seconds\n",
      "University of Zimbabwe  -  62.0%\n",
      "\t 1.81 seconds\n",
      "Ian Botham  -  64.0%\n",
      "\t 1.97 seconds\n",
      "Liam Botham  -  66.0%\n",
      "\t 1.91 seconds\n",
      "James Botham  -  68.0%\n",
      "\t 1.61 seconds\n",
      "Bill Beaumont  -  70.0%\n",
      "\t 1.77 seconds\n",
      "Lord Tim Hudson  -  72.0%\n",
      "\t 2.0 seconds\n",
      "Andy Townsend  -  74.0%\n",
      "\t 1.9 seconds\n",
      "Don Townsend  -  76.0%\n",
      "\t 1.77 seconds\n",
      "Devin Townsend  -  78.0%\n",
      "\t 1.91 seconds\n",
      "List of Chelsea F.C. players  -  80.0%\n",
      "\t 1.95 seconds\n",
      "The Championship (TV programme)  -  82.0%\n",
      "\t 1.72 seconds\n",
      "Duma  -  84.0%\n",
      "\t 1.74 seconds\n",
      "State Duma  -  86.0%\n",
      "\t 1.68 seconds\n",
      "Alexandre Dumas  -  88.0%\n",
      "\t 1.71 seconds\n",
      "Duma (disambiguation)  -  90.0%\n",
      "\t 2.37 seconds\n",
      "State Duma (Russian Empire)  -  92.0%\n",
      "\t 1.8 seconds\n",
      "Arabic  -  94.0%\n",
      "\t 1.74 seconds\n",
      "Arabic numerals  -  96.0%\n",
      "\t 1.76 seconds\n",
      "Arabic alphabet  -  98.0%\n",
      "\t 1.7 seconds\n",
      "Arabs  -  100.0%\n",
      "\t 1.81 seconds\n",
      "\n",
      "\n",
      "\n",
      "49 Wikipedia page summaries executed in 1.48 minutes\n",
      "entity_summary_ids shape: (49, 300)\n",
      "entity_mask_ids shape: (49, 300)\n",
      "entity_seq_ids shape: (49, 300)\n",
      "Number of text entities: 49\n"
     ]
    }
   ],
   "source": [
    "num_entities = 50 #len(entities_ext)\n",
    "max_length = 300\n",
    "\n",
    "entity_summary_ids, entity_mask_ids, entity_seq_ids, output_entities = createBertEntityObjects(entities_ext[1000:1050], \n",
    "                                                                              num_entities = num_entities,\n",
    "                                                                              max_length = max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sue Grafton'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = ['Sue','Graf','##ton']\n",
    "text_x = ' '.join(x).replace(' ##','')\n",
    "text_x "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode the entities as vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "#bert_module = hub.Module(\"https://tfhub.dev/google/bert_cased_L-12_H-768_A-12/1\", trainable=True)\n",
    "\n",
    "bert_entity_inputs = dict(\n",
    "    input_ids=entity_summary_ids,\n",
    "    input_mask=entity_mask_ids,\n",
    "    segment_ids=entity_seq_ids)\n",
    "\n",
    "# use pooled_output so that each entity has a 768-dimensional vector\n",
    "bert_entity_outputs = bert_module(bert_entity_inputs, signature=\"tokens\", as_dict=True)[\"pooled_output\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(2), Dimension(768)])"
      ]
     },
     "execution_count": 503,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_entity_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize_vars(sess)\n",
    "\n",
    "bert_entity_arr = bert_entity_outputs.eval(session=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 505,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(bert_entity_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(2), Dimension(768)])"
      ]
     },
     "execution_count": 506,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_entity_outputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode sentences as vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull the BERT sentence ids associated with our test entities\n",
    "\n",
    "entity_annotations = []\n",
    "entity_sentences = []\n",
    "\n",
    "entity_sentence_ids = []\n",
    "entity_masks = []\n",
    "entity_seq_ids = []\n",
    "entity_nerTag_ids = []\n",
    "\n",
    "# iterate through entity sentences\n",
    "for i in range(len(bert_entities)):\n",
    "    sentence_with_entity = None\n",
    "    # iterate through sentence tokens\n",
    "    for j in range(len(bert_entities[i])):\n",
    "    # if a sentence has an entity in our example list,\n",
    "    # append bert token, mask, and sequence ids\n",
    "        #if bert_entities[i][j] in entities_ext[1000:1050]:\n",
    "        if bert_entities[i][j] in entities_ext[1040:1042]:\n",
    "            sentence_with_entity = bert_entities[i]\n",
    "    if sentence_with_entity:\n",
    "        # raw data to help with evaluation\n",
    "        entity_annotations.append(bert_entities[i])\n",
    "        entity_sentences.append(bert_sentences[i])\n",
    "        # bert numerical inputs\n",
    "        entity_sentence_ids.append(bert_sentence_ids[i])\n",
    "        entity_masks.append(bert_mask_ids[i])\n",
    "        entity_seq_ids.append(bert_seq_ids[i])\n",
    "        entity_nerTag_ids.append(bert_nerTag_ids[i])\n",
    "        \n",
    "entity_sentence_ids = np.asarray(entity_sentence_ids)\n",
    "entity_masks = np.asarray(entity_masks)\n",
    "entity_seq_ids = np.asarray(entity_seq_ids)\n",
    "entity_nerTag_ids = np.asarray(entity_nerTag_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entity vector shapes: (2, 768)\n",
      "\t 2 summaries, each represented as a 768 dimensional vector\n"
     ]
    }
   ],
   "source": [
    "print('entity vector shapes:',bert_entity_outputs.shape)\n",
    "print('\\t',bert_entity_outputs.shape[0], 'summaries, each represented as a',\n",
    "      bert_entity_outputs.shape[1],'dimensional vector')\n",
    "\n",
    "#print('sentence vector shapes:',bert_sentence_outputs.shape)\n",
    "#print('\\t',bert_sentence_outputs.shape[0], 'sentences, each having',\n",
    "#      bert_sentence_outputs.shape[1],'tokens, each of which is represented as a',\n",
    "#     bert_sentence_outputs.shape[2],'dimensional vector')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the sentence vectors, each token within the sentence has a 768-dimensional vector, whereas the entity vectors are pooled and each entity summary is a 768-dimensional vector. To do the cosine similarity, we can take N tokens around the entity (assuming we have properly identified it) and aggregate those vector values, such that the sentence vectors and the entity vectors have the same shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now write the file out so that it can be easily used later on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Only needed when first creating entity objects**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output ndarray objects\n",
    "#entity_files = [entity_summary_ids_filepath, \n",
    "#                entity_mask_ids_filepath, \n",
    "#                entity_seq_ids_filepath]\n",
    "#\n",
    "#entity_data = [entity_summary_ids, \n",
    "#                entity_mask_ids, \n",
    "#                entity_seq_ids]\n",
    "#\n",
    "#for i in range(len(entity_files)):\n",
    "#    np.save(entity_files[i], entity_data[i])\n",
    "#    \n",
    "#\n",
    "# output list objects\n",
    "#entity_files = [output_entities_filepath,\n",
    "#                entities_ext_filepath]\n",
    "#\n",
    "#entity_data = [output_entities,\n",
    "#                entities_ext]\n",
    "#\n",
    "#for i in range(len(entity_files)):\n",
    "#    with open(entity_files[i], 'wb') as file_out:\n",
    "#        pickle.dump(entity_data[i], file_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute cosine similarity between wikipedia summary embeddings and sentence sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need a helper function to output a sequence around the entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('[CLS]', '[entCLS]', array([0])),\n",
       " ('They', None, array([0])),\n",
       " ('said', None, array([0])),\n",
       " ('Z', 'Vladimir_Zhirinovsky', array([1])),\n",
       " ('##hir', 'Vladimir_Zhirinovsky', array([1])),\n",
       " ('##ino', 'Vladimir_Zhirinovsky', array([1])),\n",
       " ('##vsky', 'Vladimir_Zhirinovsky', array([1])),\n",
       " ('told', None, array([0])),\n",
       " ('Saddam', 'Saddam_Hussein', array([1])),\n",
       " ('before', None, array([0])),\n",
       " ('he', None, array([0])),\n",
       " ('left', None, array([0])),\n",
       " ('Baghdad', 'Baghdad', array([1])),\n",
       " ('on', None, array([0])),\n",
       " ('Wednesday', None, array([0])),\n",
       " ('that', None, array([0])),\n",
       " ('his', None, array([0])),\n",
       " ('Liberal', '--NME--', array([1])),\n",
       " ('Democratic', '--NME--', array([2])),\n",
       " ('party', '--NME--', array([2])),\n",
       " ('and', None, array([0])),\n",
       " ('the', None, array([0])),\n",
       " ('Russian', 'Russia', array([1])),\n",
       " ('Du', 'Duma', array([1])),\n",
       " ('##ma', 'Duma', array([1])),\n",
       " ('(', None, array([0])),\n",
       " ('parliament', None, array([0])),\n",
       " (')', None, array([0])),\n",
       " ('[SEP]', '[entSEP]', array([0])),\n",
       " ('[PAD]', '[entPAD]', array([0])),\n",
       " ('[PAD]', '[entPAD]', array([0])),\n",
       " ('[PAD]', '[entPAD]', array([0])),\n",
       " ('[PAD]', '[entPAD]', array([0])),\n",
       " ('[PAD]', '[entPAD]', array([0])),\n",
       " ('[PAD]', '[entPAD]', array([0])),\n",
       " ('[PAD]', '[entPAD]', array([0])),\n",
       " ('[PAD]', '[entPAD]', array([0])),\n",
       " ('[PAD]', '[entPAD]', array([0])),\n",
       " ('[PAD]', '[entPAD]', array([0])),\n",
       " ('[PAD]', '[entPAD]', array([0])),\n",
       " ('[PAD]', '[entPAD]', array([0])),\n",
       " ('[PAD]', '[entPAD]', array([0])),\n",
       " ('[PAD]', '[entPAD]', array([0])),\n",
       " ('[PAD]', '[entPAD]', array([0])),\n",
       " ('[PAD]', '[entPAD]', array([0])),\n",
       " ('[PAD]', '[entPAD]', array([0])),\n",
       " ('[PAD]', '[entPAD]', array([0])),\n",
       " ('[PAD]', '[entPAD]', array([0])),\n",
       " ('[PAD]', '[entPAD]', array([0])),\n",
       " ('[PAD]', '[entPAD]', array([0]))]"
      ]
     },
     "execution_count": 509,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(entity_sentences[0],entity_annotations[0],entity_nerTag_ids[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Duma** is the entity we need to match in this example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_entity_sequences(sequence_length, nerTag_ids, ner_sentences):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        sequence_length - how many tokens to the left and right of the identified entity\n",
    "        nerTag_ids - array of NER tag categories, i.e. 0 = 'O', 1 = 'B', 2 = 'I'\n",
    "        ner_sentences - list of tokenized sentences corresponding to NER tags \n",
    "        \n",
    "    Outputs: for each identified entity, output tokenized text surrounding the entity of length = sequence_length\n",
    "    \"\"\"\n",
    "\n",
    "    mention_sequences = [] # text sub-sequences for each identified mention\n",
    "    entity_indexes = [] # used to index the original sequence for each identified entity mention\n",
    "    entity_subseq_indexes = [] # used to index the sub-sequence for each identified mention\n",
    "    \n",
    "    # list which gives the entity number within the original sequence\n",
    "    # required for indexing sub-sequences with multiple entities\n",
    "    entity_number_list = []\n",
    "    entity_number = 0\n",
    "    entity_counter = 0\n",
    "    for i in range(len(nerTag_ids)):\n",
    "        if nerTag_ids[i] > [0] and entity_counter == 0:\n",
    "            entity_number += 1\n",
    "            entity_counter += 1\n",
    "            entity_number_list.append(entity_number)\n",
    "        elif nerTag_ids[i] > [0]:\n",
    "            entity_number_list.append(entity_number)\n",
    "        elif nerTag_ids[i] == [0] and entity_counter > 0:\n",
    "            entity_number_list.append(0)\n",
    "            entity_counter = 0\n",
    "        else:\n",
    "            entity_number_list.append(0)   \n",
    "    \n",
    "    zipped_nerTag_ids = list(zip(nerTag_ids, entity_number_list))\n",
    "    \n",
    "    entity_counter = 0 # reset entity counter\n",
    "    total_entities = 0\n",
    "    \n",
    "    for i in range(len(nerTag_ids)):\n",
    "        if nerTag_ids[i] == [0] and entity_counter > 0: # i.e. reached an 'other' tag following identified entities\n",
    "            ent_index_end = i # mark the ending index of the entity mention\n",
    "            \n",
    "            seq_start = max(0, i - entity_counter - sequence_length)\n",
    "            seq_end = min(i+sequence_length, len(nerTag_ids))\n",
    "\n",
    "            if seq_start == 0: # i.e. the seqence starts at beginning of sentence, so use extra tokens\n",
    "                extra_tokens = abs(i - entity_counter - sequence_length)\n",
    "                if seq_end < len(nerTag_ids):\n",
    "                    seq_end = min(seq_end + extra_tokens, len(nerTag_ids))\n",
    "\n",
    "            if seq_end == len(nerTag_ids): # i.e. the sequence ends at the end of the sentence, so use extra tokens\n",
    "                extra_tokens = sequence_length - (len(sequence_length) - i + 1)\n",
    "                if seq_start > 0:\n",
    "                    seq_start = max(seq_start - extra_tokens, 0)\n",
    "\n",
    "            total_entities += 1\n",
    "            \n",
    "            mention_sequences.append(ner_sentences[seq_start : seq_end])\n",
    "            entity_indexes.append([ent_index_start, ent_index_end])\n",
    "            \n",
    "            # append the sub-sequence indexing for the entity mention\n",
    "            entity_counter = 0 # reset entity token counter to use in sub-sequence\n",
    "            ent_subseq_idx_end = False\n",
    "            \n",
    "            for j in range(len(zipped_nerTag_ids[seq_start : seq_end])):\n",
    "                if zipped_nerTag_ids[seq_start : seq_end][j][1] == total_entities and entity_counter == 0:\n",
    "                    ent_subseq_idx_start = j\n",
    "                    ent_subseq_idx_end = True # activate variable for end-index value\n",
    "                    entity_counter += 1\n",
    "                        \n",
    "                if zipped_nerTag_ids[seq_start : seq_end][j][0] == [0] and ent_subseq_idx_end:\n",
    "                    ent_subseq_idx_end = j\n",
    "                    entity_subseq_indexes.append([ent_subseq_idx_start,ent_subseq_idx_end])\n",
    "                    entity_counter = 0 # reset entity token counter again for full sequence\n",
    "                    ent_subseq_idx_end = False # switch end-index variable back off\n",
    "\n",
    "\n",
    "            entity_counter = 0 # reset entity token counter again for full sequence\n",
    "\n",
    "        elif nerTag_ids[i] > [0]: # i.e. identified the start of an entity token sequence\n",
    "            if entity_counter == 0: \n",
    "                ent_index_start = i # mark the starting index of the entity mention\n",
    "            \n",
    "            entity_counter += 1\n",
    "            \n",
    "    return mention_sequences, entity_indexes, entity_subseq_indexes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [],
   "source": [
    "mention_sequences, ent_idx, ent_subseq_idx = create_entity_sequences(sequence_length = 5, \n",
    "                                                                     nerTag_ids = entity_nerTag_ids[0], \n",
    "                                                                     ner_sentences = entity_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Liberal',\n",
       " 'Democratic',\n",
       " 'party',\n",
       " 'and',\n",
       " 'the',\n",
       " 'Russian',\n",
       " 'Du',\n",
       " '##ma',\n",
       " '(',\n",
       " 'parliament',\n",
       " ')',\n",
       " '[SEP]',\n",
       " '[PAD]']"
      ]
     },
     "execution_count": 511,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mention_sequences[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 8]"
      ]
     },
     "execution_count": 512,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ent_subseq_idx[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 513,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mention_sequences) == len(ent_idx) == len(ent_subseq_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_sentence_ids = np.asarray(tokenizer.convert_tokens_to_ids(['[CLS]'] + mention_sequences[4]))\n",
    "entity_masks = np.asarray(([1] * (len(entity_sentence_ids)-2)) + ([0] * 2))\n",
    "entity_seq_ids = np.asarray([1] * len(entity_sentence_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "bert_sentence_inputs = dict(\n",
    "    input_ids=np.reshape(entity_sentence_ids,(1,14)),\n",
    "    input_mask=np.reshape(entity_masks,(1,14)),\n",
    "    segment_ids=np.reshape(entity_seq_ids,(1,14))\n",
    ")\n",
    "\n",
    "# use sequence_output so that each token has a 768-dimensional vector\n",
    "bert_sentence_outputs = bert_module(bert_sentence_inputs, signature=\"tokens\", as_dict=True)[\"sequence_output\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize_vars(sess)\n",
    "\n",
    "bert_sentence_arr = bert_sentence_outputs.eval(session=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 14, 768)"
      ]
     },
     "execution_count": 517,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_sentence_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = []\n",
    "ent_weight = 8\n",
    "other_weight = 0\n",
    "\n",
    "for i in range(len(mention_sequences[4])+1):\n",
    "    if i in range(ent_subseq_idx[4][0], ent_subseq_idx[4][1]):\n",
    "        weights.append(ent_weight)\n",
    "    else:\n",
    "        weights.append(other_weight)\n",
    "        \n",
    "weights = np.asarray(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 768)"
      ]
     },
     "execution_count": 519,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_test1 = np.average(bert_sentence_arr, axis=1, weights = weights)\n",
    "avg_test1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_entity_nn(mention_arr, entities_arr, entity_list, k=1):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    cos_sim_list = []\n",
    "    for i in range(len(entities_arr)):\n",
    "        cos_sim = np.dot(mention_arr, entities_arr[i]) / (np.linalg.norm(mention_arr) * np.linalg.norm(entities_arr[i]))\n",
    "        cos_sim_list.append(cos_sim[0])\n",
    "\n",
    "    top_index = np.argsort(np.asarray(cos_sim_list))[-k:]\n",
    "\n",
    "    result = []\n",
    "    for item in top_index:\n",
    "        result.append((entity_list[item], cos_sim_list[item]))\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 485,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The Championship (TV programme)', 0.008171264037257287),\n",
       " ('Duma', 0.01670063799202193)]"
      ]
     },
     "execution_count": 520,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_entity_nn(avg_test1, bert_entity_arr, output_entities, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim_list = []\n",
    "for i in range(len(bert_entity_arr)):\n",
    "    cos_sim = np.dot(avg_test1, bert_entity_arr[i]) / (np.linalg.norm(avg_test1) * np.linalg.norm(bert_entity_arr[i]))\n",
    "    cos_sim_list.append(cos_sim[0])\n",
    "\n",
    "top_index = np.argsort(np.asarray(cos_sim_list))[-2:]\n",
    "\n",
    "result = []\n",
    "for item in top_index:\n",
    "    result.append((output_entities[item], cos_sim_list[item]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The Championship (TV programme)', 'Duma']"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0], dtype=int64)"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Duma'"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_entities[41]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A duma () was a Russian assembly with advisory or legislative functions. The term comes from the Russian verb  (dumat) meaning \"to think\" or \"to consider\". The first formally constituted duma was the State Duma introduced into the Russian Empire by Tsar Nicholas II in 1905 after the revol'"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikipedia.summary('Duma')[:300]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-large/5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_test0 = ' '.join(tokenizer.convert_ids_to_tokens(entity_summary_ids[0])).replace(' ##','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_test1 = ' '.join(tokenizer.convert_ids_to_tokens(entity_summary_ids[1])).replace(' ##','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [],
   "source": [
    "mention_test = ' '.join(mention_sequences[4]).replace(' ##','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_embeddings = embed([\n",
    "    summary_test1,\n",
    "    summary_test0,\n",
    "    mention_test\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize_vars(sess)\n",
    "\n",
    "test_embeddings_arr = test_embeddings.eval(session=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 512)"
      ]
     },
     "execution_count": 545,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_embeddings_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.31530818"
      ]
     },
     "execution_count": 546,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.inner(test_embeddings_arr[0], test_embeddings_arr[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.07048488"
      ]
     },
     "execution_count": 547,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.inner(test_embeddings_arr[1], test_embeddings_arr[2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
