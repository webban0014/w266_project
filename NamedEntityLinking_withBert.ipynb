{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Linking with BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using BERT embeddings to aid in the task of End-to-End Named Entity Linking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 - Read in the annotations and training dataset from AIDA-YAGO2 as well as the Wiki Entity Name/Id dataset\n",
    "\n",
    "These tab-seperated files will be used to train our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_file = 'C:\\\\Users\\\\andre\\\\Documents\\\\Berkeley\\\\w266\\\\w266_Project\\\\data\\\\AIDA-YAGO2-annotations.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train mentions:  25288\n",
      "validation mentions:  6349\n",
      "test mentions:  6078\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "\n",
    "train_annotations = []\n",
    "val_annotations = []\n",
    "test_annotations = []\n",
    "\n",
    "with open(annotations_file, encoding='utf-8') as tsvfile:\n",
    "    reader = csv.reader(tsvfile, delimiter='\\t')\n",
    "    i = 0\n",
    "    for row in reader:\n",
    "        i += 1\n",
    "        train_annotations.append(row)\n",
    "        if row == ['-DOCSTART- (947testa CRICKET)']: # first validation document\n",
    "            #print('validation data starts at index:', i-1)\n",
    "            val_index = i-1\n",
    "        if row == ['-DOCSTART- (1163testb SOCCER)']: # first test document\n",
    "            #print('test data starts at index:', i-1)\n",
    "            test_index = i-1\n",
    "    \n",
    "    val_annotations = train_annotations[val_index: test_index]\n",
    "    test_annotations = train_annotations[test_index:]\n",
    "    train_annotations = train_annotations[:val_index]\n",
    "    \n",
    "print('train mentions: ', len(train_annotations))\n",
    "print('validation mentions: ', len(val_annotations))\n",
    "print('test mentions: ', len(test_annotations))            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = 'C:\\\\Users\\\\andre\\\\Documents\\\\Berkeley\\\\w266\\\\w266_Project\\\\data\\\\basic_data\\\\test_datasets\\\\AIDA\\\\aida_train_unquoted.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['-DOCSTART- (1 EU)'],\n",
       " ['EU', 'B', 'EU', '--NME--'],\n",
       " ['rejects'],\n",
       " ['German',\n",
       "  'B',\n",
       "  'German',\n",
       "  'Germany',\n",
       "  'http://en.wikipedia.org/wiki/Germany',\n",
       "  '11867',\n",
       "  '/m/0345h'],\n",
       " ['call'],\n",
       " ['to'],\n",
       " ['boycott'],\n",
       " ['British',\n",
       "  'B',\n",
       "  'British',\n",
       "  'United_Kingdom',\n",
       "  'http://en.wikipedia.org/wiki/United_Kingdom',\n",
       "  '31717',\n",
       "  '/m/07ssc'],\n",
       " ['lamb'],\n",
       " ['.'],\n",
       " [],\n",
       " ['Peter', 'B', 'Peter Blackburn', '--NME--'],\n",
       " ['Blackburn', 'I', 'Peter Blackburn', '--NME--'],\n",
       " [],\n",
       " ['BRUSSELS',\n",
       "  'B',\n",
       "  'BRUSSELS',\n",
       "  'Brussels',\n",
       "  'http://en.wikipedia.org/wiki/Brussels',\n",
       "  '3708',\n",
       "  '/m/0177z'],\n",
       " ['1996-08-22'],\n",
       " [],\n",
       " ['The'],\n",
       " ['European',\n",
       "  'B',\n",
       "  'European Commission',\n",
       "  'European_Commission',\n",
       "  'http://en.wikipedia.org/wiki/European_Commission',\n",
       "  '9974',\n",
       "  '/m/02q9k'],\n",
       " ['Commission',\n",
       "  'I',\n",
       "  'European Commission',\n",
       "  'European_Commission',\n",
       "  'http://en.wikipedia.org/wiki/European_Commission',\n",
       "  '9974',\n",
       "  '/m/02q9k'],\n",
       " ['said'],\n",
       " ['on'],\n",
       " ['Thursday'],\n",
       " ['it'],\n",
       " ['disagreed'],\n",
       " ['with'],\n",
       " ['German',\n",
       "  'B',\n",
       "  'German',\n",
       "  'Germany',\n",
       "  'http://en.wikipedia.org/wiki/Germany',\n",
       "  '11867',\n",
       "  '/m/0345h'],\n",
       " ['advice'],\n",
       " ['to'],\n",
       " ['consumers'],\n",
       " ['to'],\n",
       " ['shun'],\n",
       " ['British',\n",
       "  'B',\n",
       "  'British',\n",
       "  'United_Kingdom',\n",
       "  'http://en.wikipedia.org/wiki/United_Kingdom',\n",
       "  '31717',\n",
       "  '/m/07ssc'],\n",
       " ['lamb'],\n",
       " ['until'],\n",
       " ['scientists'],\n",
       " ['determine'],\n",
       " ['whether'],\n",
       " ['mad'],\n",
       " ['cow'],\n",
       " ['disease'],\n",
       " ['can'],\n",
       " ['be'],\n",
       " ['transmitted'],\n",
       " ['to'],\n",
       " ['sheep'],\n",
       " ['.'],\n",
       " [],\n",
       " ['Germany',\n",
       "  'B',\n",
       "  'Germany',\n",
       "  'Germany',\n",
       "  'http://en.wikipedia.org/wiki/Germany',\n",
       "  '11867',\n",
       "  '/m/0345h'],\n",
       " [\"'s\"],\n",
       " ['representative'],\n",
       " ['to'],\n",
       " ['the'],\n",
       " ['European',\n",
       "  'B',\n",
       "  'European Union',\n",
       "  'European_Union',\n",
       "  'http://en.wikipedia.org/wiki/European_Union',\n",
       "  '9317',\n",
       "  '/m/02jxk'],\n",
       " ['Union',\n",
       "  'I',\n",
       "  'European Union',\n",
       "  'European_Union',\n",
       "  'http://en.wikipedia.org/wiki/European_Union',\n",
       "  '9317',\n",
       "  '/m/02jxk'],\n",
       " [\"'s\"],\n",
       " ['veterinary'],\n",
       " ['committee'],\n",
       " ['Werner', 'B', 'Werner Zwingmann', '--NME--'],\n",
       " ['Zwingmann', 'I', 'Werner Zwingmann', '--NME--'],\n",
       " ['said'],\n",
       " ['on'],\n",
       " ['Wednesday'],\n",
       " ['consumers'],\n",
       " ['should'],\n",
       " ['buy'],\n",
       " ['sheepmeat'],\n",
       " ['from'],\n",
       " ['countries'],\n",
       " ['other'],\n",
       " ['than'],\n",
       " ['Britain',\n",
       "  'B',\n",
       "  'Britain',\n",
       "  'United_Kingdom',\n",
       "  'http://en.wikipedia.org/wiki/United_Kingdom',\n",
       "  '31717',\n",
       "  '/m/07ssc'],\n",
       " ['until'],\n",
       " ['the'],\n",
       " ['scientific'],\n",
       " ['advice'],\n",
       " ['was'],\n",
       " ['clearer'],\n",
       " ['.'],\n",
       " [],\n",
       " [],\n",
       " ['We'],\n",
       " ['do'],\n",
       " [\"n't\"],\n",
       " ['support'],\n",
       " ['any'],\n",
       " ['such'],\n",
       " ['recommendation'],\n",
       " ['because'],\n",
       " ['we'],\n",
       " ['do'],\n",
       " [\"n't\"],\n",
       " ['see'],\n",
       " ['any'],\n",
       " ['grounds'],\n",
       " ['for'],\n",
       " ['it'],\n",
       " [','],\n",
       " [],\n",
       " ['the'],\n",
       " ['Commission',\n",
       "  'B',\n",
       "  'Commission',\n",
       "  'European_Commission',\n",
       "  'http://en.wikipedia.org/wiki/European_Commission',\n",
       "  '9974',\n",
       "  '/m/02q9k'],\n",
       " [\"'s\"],\n",
       " ['chief'],\n",
       " ['spokesman'],\n",
       " ['Nikolaus', 'B', 'Nikolaus van der Pas', '--NME--'],\n",
       " ['van', 'I', 'Nikolaus van der Pas', '--NME--'],\n",
       " ['der', 'I', 'Nikolaus van der Pas', '--NME--'],\n",
       " ['Pas', 'I', 'Nikolaus van der Pas', '--NME--'],\n",
       " ['told'],\n",
       " ['a'],\n",
       " ['news'],\n",
       " ['briefing'],\n",
       " ['.'],\n",
       " [],\n",
       " ['He'],\n",
       " ['said'],\n",
       " ['further'],\n",
       " ['scientific'],\n",
       " ['study'],\n",
       " ['was'],\n",
       " ['required'],\n",
       " ['and'],\n",
       " ['if'],\n",
       " ['it'],\n",
       " ['was'],\n",
       " ['found'],\n",
       " ['that'],\n",
       " ['action'],\n",
       " ['was'],\n",
       " ['needed'],\n",
       " ['it'],\n",
       " ['should'],\n",
       " ['be'],\n",
       " ['taken'],\n",
       " ['by'],\n",
       " ['the'],\n",
       " ['European',\n",
       "  'B',\n",
       "  'European Union',\n",
       "  'European_Union',\n",
       "  'http://en.wikipedia.org/wiki/European_Union',\n",
       "  '9317',\n",
       "  '/m/02jxk'],\n",
       " ['Union',\n",
       "  'I',\n",
       "  'European Union',\n",
       "  'European_Union',\n",
       "  'http://en.wikipedia.org/wiki/European_Union',\n",
       "  '9317',\n",
       "  '/m/02jxk'],\n",
       " ['.'],\n",
       " [],\n",
       " ['He'],\n",
       " ['said'],\n",
       " ['a'],\n",
       " ['proposal'],\n",
       " ['last'],\n",
       " ['month'],\n",
       " ['by'],\n",
       " ['EU', 'B', 'EU', '--NME--'],\n",
       " ['Farm'],\n",
       " ['Commissioner'],\n",
       " ['Franz',\n",
       "  'B',\n",
       "  'Franz Fischler',\n",
       "  'Franz_Fischler',\n",
       "  'http://en.wikipedia.org/wiki/Franz_Fischler',\n",
       "  '626779',\n",
       "  '/m/02y4y1'],\n",
       " ['Fischler',\n",
       "  'I',\n",
       "  'Franz Fischler',\n",
       "  'Franz_Fischler',\n",
       "  'http://en.wikipedia.org/wiki/Franz_Fischler',\n",
       "  '626779',\n",
       "  '/m/02y4y1'],\n",
       " ['to'],\n",
       " ['ban'],\n",
       " ['sheep'],\n",
       " ['brains'],\n",
       " [','],\n",
       " ['spleens'],\n",
       " ['and'],\n",
       " ['spinal'],\n",
       " ['cords'],\n",
       " ['from'],\n",
       " ['the'],\n",
       " ['human'],\n",
       " ['and'],\n",
       " ['animal'],\n",
       " ['food'],\n",
       " ['chains'],\n",
       " ['was'],\n",
       " ['a'],\n",
       " ['highly'],\n",
       " ['specific'],\n",
       " ['and'],\n",
       " ['precautionary'],\n",
       " ['move'],\n",
       " ['to'],\n",
       " ['protect'],\n",
       " ['human'],\n",
       " ['health'],\n",
       " ['.'],\n",
       " [],\n",
       " ['Fischler', 'B', 'Fischler', '--NME--'],\n",
       " ['proposed'],\n",
       " ['EU-wide', 'B', 'EU-wide', '--NME--'],\n",
       " ['measures'],\n",
       " ['after'],\n",
       " ['reports'],\n",
       " ['from'],\n",
       " ['Britain',\n",
       "  'B',\n",
       "  'Britain',\n",
       "  'United_Kingdom',\n",
       "  'http://en.wikipedia.org/wiki/United_Kingdom',\n",
       "  '31717',\n",
       "  '/m/07ssc'],\n",
       " ['and'],\n",
       " ['France',\n",
       "  'B',\n",
       "  'France',\n",
       "  'France',\n",
       "  'http://en.wikipedia.org/wiki/France',\n",
       "  '5843419',\n",
       "  '/m/0f8l9c'],\n",
       " ['that'],\n",
       " ['under'],\n",
       " ['laboratory'],\n",
       " ['conditions'],\n",
       " ['sheep'],\n",
       " ['could'],\n",
       " ['contract'],\n",
       " ['Bovine', 'B', 'Bovine Spongiform Encephalopathy', '--NME--'],\n",
       " ['Spongiform', 'I', 'Bovine Spongiform Encephalopathy', '--NME--'],\n",
       " ['Encephalopathy', 'I', 'Bovine Spongiform Encephalopathy', '--NME--'],\n",
       " ['('],\n",
       " ['BSE',\n",
       "  'B',\n",
       "  'BSE',\n",
       "  'Bovine_spongiform_encephalopathy',\n",
       "  'http://en.wikipedia.org/wiki/Bovine_spongiform_encephalopathy',\n",
       "  '19344418',\n",
       "  '/m/0jy66'],\n",
       " [')'],\n",
       " ['--'],\n",
       " ['mad'],\n",
       " ['cow'],\n",
       " ['disease'],\n",
       " ['.'],\n",
       " [],\n",
       " ['But'],\n",
       " ['Fischler', 'B', 'Fischler', '--NME--'],\n",
       " ['agreed'],\n",
       " ['to'],\n",
       " ['review'],\n",
       " ['his'],\n",
       " ['proposal'],\n",
       " ['after'],\n",
       " ['the'],\n",
       " ['EU', 'B', 'EU', '--NME--'],\n",
       " [\"'s\"],\n",
       " ['standing'],\n",
       " ['veterinary'],\n",
       " ['committee'],\n",
       " [','],\n",
       " ['mational'],\n",
       " ['animal'],\n",
       " ['health'],\n",
       " ['officials'],\n",
       " [','],\n",
       " ['questioned'],\n",
       " ['if'],\n",
       " ['such'],\n",
       " ['action'],\n",
       " ['was'],\n",
       " ['justified'],\n",
       " ['as'],\n",
       " ['there'],\n",
       " ['was'],\n",
       " ['only'],\n",
       " ['a'],\n",
       " ['slight'],\n",
       " ['risk'],\n",
       " ['to'],\n",
       " ['human'],\n",
       " ['health'],\n",
       " ['.'],\n",
       " [],\n",
       " ['Spanish',\n",
       "  'B',\n",
       "  'Spanish',\n",
       "  'Spain',\n",
       "  'http://en.wikipedia.org/wiki/Spain',\n",
       "  '26667',\n",
       "  '/m/06mkj'],\n",
       " ['Farm'],\n",
       " ['Minister'],\n",
       " ['Loyola',\n",
       "  'B',\n",
       "  'Loyola de Palacio',\n",
       "  'Loyola_de_Palacio',\n",
       "  'http://en.wikipedia.org/wiki/Loyola_de_Palacio',\n",
       "  '6394317',\n",
       "  '/m/0g3rwc'],\n",
       " ['de',\n",
       "  'I',\n",
       "  'Loyola de Palacio',\n",
       "  'Loyola_de_Palacio',\n",
       "  'http://en.wikipedia.org/wiki/Loyola_de_Palacio',\n",
       "  '6394317',\n",
       "  '/m/0g3rwc'],\n",
       " ['Palacio',\n",
       "  'I',\n",
       "  'Loyola de Palacio',\n",
       "  'Loyola_de_Palacio',\n",
       "  'http://en.wikipedia.org/wiki/Loyola_de_Palacio',\n",
       "  '6394317',\n",
       "  '/m/0g3rwc'],\n",
       " ['had'],\n",
       " ['earlier'],\n",
       " ['accused'],\n",
       " ['Fischler', 'B', 'Fischler', '--NME--'],\n",
       " ['at'],\n",
       " ['an'],\n",
       " ['EU', 'B', 'EU', '--NME--'],\n",
       " ['farm'],\n",
       " ['ministers'],\n",
       " [\"'\"],\n",
       " ['meeting'],\n",
       " ['of'],\n",
       " ['causing'],\n",
       " ['unjustified'],\n",
       " ['alarm'],\n",
       " ['through'],\n",
       " [],\n",
       " ['dangerous'],\n",
       " ['generalisation'],\n",
       " ['.'],\n",
       " [],\n",
       " [],\n",
       " ['.'],\n",
       " [],\n",
       " ['Only'],\n",
       " ['France',\n",
       "  'B',\n",
       "  'France',\n",
       "  'France',\n",
       "  'http://en.wikipedia.org/wiki/France',\n",
       "  '5843419',\n",
       "  '/m/0f8l9c'],\n",
       " ['and'],\n",
       " ['Britain',\n",
       "  'B',\n",
       "  'Britain',\n",
       "  'United_Kingdom',\n",
       "  'http://en.wikipedia.org/wiki/United_Kingdom',\n",
       "  '31717',\n",
       "  '/m/07ssc'],\n",
       " ['backed'],\n",
       " ['Fischler', 'B', 'Fischler', '--NME--'],\n",
       " [\"'s\"],\n",
       " ['proposal'],\n",
       " ['.'],\n",
       " [],\n",
       " ['The'],\n",
       " ['EU', 'B', 'EU', '--NME--'],\n",
       " [\"'s\"],\n",
       " ['scientific'],\n",
       " ['veterinary'],\n",
       " ['and'],\n",
       " ['multidisciplinary'],\n",
       " ['committees'],\n",
       " ['are'],\n",
       " ['due'],\n",
       " ['to'],\n",
       " ['re-examine'],\n",
       " ['the'],\n",
       " ['issue'],\n",
       " ['early'],\n",
       " ['next'],\n",
       " ['month'],\n",
       " ['and'],\n",
       " ['make'],\n",
       " ['recommendations'],\n",
       " ['to'],\n",
       " ['the'],\n",
       " ['senior'],\n",
       " ['veterinary'],\n",
       " ['officials'],\n",
       " ['.'],\n",
       " [],\n",
       " ['Sheep'],\n",
       " ['have'],\n",
       " ['long'],\n",
       " ['been'],\n",
       " ['known'],\n",
       " ['to'],\n",
       " ['contract'],\n",
       " ['scrapie'],\n",
       " [','],\n",
       " ['a'],\n",
       " ['brain-wasting'],\n",
       " ['disease'],\n",
       " ['similar'],\n",
       " ['to'],\n",
       " ['BSE',\n",
       "  'B',\n",
       "  'BSE',\n",
       "  'Bovine_spongiform_encephalopathy',\n",
       "  'http://en.wikipedia.org/wiki/Bovine_spongiform_encephalopathy',\n",
       "  '19344418',\n",
       "  '/m/0jy66'],\n",
       " ['which'],\n",
       " ['is'],\n",
       " ['believed'],\n",
       " ['to'],\n",
       " ['have'],\n",
       " ['been'],\n",
       " ['transferred'],\n",
       " ['to'],\n",
       " ['cattle'],\n",
       " ['through'],\n",
       " ['feed'],\n",
       " ['containing'],\n",
       " ['animal'],\n",
       " ['waste'],\n",
       " ['.'],\n",
       " [],\n",
       " ['British',\n",
       "  'B',\n",
       "  'British',\n",
       "  'United_Kingdom',\n",
       "  'http://en.wikipedia.org/wiki/United_Kingdom',\n",
       "  '31717',\n",
       "  '/m/07ssc'],\n",
       " ['farmers'],\n",
       " ['denied'],\n",
       " ['on'],\n",
       " ['Thursday'],\n",
       " ['there'],\n",
       " ['was'],\n",
       " ['any'],\n",
       " ['danger'],\n",
       " ['to'],\n",
       " ['human'],\n",
       " ['health'],\n",
       " ['from'],\n",
       " ['their'],\n",
       " ['sheep'],\n",
       " [','],\n",
       " ['but'],\n",
       " ['expressed'],\n",
       " ['concern'],\n",
       " ['that'],\n",
       " ['German',\n",
       "  'B',\n",
       "  'German',\n",
       "  'Germany',\n",
       "  'http://en.wikipedia.org/wiki/Germany',\n",
       "  '11867',\n",
       "  '/m/0345h'],\n",
       " ['government'],\n",
       " ['advice'],\n",
       " ['to'],\n",
       " ['consumers'],\n",
       " ['to'],\n",
       " ['avoid'],\n",
       " ['British',\n",
       "  'B',\n",
       "  'British',\n",
       "  'United_Kingdom',\n",
       "  'http://en.wikipedia.org/wiki/United_Kingdom',\n",
       "  '31717',\n",
       "  '/m/07ssc'],\n",
       " ['lamb'],\n",
       " ['might'],\n",
       " ['influence'],\n",
       " ['consumers'],\n",
       " ['across'],\n",
       " ['Europe',\n",
       "  'B',\n",
       "  'Europe',\n",
       "  'Europe',\n",
       "  'http://en.wikipedia.org/wiki/Europe',\n",
       "  '9239',\n",
       "  '/m/02j9z'],\n",
       " ['.'],\n",
       " [],\n",
       " [],\n",
       " ['What'],\n",
       " ['we'],\n",
       " ['have'],\n",
       " ['to'],\n",
       " ['be'],\n",
       " ['extremely'],\n",
       " ['careful'],\n",
       " ['of'],\n",
       " ['is'],\n",
       " ['how'],\n",
       " ['other'],\n",
       " ['countries'],\n",
       " ['are'],\n",
       " ['going'],\n",
       " ['to'],\n",
       " ['take'],\n",
       " ['Germany',\n",
       "  'B',\n",
       "  'Germany',\n",
       "  'Germany',\n",
       "  'http://en.wikipedia.org/wiki/Germany',\n",
       "  '11867',\n",
       "  '/m/0345h'],\n",
       " [\"'s\"],\n",
       " ['lead'],\n",
       " [','],\n",
       " [],\n",
       " ['Welsh', 'B', \"Welsh National Farmers ' Union\", '--NME--'],\n",
       " ['National', 'I', \"Welsh National Farmers ' Union\", '--NME--'],\n",
       " ['Farmers', 'I', \"Welsh National Farmers ' Union\", '--NME--'],\n",
       " [\"'\", 'I', \"Welsh National Farmers ' Union\", '--NME--'],\n",
       " ['Union', 'I', \"Welsh National Farmers ' Union\", '--NME--'],\n",
       " ['('],\n",
       " ['NFU', 'B', 'NFU', '--NME--'],\n",
       " [')'],\n",
       " ['chairman'],\n",
       " ['John', 'B', 'John Lloyd Jones', '--NME--'],\n",
       " ['Lloyd', 'I', 'John Lloyd Jones', '--NME--'],\n",
       " ['Jones', 'I', 'John Lloyd Jones', '--NME--'],\n",
       " ['said'],\n",
       " ['on'],\n",
       " ['BBC', 'B', 'BBC radio', '--NME--'],\n",
       " ['radio', 'I', 'BBC radio', '--NME--'],\n",
       " ['.'],\n",
       " [],\n",
       " ['Bonn',\n",
       "  'B',\n",
       "  'Bonn',\n",
       "  'Bonn',\n",
       "  'http://en.wikipedia.org/wiki/Bonn',\n",
       "  '3295',\n",
       "  '/m/0150n'],\n",
       " ['has'],\n",
       " ['led'],\n",
       " ['efforts'],\n",
       " ['to'],\n",
       " ['protect'],\n",
       " ['public'],\n",
       " ['health'],\n",
       " ['after'],\n",
       " ['consumer'],\n",
       " ['confidence'],\n",
       " ['collapsed'],\n",
       " ['in'],\n",
       " ['March'],\n",
       " ['after'],\n",
       " ['a'],\n",
       " ['British',\n",
       "  'B',\n",
       "  'British',\n",
       "  'United_Kingdom',\n",
       "  'http://en.wikipedia.org/wiki/United_Kingdom',\n",
       "  '31717',\n",
       "  '/m/07ssc'],\n",
       " ['report'],\n",
       " ['suggested'],\n",
       " ['humans'],\n",
       " ['could'],\n",
       " ['contract'],\n",
       " ['an'],\n",
       " ['illness'],\n",
       " ['similar'],\n",
       " ['to'],\n",
       " ['mad'],\n",
       " ['cow'],\n",
       " ['disease'],\n",
       " ['by'],\n",
       " ['eating'],\n",
       " ['contaminated'],\n",
       " ['beef'],\n",
       " ['.'],\n",
       " [],\n",
       " ['Germany',\n",
       "  'B',\n",
       "  'Germany',\n",
       "  'Germany',\n",
       "  'http://en.wikipedia.org/wiki/Germany',\n",
       "  '11867',\n",
       "  '/m/0345h'],\n",
       " ['imported'],\n",
       " ['47,600'],\n",
       " ['sheep'],\n",
       " ['from'],\n",
       " ['Britain',\n",
       "  'B',\n",
       "  'Britain',\n",
       "  'United_Kingdom',\n",
       "  'http://en.wikipedia.org/wiki/United_Kingdom',\n",
       "  '31717',\n",
       "  '/m/07ssc'],\n",
       " ['last'],\n",
       " ['year'],\n",
       " [','],\n",
       " ['nearly'],\n",
       " ['half'],\n",
       " ['of'],\n",
       " ['total'],\n",
       " ['imports'],\n",
       " ['.'],\n",
       " [],\n",
       " ['It'],\n",
       " ['brought'],\n",
       " ['in'],\n",
       " ['4,275'],\n",
       " ['tonnes'],\n",
       " ['of'],\n",
       " ['British',\n",
       "  'B',\n",
       "  'British',\n",
       "  'United_Kingdom',\n",
       "  'http://en.wikipedia.org/wiki/United_Kingdom',\n",
       "  '31717',\n",
       "  '/m/07ssc'],\n",
       " ['mutton'],\n",
       " [','],\n",
       " ['some'],\n",
       " ['10'],\n",
       " ['percent'],\n",
       " ['of'],\n",
       " ['overall'],\n",
       " ['imports'],\n",
       " ['.'],\n",
       " []]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_raw = []\n",
    "\n",
    "with open(train_file, encoding='utf-8') as train_tsvfile:\n",
    "    train_reader = csv.reader(train_tsvfile, delimiter='\\t')\n",
    "    for row in train_reader:\n",
    "        train_data_raw.append(row)\n",
    "    \n",
    "train_data_raw[:490] # just show up to the second document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create raw validation and test datasets\n",
    "test_file = 'C:\\\\Users\\\\andre\\\\Documents\\\\Berkeley\\\\w266\\\\w266_project\\\\data\\\\basic_data\\\\test_datasets\\\\AIDA\\\\testa_testb_aggregate_unquoted.txt'\n",
    "\n",
    "validation_data_raw = []\n",
    "test_data_raw = []\n",
    "\n",
    "with open(test_file, encoding='utf-8') as test_tsvfile:\n",
    "    test_reader = csv.reader(test_tsvfile, delimiter='\\t')\n",
    "    for row in test_reader:\n",
    "        validation_data_raw.append(row)\n",
    "\n",
    "for i in range(len(validation_data_raw)):\n",
    "    if validation_data_raw[i] == ['-DOCSTART- (1163testb SOCCER)']: # first test document\n",
    "        test_index = i\n",
    "        \n",
    "test_data_raw = validation_data_raw[test_index:]\n",
    "validation_data_raw = validation_data_raw[:test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikiName_id_mapFile = 'C:\\\\Users\\\\andre\\\\Documents\\\\Berkeley\\\\w266\\\\w266_project\\\\data\\\\basic_data\\\\wiki_name_id_map.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'United Kingdom'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(wikiName_id_mapFile, encoding='utf-8') as wikiMap_tsvfile:\n",
    "    wikiMap_reader = csv.reader(wikiMap_tsvfile, delimiter='\\t')\n",
    "    wikiName_id_map_dict = {int(row[1]): row[0] for row in wikiMap_reader if row[0][0].isalpha() or row[0][0].isnumeric()}\n",
    "\n",
    "wikiName_id_map_dict[31717]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2 - Pre-process the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules that will be used downstream\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import time\n",
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.python.keras.layers import Lambda\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.backend import sparse_categorical_crossentropy\n",
    "from tensorflow.keras.layers import Dense, TimeDistributed\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_bert_path = 'C:\\\\Users\\\\andre\\\\Documents\\\\Berkeley\\\\w266\\\\w266_project\\\\bert'\n",
    "# local path where the bert module has been cloned from git\n",
    "\n",
    "# make sure that the paths are accessible within the notebook\n",
    "sys.path.insert(0,local_bert_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\andre\\Documents\\Berkeley\\w266\\w266_project\\bert\\optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import optimization\n",
    "import run_classifier\n",
    "import tokenization\n",
    "import run_classifier_with_tfhub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the bert model that is cased\n",
    "bert_url = \"https://tfhub.dev/google/bert_cased_L-12_H-768_A-12/1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a BERT tokenizer so that words can be tokenized for BERT input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\andre\\Documents\\Berkeley\\w266\\w266_project\\bert\\tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\andre\\Documents\\Berkeley\\w266\\w266_project\\bert\\tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bert_module = hub.Module(bert_url)\n",
    "tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
    "\n",
    "vocab_file, do_lower_case = tf.Session().run([tokenization_info[\"vocab_file\"],\n",
    "                                            tokenization_info[\"do_lower_case\"]])\n",
    "\n",
    "tokenizer = tokenization.FullTokenizer(\n",
    "    vocab_file = vocab_file, do_lower_case = do_lower_case\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'token', '##izer', ',', 'is', 'it', 'even', 'working', '?']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make sure the tokenizer works\n",
    "tokenizer_test = tokenizer.tokenize('This tokenizer, is it even working?')\n",
    "tokenizer_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1188, 22559, 17260, 117, 1110, 1122, 1256, 1684, 136]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_ids_test = tokenizer.convert_tokens_to_ids(tokenizer_test)\n",
    "tokenizer_ids_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'token', '##izer', ',', 'is', 'it', 'even', 'working', '?']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(tokenizer_ids_test)\n",
    "# seems legit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the BERT tokenizer can be used to create tokens from the input text that can be used by the BERT model (more on that later).\n",
    "\n",
    "To simplify things, create a function that creates dictionary objects for document text, NER tags, entity Ids, and wikipedia URLs given row-by-row data (from training or test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "218505"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createUtilityDicts(data_rows):\n",
    "\n",
    "    data_text_dict = {}\n",
    "    data_nerTag_dict = {}\n",
    "    data_entityId_dict = {}\n",
    "    data_wikiUrl_dict = {}\n",
    "\n",
    "    # set the first DOCSTART of the train data as the dict key\n",
    "    dict_key = data_rows[0][0]\n",
    "    # create empty lists for dict values\n",
    "    text_dict_values = []\n",
    "    nerTag_dict_values = []\n",
    "    entityId_dict_values = []\n",
    "    wikiUrl_dict_values = []\n",
    "\n",
    "    for row in data_rows[1:]:\n",
    "\n",
    "        if row == []: #signaling end of sentence\n",
    "            text_dict_values.append('[SEP]')\n",
    "            nerTag_dict_values.append('[nerSEP]')\n",
    "            entityId_dict_values.append('[entSEP]')\n",
    "            wikiUrl_dict_values.append('[urlSEP]')\n",
    "\n",
    "        elif len(row[0]) > 9 and row[0][:10] == '-DOCSTART-': # signaling a new document\n",
    "            data_text_dict.update({dict_key : text_dict_values})\n",
    "            data_nerTag_dict.update({dict_key : nerTag_dict_values})\n",
    "            data_entityId_dict.update({dict_key : entityId_dict_values})\n",
    "            data_wikiUrl_dict.update({dict_key : wikiUrl_dict_values})\n",
    "\n",
    "            # reset key and value objects\n",
    "            dict_key = row[0]\n",
    "            text_dict_values = []\n",
    "            nerTag_dict_values = []\n",
    "            entityId_dict_values = []\n",
    "            wikiUrl_dict_values = []\n",
    "\n",
    "        elif len(row) > 1: # i.e. there is an entity mention\n",
    "            for i in range(len(row)): # skip the possibly variable number of wiki names\n",
    "                if row[i][:4] == 'http' or row[i] == '--NME--':\n",
    "                    wikiUrl = row[i]                \n",
    "                    if row[i][:4] == 'http': # only for cases where there is a wiki url\n",
    "                        entityId = row[i-1] # append the wiki entity name, which precedes the url\n",
    "                    else:\n",
    "                        entityId = '--NME--'\n",
    "            \n",
    "            for i in range(len(tokenizer.tokenize(row[0]))): \n",
    "                text_dict_values.append(tokenizer.tokenize(row[0])[i]) # append all tokenized strings\n",
    "                nerTag_dict_values.append(row[1]) # append same NER tag for each token\n",
    "                wikiUrl_dict_values.append(wikiUrl) # append same Wiki URL for each token\n",
    "                entityId_dict_values.append(entityId) # append same enitity Id for each token\n",
    "\n",
    "        else: # i.e. there is no entity mention\n",
    "            for i in range(len(tokenizer.tokenize(row[0]))): # append all tokenized strings\n",
    "                text_dict_values.append(tokenizer.tokenize(row[0])[i])\n",
    "                if i == 0:\n",
    "                    nerTag_dict_values.append('O')\n",
    "                else:\n",
    "                    nerTag_dict_values.append('nerX')\n",
    "                entityId_dict_values.append(None)\n",
    "                wikiUrl_dict_values.append(None)\n",
    "\n",
    "    return data_text_dict, data_nerTag_dict, data_entityId_dict, data_wikiUrl_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_dict, train_nerTag_dict, train_entityId_dict, train_wikiUrl_dict = createUtilityDicts(train_data_raw)\n",
    "\n",
    "val_text_dict, val_nerTag_dict, val_entityId_dict, val_wikiUrl_dict = createUtilityDicts(validation_data_raw)\n",
    "\n",
    "test_text_dict, test_nerTag_dict, test_entityId_dict, test_wikiUrl_dict = createUtilityDicts(test_data_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check some descriptive statistics on the sentence lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentenceLengths = []\n",
    "\n",
    "for sentences in train_text_dict.values():\n",
    "    i = 0\n",
    "    for token in sentences:\n",
    "        if token != '[SEP]':\n",
    "            i += 1\n",
    "        else:\n",
    "            sentenceLengths.append(i)\n",
    "            i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence mean 16.784703253041968\n",
      "max sentence length 171\n"
     ]
    }
   ],
   "source": [
    "print('sentence mean', np.mean(sentenceLengths))\n",
    "print('max sentence length', max(sentenceLengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.18e+03, 2.09e+02, 4.28e+02, 4.72e+02, 4.55e+02, 4.75e+02,\n",
       "        4.29e+02, 7.31e+02, 9.63e+02, 9.36e+02, 7.56e+02, 7.30e+02,\n",
       "        6.76e+02, 5.10e+02, 4.33e+02, 3.84e+02, 3.64e+02, 3.58e+02,\n",
       "        2.68e+02, 2.77e+02, 2.93e+02, 2.98e+02, 2.60e+02, 2.62e+02,\n",
       "        2.50e+02, 2.51e+02, 2.33e+02, 2.33e+02, 2.24e+02, 2.11e+02,\n",
       "        1.94e+02, 1.99e+02, 1.87e+02, 1.88e+02, 1.53e+02, 1.68e+02,\n",
       "        1.36e+02, 1.42e+02, 1.25e+02, 1.27e+02, 7.90e+01, 8.30e+01,\n",
       "        8.70e+01, 7.40e+01, 7.90e+01, 6.70e+01, 6.30e+01, 4.10e+01,\n",
       "        5.20e+01, 5.40e+01, 3.10e+01, 3.30e+01, 2.10e+01, 3.40e+01,\n",
       "        2.40e+01, 2.10e+01, 9.00e+00, 1.30e+01, 9.00e+00, 6.00e+00,\n",
       "        9.00e+00, 7.00e+00, 4.00e+00, 7.00e+00, 5.00e+00, 1.00e+00,\n",
       "        2.00e+00, 1.00e+00, 1.00e+00, 6.00e+00, 1.00e+00, 3.00e+00,\n",
       "        2.00e+00, 1.00e+00, 1.00e+00, 2.00e+00, 0.00e+00, 1.00e+00,\n",
       "        0.00e+00, 0.00e+00, 1.00e+00, 1.00e+00, 0.00e+00, 0.00e+00,\n",
       "        1.00e+00, 0.00e+00, 1.00e+00, 0.00e+00, 0.00e+00, 1.00e+00,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "        0.00e+00, 1.00e+00]),\n",
       " array([  0.        ,   1.00588235,   2.01176471,   3.01764706,\n",
       "          4.02352941,   5.02941176,   6.03529412,   7.04117647,\n",
       "          8.04705882,   9.05294118,  10.05882353,  11.06470588,\n",
       "         12.07058824,  13.07647059,  14.08235294,  15.08823529,\n",
       "         16.09411765,  17.1       ,  18.10588235,  19.11176471,\n",
       "         20.11764706,  21.12352941,  22.12941176,  23.13529412,\n",
       "         24.14117647,  25.14705882,  26.15294118,  27.15882353,\n",
       "         28.16470588,  29.17058824,  30.17647059,  31.18235294,\n",
       "         32.18823529,  33.19411765,  34.2       ,  35.20588235,\n",
       "         36.21176471,  37.21764706,  38.22352941,  39.22941176,\n",
       "         40.23529412,  41.24117647,  42.24705882,  43.25294118,\n",
       "         44.25882353,  45.26470588,  46.27058824,  47.27647059,\n",
       "         48.28235294,  49.28823529,  50.29411765,  51.3       ,\n",
       "         52.30588235,  53.31176471,  54.31764706,  55.32352941,\n",
       "         56.32941176,  57.33529412,  58.34117647,  59.34705882,\n",
       "         60.35294118,  61.35882353,  62.36470588,  63.37058824,\n",
       "         64.37647059,  65.38235294,  66.38823529,  67.39411765,\n",
       "         68.4       ,  69.40588235,  70.41176471,  71.41764706,\n",
       "         72.42352941,  73.42941176,  74.43529412,  75.44117647,\n",
       "         76.44705882,  77.45294118,  78.45882353,  79.46470588,\n",
       "         80.47058824,  81.47647059,  82.48235294,  83.48823529,\n",
       "         84.49411765,  85.5       ,  86.50588235,  87.51176471,\n",
       "         88.51764706,  89.52352941,  90.52941176,  91.53529412,\n",
       "         92.54117647,  93.54705882,  94.55294118,  95.55882353,\n",
       "         96.56470588,  97.57058824,  98.57647059,  99.58235294,\n",
       "        100.58823529, 101.59411765, 102.6       , 103.60588235,\n",
       "        104.61176471, 105.61764706, 106.62352941, 107.62941176,\n",
       "        108.63529412, 109.64117647, 110.64705882, 111.65294118,\n",
       "        112.65882353, 113.66470588, 114.67058824, 115.67647059,\n",
       "        116.68235294, 117.68823529, 118.69411765, 119.7       ,\n",
       "        120.70588235, 121.71176471, 122.71764706, 123.72352941,\n",
       "        124.72941176, 125.73529412, 126.74117647, 127.74705882,\n",
       "        128.75294118, 129.75882353, 130.76470588, 131.77058824,\n",
       "        132.77647059, 133.78235294, 134.78823529, 135.79411765,\n",
       "        136.8       , 137.80588235, 138.81176471, 139.81764706,\n",
       "        140.82352941, 141.82941176, 142.83529412, 143.84117647,\n",
       "        144.84705882, 145.85294118, 146.85882353, 147.86470588,\n",
       "        148.87058824, 149.87647059, 150.88235294, 151.88823529,\n",
       "        152.89411765, 153.9       , 154.90588235, 155.91176471,\n",
       "        156.91764706, 157.92352941, 158.92941176, 159.93529412,\n",
       "        160.94117647, 161.94705882, 162.95294118, 163.95882353,\n",
       "        164.96470588, 165.97058824, 166.97647059, 167.98235294,\n",
       "        168.98823529, 169.99411765, 171.        ]),\n",
       " <a list of 170 Patch objects>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(sentenceLengths, bins=170)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 - Generate input for the BERT model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because BERT has a limit of 512 tokens per input, I will try to split documents up into the component sentences, using the max sentence length of 50-75 for training. As can be seen by the histogram above, almost all sentences will fall into this range.\n",
    "\n",
    "Additionally, for each sentence we will need to add [CLS] and [PAD] tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will need **Sentences, NER Tokens,** and **entity ids**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createBertTextObjects(text_dict, nerTag_dict, entId_dict, max_sentence_length):\n",
    "    \"\"\"\n",
    "    Add [CLS] and [PAD] tokens to text, NER tag, and entity Id data \n",
    "    [PAD] tokens are added until max_sentence_length\n",
    "    returns bert_sentences, bert_ner_tags, bert_entities, bert_mask_ids\n",
    "    *Concepts borrowed from BERT_NER_v1\n",
    "    \"\"\"\n",
    "    max_sentence_length -= 1 # to account for Python's zero-based indexing\n",
    "    \n",
    "    bert_sentences = []\n",
    "    bert_ner_tags = []\n",
    "    bert_entities = []\n",
    "    bert_mask_ids = []\n",
    "    \n",
    "    # start with [CLS] token\n",
    "    sentence = ['[CLS]'] \n",
    "    ner_tags = ['[nerCLS]']\n",
    "    entity_ids = ['[entCLS]']\n",
    "    \n",
    "    for item in text_dict:\n",
    "    \n",
    "        for i in range(len(text_dict[item])):\n",
    "\n",
    "            if text_dict[item][i] == '[SEP]':\n",
    "                sentence_length = min(max_sentence_length, len(sentence))\n",
    "\n",
    "                # truncate sequences that are longer than the max_sentence_length\n",
    "                sentence = sentence[:max_sentence_length-1]\n",
    "                ner_tags = ner_tags[:max_sentence_length-1]\n",
    "                entity_ids = entity_ids[:max_sentence_length-1]\n",
    "\n",
    "                # mask all [PAD] items\n",
    "                bert_mask_ids.append([1] * (sentence_length + 1) + \n",
    "                                     [0] * (max_sentence_length - sentence_length))\n",
    "\n",
    "                # insert final [SEP] and [PAD] tokens\n",
    "                sentence += [text_dict[item][i]] + ['[PAD]'] * (max_sentence_length - len(sentence))\n",
    "                ner_tags += [nerTag_dict[item][i]] + ['[nerPAD]'] * (max_sentence_length - len(ner_tags))\n",
    "                entity_ids += [entId_dict[item][i]] + ['[entPAD]'] * (max_sentence_length - len(entity_ids))\n",
    "\n",
    "                bert_sentences.append(sentence)\n",
    "                bert_ner_tags.append(ner_tags)\n",
    "                bert_entities.append(entity_ids)\n",
    "\n",
    "                # restart with [CLS] token\n",
    "                sentence = ['[CLS]'] \n",
    "                ner_tags = ['[nerCLS]']\n",
    "                entity_ids = ['[entCLS]']\n",
    "            else:    \n",
    "                sentence += [text_dict[item][i]]\n",
    "                ner_tags += [nerTag_dict[item][i]]\n",
    "                entity_ids += [entId_dict[item][i]]\n",
    "            \n",
    "    bert_mask_ids = np.asarray(bert_mask_ids)\n",
    "            \n",
    "    return bert_sentences, bert_ner_tags, bert_entities, bert_mask_ids\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sentence_length = 50\n",
    "\n",
    "bert_sentences, bert_ner_tags, bert_entities, bert_mask_ids = createBertTextObjects(train_text_dict,\n",
    "                                                                       train_nerTag_dict,\n",
    "                                                                       train_entityId_dict,\n",
    "                                                                       max_sentence_length = max_sentence_length)\n",
    "\n",
    "val_sentences, val_ner_tags, val_entities, val_mask_ids = createBertTextObjects(val_text_dict,\n",
    "                                                                       val_nerTag_dict,\n",
    "                                                                       val_entityId_dict,\n",
    "                                                                       max_sentence_length = max_sentence_length)\n",
    "\n",
    "test_sentences, test_ner_tags, test_entities, test_mask_ids = createBertTextObjects(test_text_dict,\n",
    "                                                                       test_nerTag_dict,\n",
    "                                                                       test_entityId_dict,\n",
    "                                                                       max_sentence_length = max_sentence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our BERT inputs as text, we need to modify all inputs to be numerical values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approach 1 - NER tags as categorical codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createBertNumObjects1(bert_sentences, bert_ner_tags):\n",
    "    \"\"\"\n",
    "    Converts BERT text objects created from the createBertTextObjects output into numerical arrays\n",
    "    \"\"\"\n",
    "    \n",
    "    bert_sentence_ids = []\n",
    "    \n",
    "    # convert sentence text to token ids using tokenizer\n",
    "    for sentence in bert_sentences:\n",
    "        bert_sentence_ids.append(tokenizer.convert_tokens_to_ids(sentence))\n",
    "        \n",
    "    bert_sentence_ids = np.asarray(bert_sentence_ids, dtype=np.int32)\n",
    "        \n",
    "    # convert NER tag text to categorical codes\n",
    "    nerTag_categories = pd.DataFrame(np.array(bert_ner_tags).reshape(-1))\n",
    "    nerTag_categories.columns = ['text']\n",
    "    nerTag_categories.text = pd.Categorical(nerTag_categories.text)\n",
    "    nerTag_categories['cat'] = nerTag_categories.text.cat.codes\n",
    "    nerTag_categories['sym'] = nerTag_categories.cat \n",
    "    \n",
    "    nerDistribution = (nerTag_categories.groupby(['text','cat']).agg({'sym':'count'}).reset_index()\n",
    "                   .rename(columns={'sym':'occurences'}))\n",
    "    #print(nerDistribution)\n",
    "    \n",
    "    bert_nerTag_ids = np.array(nerTag_categories.cat, dtype=np.int32).reshape(len(bert_ner_tags), len(bert_ner_tags[0]))\n",
    "    \n",
    "    # create a sequence id array (which will just be all zero values)\n",
    "    bert_seq_ids = []\n",
    "\n",
    "    for i in range(len(bert_sentence_ids)):\n",
    "        seq_list = ([0] * len(bert_sentence_ids[i]))\n",
    "        bert_seq_ids.append(seq_list)\n",
    "\n",
    "    bert_seq_ids = np.asarray(bert_seq_ids)\n",
    "    \n",
    "    return bert_sentence_ids, bert_nerTag_ids, bert_seq_ids\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approach 2 - Binary tags (i.e. is entity or is not entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createBertNumObjects2(bert_sentences, bert_ner_tags):\n",
    "    \"\"\"\n",
    "    Converts BERT text objects created from the createBertTextObjects output into numerical arrays\n",
    "    \"\"\"\n",
    "    \n",
    "    bert_sentence_ids = []\n",
    "    \n",
    "    # convert sentence text to token ids using tokenizer\n",
    "    for sentence in bert_sentences:\n",
    "        bert_sentence_ids.append(tokenizer.convert_tokens_to_ids(sentence))\n",
    "        \n",
    "    bert_sentence_ids = np.asarray(bert_sentence_ids, dtype=np.int32)\n",
    "        \n",
    "    # convert NER tag text to binary 'is-entity' boolean\n",
    "    bert_nerTag_ids = []\n",
    "    for item in bert_ner_tags:\n",
    "        sentenceTags = []\n",
    "        for tag in item:\n",
    "            if tag in ['B','I']: # i.e. tag corresponds with an entity\n",
    "                sentenceTags.append([1])\n",
    "            else:\n",
    "                sentenceTags.append([0])\n",
    "        bert_nerTag_ids.append(sentenceTags)\n",
    "    \n",
    "    bert_nerTag_ids = np.asarray(bert_nerTag_ids)\n",
    "    \n",
    "    # create a sequence id array (which will just be all zero values)\n",
    "    bert_seq_ids = []\n",
    "\n",
    "    for i in range(len(bert_sentence_ids)):\n",
    "        seq_list = ([0] * len(bert_sentence_ids[i]))\n",
    "        bert_seq_ids.append(seq_list)\n",
    "\n",
    "    bert_seq_ids = np.asarray(bert_seq_ids)\n",
    "    \n",
    "    return bert_sentence_ids, bert_nerTag_ids, bert_seq_ids\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approach 3 - codes for only relevant tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createBertNumObjects3(bert_sentences, bert_ner_tags):\n",
    "    \"\"\"\n",
    "    Converts BERT text objects created from the createBertTextObjects output into numerical arrays\n",
    "    \"\"\"\n",
    "    \n",
    "    bert_sentence_ids = []\n",
    "    \n",
    "    # convert sentence text to token ids using tokenizer\n",
    "    for sentence in bert_sentences:\n",
    "        bert_sentence_ids.append(tokenizer.convert_tokens_to_ids(sentence))\n",
    "        \n",
    "    bert_sentence_ids = np.asarray(bert_sentence_ids, dtype=np.int32)\n",
    "        \n",
    "    # convert NER tag text to binary 'is-entity' boolean\n",
    "    bert_nerTag_ids = []\n",
    "    for item in bert_ner_tags:\n",
    "        sentenceTags = []\n",
    "        for tag in item:\n",
    "            if tag == 'B': # i.e. tag corresponds with an entity\n",
    "                sentenceTags.append([1])\n",
    "            elif tag == 'I':\n",
    "                sentenceTags.append([2])\n",
    "            else:\n",
    "                sentenceTags.append([0])\n",
    "        bert_nerTag_ids.append(sentenceTags)\n",
    "    \n",
    "    bert_nerTag_ids = np.asarray(bert_nerTag_ids)\n",
    "    \n",
    "    # create a sequence id array (which will just be all zero values)\n",
    "    bert_seq_ids = []\n",
    "\n",
    "    for i in range(len(bert_sentence_ids)):\n",
    "        seq_list = ([0] * len(bert_sentence_ids[i]))\n",
    "        bert_seq_ids.append(seq_list)\n",
    "\n",
    "    bert_seq_ids = np.asarray(bert_seq_ids)\n",
    "    \n",
    "    return bert_sentence_ids, bert_nerTag_ids, bert_seq_ids\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bert_sentence_ids, bert_nerTag_ids, bert_seq_ids = createBertNumObjects3(bert_sentences, bert_ner_tags)\n",
    "\n",
    "val_sentence_ids, val_nerTag_ids, val_seq_ids = createBertNumObjects3(val_sentences, val_ner_tags)\n",
    "\n",
    "test_sentence_ids, test_nerTag_ids, test_seq_ids = createBertNumObjects3(test_sentences, test_ner_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16108, 50)\n",
      "(16108, 50, 1)\n",
      "(16108, 50)\n"
     ]
    }
   ],
   "source": [
    "print(bert_sentence_ids.shape)\n",
    "print(bert_nerTag_ids.shape) # extra dim of 1 for labels\n",
    "print(bert_seq_ids.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4 - Create entity embeddings for Wikipedia entities\n",
    "\n",
    "### This will help the task of Candidate Generation for the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the wikiName_id_map_dict created earlier, scrape wikipedia articles for text descriptions of the entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brussels (French: Bruxelles [bʁysɛl] (listen) or [bʁyksɛl]; Dutch: Brussel [ˈbrʏsəl] (listen)), officially the Brussels-Capital Region (French: Région de Bruxelles-Capitale; Dutch: Brussels Hoofdstedelijk Gewest), is a region of Belgium comprising 19 municipalities, including the City of Brussels, which is the capital of Belgium. The Brussels-Capital Region is located in the central portion of the country and is a part of both the French Community of Belgium and the Flemish Community, but is separate from t\n"
     ]
    }
   ],
   "source": [
    "print(wikipedia.WikipediaPage(title = 'Brussels').summary[:512])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py:389: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file C:\\Users\\andre\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 Wikipedia page summaries executed in 167.78 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "wikiId_summaryTokens_dict = {}\n",
    "i = 0\n",
    "for wikiId in wikiName_id_map_dict:\n",
    "\n",
    "    if i > 100:\n",
    "        break # control\n",
    "    i += 1\n",
    "    try:\n",
    "        summary = wikipedia.summary(wikiName_id_map_dict[wikiId])\n",
    "    except:\n",
    "        summary = None\n",
    "        \n",
    "    if summary:\n",
    "        dict_value = tokenizer.tokenize(summary)[:512] # max_length\n",
    "        dict_value = tokenizer.convert_tokens_to_ids(dict_value)\n",
    "        wikiId_summaryTokens_dict.update({wikiId: dict_value})\n",
    "\n",
    "timing = time.time() - start_time\n",
    "print('100 Wikipedia page summaries executed in',round(timing,2),'seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that the summary extraction takes so long, I'm going to cheat a bit to encode the entities as BERT 768 dimensional pooled vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = []\n",
    "\n",
    "for item in train_data_raw:\n",
    "    if len(item) > 1 and item[1][:7] != '--NME--': # skip -DOCSTART-, --NME--, and non-entities\n",
    "        for i in range(len(item)):\n",
    "            if item[i][:4] == 'http': # index of the url\n",
    "                stop_index = i\n",
    "                entities.extend(item[2:stop_index])\n",
    "        \n",
    "for item in validation_data_raw:\n",
    "    if len(item) > 1 and item[1][:7] != '--NME--': # skip -DOCSTART-, --NME--, and non-entities    \n",
    "        for i in range(len(item)):\n",
    "            if item[i][:4] == 'http': # index of the url\n",
    "                stop_index = i\n",
    "                entities.extend(item[2:stop_index])\n",
    "        \n",
    "for item in test_data_raw:\n",
    "    if len(item) > 1 and item[1][:7] != '--NME--': # skip -DOCSTART-, --NME--, and non-entities\n",
    "        for i in range(len(item)):\n",
    "            if item[i][:4] == 'http': # index of the url\n",
    "                stop_index = i\n",
    "                entities.extend(item[2:stop_index])\n",
    "        \n",
    "entities = list(set(entities)) # unique entities only\n",
    "\n",
    "# replace unicode characters\n",
    "for i in range(len(entities)):\n",
    "    if '\\\\u' in entities[i]:\n",
    "        entities[i] = entities[i].encode().decode('unicode_escape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sinton',\n",
       " 'Nicolas_Ouédec',\n",
       " 'Toronto Blue Jays',\n",
       " 'Canopic_jar',\n",
       " 'Rory Underwood',\n",
       " 'Jonathan_Bachini',\n",
       " 'Brown Deer Park Golf Course',\n",
       " 'Palestinian_Legislative_Council',\n",
       " 'Akira_Ryō',\n",
       " 'Mike_Harwood',\n",
       " 'Prague_Stock_Exchange',\n",
       " 'Venkatesh Prasad',\n",
       " 'Super_League',\n",
       " 'Aron_Winter',\n",
       " 'Subaru Impreza',\n",
       " 'Heidrun',\n",
       " 'Wally Whitehurst',\n",
       " 'Antonio Tartaglia',\n",
       " 'Orrell_R.U.F.C.',\n",
       " 'Mats Wilander']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's add some noise to our entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "entities_ext = []\n",
    "start_time = time.time()\n",
    "i = 1\n",
    "\n",
    "for entity in entities:\n",
    "    ent_start_time = time.time()\n",
    "    print(entity, ' - ', str(round((i/len(entities)*100),3))+'%')\n",
    "    entities_ext.extend(wikipedia.search(entity,results=5))\n",
    "    timing = time.time() - ent_start_time\n",
    "    print('\\t', round(timing,2),'seconds')\n",
    "    i += 1\n",
    "\n",
    "timing = time.time() - start_time\n",
    "\n",
    "print('\\n\\n')\n",
    "print('Created noise for',len(entities),'entities. Executed in', round((timing/60),2),'minutes \\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createBertEntityObjects(entity_list, num_entities = 10, max_length=512):\n",
    "    \"\"\"\n",
    "    Given an list of entities, output numerical bert token ids, mask ids, and sequence ids\n",
    "    Last output is the text list of each entity that is contained in the vectors\n",
    "    Used for input into the bert entity vectors\n",
    "    Starts at 10 entities for output\n",
    "    \"\"\"\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    max_length -= 2 # account for CLS and SEP tokens\n",
    "    \n",
    "    entity_summary_ids = []\n",
    "    entity_mask_ids = []\n",
    "    entity_seq_ids = []\n",
    "    output_entities = []\n",
    "    \n",
    "    i = 1\n",
    "\n",
    "    for item in entity_list[:num_entities]:\n",
    "        # give run time information as this step can take a long time\n",
    "        item_start_time = time.time()\n",
    "        print(item, ' - ', str(round((i/num_entities*100),3))+'%')\n",
    "        try:\n",
    "            summary = tokenizer.tokenize(wikipedia.summary(item))[:max_length] \n",
    "        except:\n",
    "            summary = None\n",
    "\n",
    "        if summary:\n",
    "            num_pads = max_length - len(summary) \n",
    "            masks = ([1] * (len(summary)+2)) + ([0] * num_pads) # no need to account for CLS and SEP pads\n",
    "            summary = ['[CLS]'] + summary + ['[SEP]'] + (['[PAD]'] * num_pads)\n",
    "            summary = tokenizer.convert_tokens_to_ids(summary)\n",
    "\n",
    "            output_entities.append(item)\n",
    "            entity_summary_ids.append(np.asarray(summary))\n",
    "            entity_mask_ids.append(np.asarray(masks))\n",
    "            entity_seq_ids.append(np.asarray([1] * (max_length + 2))) # no need to account for CLS and SEP pads\n",
    "        \n",
    "        timing = time.time() - item_start_time\n",
    "        print('\\t', round(timing,2),'seconds')\n",
    "        i += 1\n",
    "\n",
    "    entity_summary_ids = np.asarray(entity_summary_ids)\n",
    "    entity_mask_ids = np.asarray(entity_mask_ids)\n",
    "    entity_seq_ids = np.asarray(entity_seq_ids)\n",
    "\n",
    "    timing = time.time() - start_time\n",
    "    print('\\n\\n')\n",
    "    \n",
    "    print(len(entity_summary_ids), 'Wikipedia page summaries executed in',round((timing/60),2),'minutes')\n",
    "    print('entity_summary_ids shape:', entity_summary_ids.shape)\n",
    "    print('entity_mask_ids shape:', entity_mask_ids.shape)\n",
    "    print('entity_seq_ids shape:', entity_seq_ids.shape)\n",
    "    print('Number of text entities:',len(output_entities))\n",
    "    \n",
    "    return entity_summary_ids, entity_mask_ids, entity_seq_ids, output_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58846"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(entities_ext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carlsberg Group  -  10.0%\n",
      "\t 1.6 seconds\n",
      "Carlsberg  -  20.0%\n",
      "\t 1.57 seconds\n",
      "Anfield  -  30.0%\n",
      "\t 1.51 seconds\n",
      "Carlsberg (district)  -  40.0%\n",
      "\t 1.61 seconds\n",
      "Ny Carlsberg Glyptotek  -  50.0%\n",
      "\t 1.88 seconds\n",
      "Hong Kong dollar  -  60.0%\n",
      "\t 1.63 seconds\n",
      ".hk  -  70.0%\n",
      "\t 1.6 seconds\n",
      "Heckler & Koch HK416  -  80.0%\n",
      "\t 1.52 seconds\n",
      "Hong Kong  -  90.0%\n",
      "\t 1.86 seconds\n",
      "Heckler & Koch  -  100.0%\n",
      "\t 1.55 seconds\n",
      "\n",
      "\n",
      "\n",
      "10 Wikipedia page summaries executed in 0.27 minutes\n",
      "entity_summary_ids shape: (10, 300)\n",
      "entity_mask_ids shape: (10, 300)\n",
      "entity_seq_ids shape: (10, 300)\n",
      "Number of text entities: 10\n"
     ]
    }
   ],
   "source": [
    "num_entities = 10\n",
    "max_length = 300\n",
    "\n",
    "entity_summary_ids, entity_mask_ids, entity_seq_ids, output_entities = createBertEntityObjects(entities_ext, \n",
    "                                                                              num_entities = num_entities,\n",
    "                                                                              max_length = max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sue Grafton'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = ['Sue','Graf','##ton']\n",
    "text_x = ' '.join(x).replace(' ##','')\n",
    "text_x "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode the entities as vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "bert_module = hub.Module(\"https://tfhub.dev/google/bert_cased_L-12_H-768_A-12/1\", trainable=True)\n",
    "\n",
    "bert_entity_inputs = dict(\n",
    "    input_ids=entity_summary_ids,\n",
    "    input_mask=entity_mask_ids,\n",
    "    segment_ids=entity_seq_ids)\n",
    "\n",
    "# use pooled_output so that each entity has a 768-dimensional vector\n",
    "bert_entity_outputs = bert_module(bert_entity_inputs, signature=\"tokens\", as_dict=True)[\"pooled_output\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode sentences as vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull the BERT sentence ids associated with our test entities\n",
    "\n",
    "entity_annotations = []\n",
    "entity_sentences = []\n",
    "\n",
    "entity_sentence_ids = []\n",
    "entity_masks = []\n",
    "entity_seq_ids = []\n",
    "\n",
    "# iterate through entity sentences\n",
    "for i in range(len(bert_entities)):\n",
    "    sentence_with_entity = None\n",
    "    # iterate through sentence tokens\n",
    "    for j in range(len(bert_entities[i])):\n",
    "    # if a sentence has an entity in our example list,\n",
    "    # append bert token, mask, and sequence ids\n",
    "        if bert_entities[i][j] in entities[:100]:\n",
    "            sentence_with_entity = bert_entities[i]\n",
    "    if sentence_with_entity:\n",
    "        # raw data to help with evaluation\n",
    "        entity_annotations.append(bert_entities[i])\n",
    "        entity_sentences.append(bert_sentences[i])\n",
    "        # bert numerical inputs\n",
    "        entity_sentence_ids.append(bert_sentence_ids[i])\n",
    "        entity_masks.append(bert_mask_ids[i])\n",
    "        entity_seq_ids.append(bert_seq_ids[i])\n",
    "        \n",
    "entity_sentence_ids = np.asarray(entity_sentence_ids)\n",
    "entity_masks = np.asarray(entity_masks)\n",
    "entity_seq_ids = np.asarray(entity_seq_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the example sentence actually has an entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_annotations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities[16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_sentences[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities[68]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hooray!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "bert_sentence_inputs = dict(\n",
    "    input_ids=entity_sentence_ids,\n",
    "    input_mask=entity_masks,\n",
    "    segment_ids=entity_seq_ids)\n",
    "\n",
    "# use sequence_output so that each token has a 768-dimensional vector\n",
    "bert_sentence_outputs = bert_module(bert_sentence_inputs, signature=\"tokens\", as_dict=True)[\"sequence_output\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entity vector shapes: (10, 768)\n",
      "\t 10 summaries, each represented as a 768 dimensional vector\n",
      "sentence vector shapes: (280, 50, 768)\n",
      "\t 280 sentences, each having 50 tokens, each of which is represented as a 768 dimensional vector\n"
     ]
    }
   ],
   "source": [
    "print('entity vector shapes:',bert_entity_outputs.shape)\n",
    "print('\\t',bert_entity_outputs.shape[0], 'summaries, each represented as a',\n",
    "      bert_entity_outputs.shape[1],'dimensional vector')\n",
    "\n",
    "print('sentence vector shapes:',bert_sentence_outputs.shape)\n",
    "print('\\t',bert_sentence_outputs.shape[0], 'sentences, each having',\n",
    "      bert_sentence_outputs.shape[1],'tokens, each of which is represented as a',\n",
    "     bert_sentence_outputs.shape[2],'dimensional vector')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the sentence vectors, each token within the sentence has a 768-dimensional vector, whereas the entity vectors are pooled and each entity summary is a 768-dimensional vector. To do the cosine similarity, we can take N tokens around the entity (assuming we have properly identified it) and aggregate those vector values, such that the sentence vectors and the entity vectors have the same shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize_vars(sess)\n",
    "\n",
    "#with graph.as_default():\n",
    "    #print(type(tf.Session().run(bert_sentence_outputs)))\n",
    "#bert_sentence_outputs.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now write the file out so that it can be easily used later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_summary_ids_filepath = 'C:\\\\Users\\\\andre\\\\Documents\\\\Berkeley\\\\w266\\\\w266_Project\\\\data\\\\entity_summary_ids.npy'\n",
    "entity_mask_ids_filepath = 'C:\\\\Users\\\\andre\\\\Documents\\\\Berkeley\\\\w266\\\\w266_Project\\\\data\\\\entity_mask_ids.npy'\n",
    "entity_seq_ids_filepath = 'C:\\\\Users\\\\andre\\\\Documents\\\\Berkeley\\\\w266\\\\w266_Project\\\\data\\\\entity_seq_ids.npy'\n",
    "output_entities_filepath = 'C:\\\\Users\\\\andre\\\\Documents\\\\Berkeley\\\\w266\\\\w266_Project\\\\data\\\\output_entities.data'\n",
    "entities_ext_filepath = 'C:\\\\Users\\\\andre\\\\Documents\\\\Berkeley\\\\w266\\\\w266_Project\\\\data\\\\entity_ext.data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output ndarray objects\n",
    "entity_files = [entity_summary_ids_filepath, \n",
    "                entity_mask_ids_filepath, \n",
    "                entity_seq_ids_filepath]\n",
    "\n",
    "entity_data = [entity_summary_ids, \n",
    "                entity_mask_ids, \n",
    "                entity_seq_ids]\n",
    "\n",
    "for i in range(len(entity_files)):\n",
    "    np.save(entity_files[i], entity_data[i])\n",
    "    \n",
    "\n",
    "# output list objects\n",
    "entity_files = [output_entities_filepath,\n",
    "                entities_ext_filepath]\n",
    "\n",
    "entity_data = [output_entities,\n",
    "                entities_ext]\n",
    "\n",
    "for i in range(len(entity_files)):\n",
    "    with open(entity_files[i], 'wb') as file_out:\n",
    "        pickle.dump(entity_data[i], file_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below cell can be run to open files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_summary_ids = np.load(entity_summary_ids_filepath)\n",
    "entity_mask_ids = np.load(entity_mask_ids_filepath)\n",
    "entity_seq_ids = np.load(entity_seq_ids_filepath)\n",
    "\n",
    "with open(output_entities_filepath, 'rb') as file_in:\n",
    "    output_entities = pickle.load(file_in)\n",
    "\n",
    "with open(entities_ext_filepath, 'rb') as file_in:\n",
    "    entities_ext = pickle.load(file_in)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the BERT model with custom accuracy function!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code borrowed from: https://towardsdatascience.com/bert-in-keras-with-tensorflow-hub-76bcbc9417b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will use a custom accuracy function to show how accurate the model performs with regards to 'B' and 'I' tags only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "def initialize_vars(sess):\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.tables_initializer())\n",
    "    K.set_session(sess)\n",
    "\n",
    "sess = tf.Session()\n",
    "graph = tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_entity_accuracy(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate accuracy of predicting 'B' and 'I' lables only\n",
    "    \n",
    "    y_true: actual NER tag codes with the 'O' value (0) masked\n",
    "    y_pred: predicted NER tag codes with the 'O' value (0) masked \n",
    "    \n",
    "    returns accuracy measure\n",
    "    \"\"\"\n",
    "\n",
    "    #get labels and predictions\n",
    "    \n",
    "    y_label = tf.reshape(tf.layers.Flatten()(tf.cast(y_true, tf.int32)),[-1])\n",
    "    mask = (y_label > 0) # evaluates to True for all non 'O'/0 labels\n",
    "    y_label_masked = tf.boolean_mask(y_label, mask)  # mask the labels\n",
    "    \n",
    "    y_flat_pred = tf.reshape(tf.layers.Flatten()(tf.cast(y_pred, tf.float64)),[-1,3])\n",
    "    y_max_pred = tf.cast(tf.math.argmax(input = y_flat_pred, axis=1), tf.int32) # take the max predicted value\n",
    "    y_pred_masked = tf.boolean_mask(y_max_pred, mask) # mask the predictions as well\n",
    "    \n",
    "    return tf.reduce_mean(tf.cast(tf.equal(y_pred_masked, y_label_masked), dtype=tf.float64))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure this works the way we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x00000187C355DEF0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x00000187C355DEF0>>: AttributeError: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x00000187C355DEF0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x00000187C355DEF0>>: AttributeError: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x00000187C355DEF0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x00000187C355DEF0>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:From C:\\Users\\andre\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\andre\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x00000187C366C6A0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x00000187C366C6A0>>: AttributeError: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x00000187C366C6A0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x00000187C366C6A0>>: AttributeError: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x00000187C366C6A0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x00000187C366C6A0>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "y_true = tf.constant([1,0,1,0])\n",
    "y_pred = tf.constant([[0.9,0.05,0.05],[0.9,0.05,0.05],[0.05,0.9,0.05],[0.05,0.9,0.05]])\n",
    "\n",
    "print(is_entity_accuracy(y_true, y_pred).eval(session=sess))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that it is working, we can create the BERT layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Create BERT layer, following https://towardsdatascience.com/bert-in-keras-with-tensorflow-hub-76bcbc9417b\n",
    "    init:  initialize layer. Specify various parameters regarding output types and dimensions. Very important is\n",
    "           to set the number of trainable layers.\n",
    "    build: build the layer based on parameters\n",
    "    call:  call the BERT layer within a model\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        n_fine_tune_layers=10,\n",
    "        pooling=\"sequence\",\n",
    "        bert_url=\"https://tfhub.dev/google/bert_cased_L-12_H-768_A-12/1\",\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.n_fine_tune_layers = n_fine_tune_layers\n",
    "        self.trainable = True\n",
    "        self.output_size = 768\n",
    "        self.pooling = pooling\n",
    "        self.bert_url = bert_url\n",
    "\n",
    "        super(BertLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.bert = hub.Module(\n",
    "            self.bert_url, trainable=self.trainable, name=f\"{self.name}_module\"\n",
    "        )\n",
    "\n",
    "        # Remove unused layers\n",
    "        trainable_vars = self.bert.variables\n",
    "        trainable_vars = [\n",
    "                var\n",
    "                for var in trainable_vars\n",
    "                if not \"/cls/\" in var.name and not \"/pooler/\" in var.name\n",
    "            ]\n",
    "        trainable_layers = []\n",
    "\n",
    "\n",
    "        # Select how many layers to fine tune\n",
    "        for i in range(self.n_fine_tune_layers):\n",
    "            trainable_layers.append(f\"encoder/layer_{str(11 - i)}\")\n",
    "\n",
    "        # Update trainable vars to contain only the specified layers\n",
    "        trainable_vars = [\n",
    "            var\n",
    "            for var in trainable_vars\n",
    "            if any([l in var.name for l in trainable_layers])\n",
    "        ]\n",
    "\n",
    "        # Add to trainable weights\n",
    "        for var in trainable_vars:\n",
    "            self._trainable_weights.append(var)\n",
    "\n",
    "        for var in self.bert.variables:\n",
    "            if var not in self._trainable_weights:\n",
    "                self._non_trainable_weights.append(var)\n",
    "\n",
    "        super(BertLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        inputs = [K.cast(x, dtype=\"int32\") for x in inputs]\n",
    "        input_ids, input_mask, segment_ids = inputs\n",
    "        bert_inputs = dict(\n",
    "            input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids\n",
    "        )\n",
    "        result = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\n",
    "                \"sequence_output\"\n",
    "            ]\n",
    "\n",
    "        mul_mask = lambda x, m: x * tf.expand_dims(m, axis=-1)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary metrics for the Tensor board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variable_summaries(var):\n",
    "    \"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean', mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        tf.summary.scalar('stddev', stddev)\n",
    "        tf.summary.scalar('max', tf.reduce_max(var))\n",
    "        tf.summary.scalar('min', tf.reduce_min(var))\n",
    "        tf.summary.histogram('histogram', var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes for modeling\n",
    "\n",
    "### 1. Sentence input -> BERT -> NER hidden layers -> predict NER tags / is-entity, is-not-entity\n",
    "### 2. Create BERT entity embeddings -> Take first N tokens from wiki summary, aggregate (average) the word embeddings to get a single vector\n",
    "### 3. Use some similarity function to generate N possible Entity Candidates -> Dense layer(N, softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.experimental.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method BertLayer.call of <__main__.BertLayer object at 0x00000187CB03FE10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BertLayer.call of <__main__.BertLayer object at 0x00000187CB03FE10>>: AttributeError: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method BertLayer.call of <__main__.BertLayer object at 0x00000187CB03FE10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BertLayer.call of <__main__.BertLayer object at 0x00000187CB03FE10>>: AttributeError: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method BertLayer.call of <__main__.BertLayer object at 0x00000187CB03FE10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BertLayer.call of <__main__.BertLayer object at 0x00000187CB03FE10>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"bert_layer/bert_layer_module_apply_tokens/bert/encoder/Reshape_13:0\", shape=(?, ?, 768), dtype=float32)\n",
      "WARNING:tensorflow:From C:\\Users\\andre\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\andre\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "bert_token_ids (InputLayer)     [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert_input_masks (InputLayer)   [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert_seq_ids (InputLayer)       [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert_layer (BertLayer)          (None, None, 768)    108931396   bert_token_ids[0][0]             \n",
      "                                                                 bert_input_masks[0][0]           \n",
      "                                                                 bert_seq_ids[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 128)    98432       bert_layer[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, None, 128)    0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, None, 128)    16512       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, None, 128)    0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "entity_identification (Dense)   (None, None, 3)      387         dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 109,046,727\n",
      "Trainable params: 28,466,819\n",
      "Non-trainable params: 80,579,908\n",
      "__________________________________________________________________________________________________\n",
      "Train on 16108 samples, validate on 3846 samples\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "2 root error(s) found.\n  (0) Internal: Blas GEMM launch failed : a.shape=(1600, 2), b.shape=(2, 768), m=1600, n=768, k=2\n\t [[{{node bert_layer/bert_layer_module_apply_tokens/bert/embeddings/MatMul}}]]\n\t [[loss/mul/_927]]\n  (1) Internal: Blas GEMM launch failed : a.shape=(1600, 2), b.shape=(2, 768), m=1600, n=768, k=2\n\t [[{{node bert_layer/bert_layer_module_apply_tokens/bert/embeddings/MatMul}}]]\n0 successful operations.\n0 derived errors ignored.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-58-60f9b2f182fc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mval_sentence_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_mask_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_seq_ids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_nerTag_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m         \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m     )\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    778\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m           \u001b[0mvalidation_freq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m           steps_name='steps_per_epoch')\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m         \u001b[1;31m# Get outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 363\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    364\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3290\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3292\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1458\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1459\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInternalError\u001b[0m: 2 root error(s) found.\n  (0) Internal: Blas GEMM launch failed : a.shape=(1600, 2), b.shape=(2, 768), m=1600, n=768, k=2\n\t [[{{node bert_layer/bert_layer_module_apply_tokens/bert/embeddings/MatMul}}]]\n\t [[loss/mul/_927]]\n  (1) Internal: Blas GEMM launch failed : a.shape=(1600, 2), b.shape=(2, 768), m=1600, n=768, k=2\n\t [[{{node bert_layer/bert_layer_module_apply_tokens/bert/embeddings/MatMul}}]]\n0 successful operations.\n0 derived errors ignored."
     ]
    }
   ],
   "source": [
    "# Build model #1 -> sentence inputs -> Bert layer -> Dense layer -> softmax is-entity prediction\n",
    "\n",
    "in_id = tf.keras.layers.Input(shape=(max_sentence_length,), name=\"bert_token_ids\")\n",
    "in_mask = tf.keras.layers.Input(shape=(max_sentence_length,), name=\"bert_input_masks\")\n",
    "in_segment = tf.keras.layers.Input(shape=(max_sentence_length,), name=\"bert_seq_ids\")\n",
    "\n",
    "bert_inputs = [in_id, in_mask, in_segment]\n",
    "\n",
    "# Instantiate the custom Bert Layer defined above\n",
    "bert_output = BertLayer(n_fine_tune_layers=4)(bert_inputs)\n",
    "print(bert_output)\n",
    "\n",
    "# Build the rest of the classifier \n",
    "dense = tf.keras.layers.Dense(128, activation='relu', name='dense_1')(bert_output)\n",
    "dense = tf.keras.layers.Dropout(rate=0.1, name='dropout_1')(dense)\n",
    "dense = tf.keras.layers.Dense(128, activation='relu', name='dense_2')(dense)\n",
    "dense = tf.keras.layers.Dropout(rate=0.1, name='dropout_2')(dense)\n",
    "\n",
    "#pred = tf.keras.layers.Dense(2, activation='softmax', name='is-entity')(dense)\n",
    "ner_pred = tf.keras.layers.Dense(3, activation='softmax', name='entity_identification')(dense)\n",
    "# take the maximum predicted value\n",
    "#pred = tf.math.argmax(input = tf.reshape(tf.keras.layers.Flatten()(tf.cast(pred, tf.float64)),[-1, 3]), axis=1)\n",
    "\n",
    "ner_model = tf.keras.models.Model(inputs=bert_inputs, outputs=ner_pred)\n",
    "#model.compile(loss={'NER':custom_loss}, optimizer='adam', metrics=['accuracy'])\n",
    "ner_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])#,\n",
    "                                                                                #is_entity_accuracy])\n",
    "\n",
    "ner_model.summary()\n",
    "\n",
    "\n",
    "#tf.reset_default_graph()\n",
    "#sess = tf.Session()\n",
    "#graph = tf.get_default_graph()\n",
    "\n",
    "initialize_vars(sess)\n",
    "\n",
    "with graph.as_default():\n",
    "    K.set_session(sess)\n",
    "    \n",
    "    ner_model.fit(\n",
    "        [bert_sentence_ids, bert_mask_ids, bert_seq_ids], \n",
    "        bert_nerTag_ids,\n",
    "        validation_data=([val_sentence_ids, val_mask_ids, val_seq_ids], val_nerTag_ids),\n",
    "        epochs=1,\n",
    "        batch_size=32\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_test = [val_sentence_ids, val_mask_ids, val_seq_ids]\n",
    "\n",
    "result = model.predict(\n",
    "    bert_test, \n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_entity_preds = []\n",
    "\n",
    "# for each sentence in result\n",
    "for i in range(len(result)):\n",
    "    # for each token in the sentence\n",
    "    sentence_preds = []\n",
    "    for j in range(len(result[i])):\n",
    "        # append the index of the max predicted value\n",
    "        index_value = np.where(result[i][j] == np.amax(result[i][j]))[0][0]\n",
    "        sentence_preds.append(index_value)\n",
    "    is_entity_preds.append(sentence_preds) \n",
    "    \n",
    "is_entity_preds = np.asarray(is_entity_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_entity_preds[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_nerTag_ids[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_sentences[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute cosine similarity between wikipedia summary embeddings and sentence sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need a helper function to output a sequence around the entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq_start: 1\n",
      "seq_end: 12\n",
      "seq_start: 11\n",
      "seq_end: 22\n"
     ]
    }
   ],
   "source": [
    "sequence_length = 5\n",
    "\n",
    "mention_sequences = []\n",
    "\n",
    "test = val_nerTag_ids[5]\n",
    "\n",
    "entity_counter = 0\n",
    "for i in range(len(test)):\n",
    "    if test[i] == [0] and entity_counter > 0: # i.e. reached an 'other' tag following identified entities\n",
    "        \n",
    "        seq_start = max(0, i - entity_counter - sequence_length)\n",
    "        seq_end = min(i+sequence_length, len(test))\n",
    "        \n",
    "        if seq_start == 0:\n",
    "            extra_tokens = abs(i - entity_counter - sequence_length)\n",
    "            if seq_end < len(test):\n",
    "                seq_end = min(seq_end + extra_tokens, len(test))\n",
    "        \n",
    "        if seq_end == len(test):\n",
    "            extra_tokens = sequence_length - (len(sequence_length) - i + 1)\n",
    "            if seq_start > 0:\n",
    "                seq_start = max(seq_start - extra_tokens, 0)\n",
    "                \n",
    "        mention_sequences.append(val_sentences[5][seq_start : seq_end])\n",
    "        print('seq_start:',seq_start)\n",
    "        print('seq_end:', seq_end)\n",
    "        \n",
    "        entity_counter = 0 # reset entity token counter\n",
    "        \n",
    "    elif test[i] > [0]: # i.e. identified the start of an entity token sequence\n",
    "        entity_counter += 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_nerTag_ids[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Trail',\n",
       "  '##ing',\n",
       "  'by',\n",
       "  '213',\n",
       "  ',',\n",
       "  'Somerset',\n",
       "  'got',\n",
       "  'a',\n",
       "  'solid',\n",
       "  'start',\n",
       "  'to'],\n",
       " ['to',\n",
       "  'their',\n",
       "  'second',\n",
       "  'innings',\n",
       "  'before',\n",
       "  'Simmons',\n",
       "  'stepped',\n",
       "  'in',\n",
       "  'to',\n",
       "  'bundle',\n",
       "  'them']]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mention_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_nerTag_ids[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_nerTag_ids[2][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'L', '##ON', '##D', '##ON', '1996', '-', '08', '-', '30']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_sentences[1][0:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
