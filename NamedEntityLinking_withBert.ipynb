{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Linking with BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using BERT embeddings to aid in the task of End-to-End Named Entity Linking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 - Read in the annotations and training dataset from AIDA-YAGO2 as well as the Wiki Entity Name/Id dataset\n",
    "\n",
    "These tab-seperated files will be used to train our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_file = 'C:\\\\Users\\\\andre\\\\Documents\\\\Berkeley\\\\w266\\\\w266_Project\\\\data\\\\AIDA-YAGO2-annotations.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train mentions:  25288\n",
      "validation mentions:  6349\n",
      "test mentions:  6078\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "\n",
    "train_annotations = []\n",
    "val_annotations = []\n",
    "test_annotations = []\n",
    "\n",
    "with open(annotations_file, encoding='utf-8') as tsvfile:\n",
    "    reader = csv.reader(tsvfile, delimiter='\\t')\n",
    "    i = 0\n",
    "    for row in reader:\n",
    "        i += 1\n",
    "        train_annotations.append(row)\n",
    "        if row == ['-DOCSTART- (947testa CRICKET)']: # first validation document\n",
    "            #print('validation data starts at index:', i-1)\n",
    "            val_index = i-1\n",
    "        if row == ['-DOCSTART- (1163testb SOCCER)']: # first test document\n",
    "            #print('test data starts at index:', i-1)\n",
    "            test_index = i-1\n",
    "    \n",
    "    val_annotations = train_annotations[val_index: test_index]\n",
    "    test_annotations = train_annotations[test_index:]\n",
    "    train_annotations = train_annotations[:val_index]\n",
    "    \n",
    "print('train mentions: ', len(train_annotations))\n",
    "print('validation mentions: ', len(val_annotations))\n",
    "print('test mentions: ', len(test_annotations))            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = 'C:\\\\Users\\\\andre\\\\Documents\\\\Berkeley\\\\w266\\\\w266_Project\\\\data\\\\basic_data\\\\test_datasets\\\\AIDA\\\\aida_train_unquoted.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['-DOCSTART- (1 EU)'],\n",
       " ['EU', 'B', 'EU', '--NME--'],\n",
       " ['rejects'],\n",
       " ['German',\n",
       "  'B',\n",
       "  'German',\n",
       "  'Germany',\n",
       "  'http://en.wikipedia.org/wiki/Germany',\n",
       "  '11867',\n",
       "  '/m/0345h'],\n",
       " ['call'],\n",
       " ['to'],\n",
       " ['boycott'],\n",
       " ['British',\n",
       "  'B',\n",
       "  'British',\n",
       "  'United_Kingdom',\n",
       "  'http://en.wikipedia.org/wiki/United_Kingdom',\n",
       "  '31717',\n",
       "  '/m/07ssc'],\n",
       " ['lamb'],\n",
       " ['.'],\n",
       " [],\n",
       " ['Peter', 'B', 'Peter Blackburn', '--NME--'],\n",
       " ['Blackburn', 'I', 'Peter Blackburn', '--NME--'],\n",
       " [],\n",
       " ['BRUSSELS',\n",
       "  'B',\n",
       "  'BRUSSELS',\n",
       "  'Brussels',\n",
       "  'http://en.wikipedia.org/wiki/Brussels',\n",
       "  '3708',\n",
       "  '/m/0177z'],\n",
       " ['1996-08-22'],\n",
       " [],\n",
       " ['The'],\n",
       " ['European',\n",
       "  'B',\n",
       "  'European Commission',\n",
       "  'European_Commission',\n",
       "  'http://en.wikipedia.org/wiki/European_Commission',\n",
       "  '9974',\n",
       "  '/m/02q9k'],\n",
       " ['Commission',\n",
       "  'I',\n",
       "  'European Commission',\n",
       "  'European_Commission',\n",
       "  'http://en.wikipedia.org/wiki/European_Commission',\n",
       "  '9974',\n",
       "  '/m/02q9k'],\n",
       " ['said'],\n",
       " ['on'],\n",
       " ['Thursday'],\n",
       " ['it'],\n",
       " ['disagreed'],\n",
       " ['with'],\n",
       " ['German',\n",
       "  'B',\n",
       "  'German',\n",
       "  'Germany',\n",
       "  'http://en.wikipedia.org/wiki/Germany',\n",
       "  '11867',\n",
       "  '/m/0345h'],\n",
       " ['advice'],\n",
       " ['to'],\n",
       " ['consumers'],\n",
       " ['to'],\n",
       " ['shun'],\n",
       " ['British',\n",
       "  'B',\n",
       "  'British',\n",
       "  'United_Kingdom',\n",
       "  'http://en.wikipedia.org/wiki/United_Kingdom',\n",
       "  '31717',\n",
       "  '/m/07ssc'],\n",
       " ['lamb'],\n",
       " ['until'],\n",
       " ['scientists'],\n",
       " ['determine'],\n",
       " ['whether'],\n",
       " ['mad'],\n",
       " ['cow'],\n",
       " ['disease'],\n",
       " ['can'],\n",
       " ['be'],\n",
       " ['transmitted'],\n",
       " ['to'],\n",
       " ['sheep'],\n",
       " ['.'],\n",
       " [],\n",
       " ['Germany',\n",
       "  'B',\n",
       "  'Germany',\n",
       "  'Germany',\n",
       "  'http://en.wikipedia.org/wiki/Germany',\n",
       "  '11867',\n",
       "  '/m/0345h'],\n",
       " [\"'s\"],\n",
       " ['representative'],\n",
       " ['to'],\n",
       " ['the'],\n",
       " ['European',\n",
       "  'B',\n",
       "  'European Union',\n",
       "  'European_Union',\n",
       "  'http://en.wikipedia.org/wiki/European_Union',\n",
       "  '9317',\n",
       "  '/m/02jxk'],\n",
       " ['Union',\n",
       "  'I',\n",
       "  'European Union',\n",
       "  'European_Union',\n",
       "  'http://en.wikipedia.org/wiki/European_Union',\n",
       "  '9317',\n",
       "  '/m/02jxk'],\n",
       " [\"'s\"],\n",
       " ['veterinary'],\n",
       " ['committee'],\n",
       " ['Werner', 'B', 'Werner Zwingmann', '--NME--'],\n",
       " ['Zwingmann', 'I', 'Werner Zwingmann', '--NME--'],\n",
       " ['said'],\n",
       " ['on'],\n",
       " ['Wednesday'],\n",
       " ['consumers'],\n",
       " ['should'],\n",
       " ['buy'],\n",
       " ['sheepmeat'],\n",
       " ['from'],\n",
       " ['countries'],\n",
       " ['other'],\n",
       " ['than'],\n",
       " ['Britain',\n",
       "  'B',\n",
       "  'Britain',\n",
       "  'United_Kingdom',\n",
       "  'http://en.wikipedia.org/wiki/United_Kingdom',\n",
       "  '31717',\n",
       "  '/m/07ssc'],\n",
       " ['until'],\n",
       " ['the'],\n",
       " ['scientific'],\n",
       " ['advice'],\n",
       " ['was'],\n",
       " ['clearer'],\n",
       " ['.'],\n",
       " [],\n",
       " [],\n",
       " ['We'],\n",
       " ['do'],\n",
       " [\"n't\"],\n",
       " ['support'],\n",
       " ['any'],\n",
       " ['such'],\n",
       " ['recommendation'],\n",
       " ['because'],\n",
       " ['we'],\n",
       " ['do'],\n",
       " [\"n't\"],\n",
       " ['see'],\n",
       " ['any'],\n",
       " ['grounds'],\n",
       " ['for'],\n",
       " ['it'],\n",
       " [','],\n",
       " [],\n",
       " ['the'],\n",
       " ['Commission',\n",
       "  'B',\n",
       "  'Commission',\n",
       "  'European_Commission',\n",
       "  'http://en.wikipedia.org/wiki/European_Commission',\n",
       "  '9974',\n",
       "  '/m/02q9k'],\n",
       " [\"'s\"],\n",
       " ['chief'],\n",
       " ['spokesman'],\n",
       " ['Nikolaus', 'B', 'Nikolaus van der Pas', '--NME--'],\n",
       " ['van', 'I', 'Nikolaus van der Pas', '--NME--'],\n",
       " ['der', 'I', 'Nikolaus van der Pas', '--NME--'],\n",
       " ['Pas', 'I', 'Nikolaus van der Pas', '--NME--'],\n",
       " ['told'],\n",
       " ['a'],\n",
       " ['news'],\n",
       " ['briefing'],\n",
       " ['.'],\n",
       " [],\n",
       " ['He'],\n",
       " ['said'],\n",
       " ['further'],\n",
       " ['scientific'],\n",
       " ['study'],\n",
       " ['was'],\n",
       " ['required'],\n",
       " ['and'],\n",
       " ['if'],\n",
       " ['it'],\n",
       " ['was'],\n",
       " ['found'],\n",
       " ['that'],\n",
       " ['action'],\n",
       " ['was'],\n",
       " ['needed'],\n",
       " ['it'],\n",
       " ['should'],\n",
       " ['be'],\n",
       " ['taken'],\n",
       " ['by'],\n",
       " ['the'],\n",
       " ['European',\n",
       "  'B',\n",
       "  'European Union',\n",
       "  'European_Union',\n",
       "  'http://en.wikipedia.org/wiki/European_Union',\n",
       "  '9317',\n",
       "  '/m/02jxk'],\n",
       " ['Union',\n",
       "  'I',\n",
       "  'European Union',\n",
       "  'European_Union',\n",
       "  'http://en.wikipedia.org/wiki/European_Union',\n",
       "  '9317',\n",
       "  '/m/02jxk'],\n",
       " ['.'],\n",
       " [],\n",
       " ['He'],\n",
       " ['said'],\n",
       " ['a'],\n",
       " ['proposal'],\n",
       " ['last'],\n",
       " ['month'],\n",
       " ['by'],\n",
       " ['EU', 'B', 'EU', '--NME--'],\n",
       " ['Farm'],\n",
       " ['Commissioner'],\n",
       " ['Franz',\n",
       "  'B',\n",
       "  'Franz Fischler',\n",
       "  'Franz_Fischler',\n",
       "  'http://en.wikipedia.org/wiki/Franz_Fischler',\n",
       "  '626779',\n",
       "  '/m/02y4y1'],\n",
       " ['Fischler',\n",
       "  'I',\n",
       "  'Franz Fischler',\n",
       "  'Franz_Fischler',\n",
       "  'http://en.wikipedia.org/wiki/Franz_Fischler',\n",
       "  '626779',\n",
       "  '/m/02y4y1'],\n",
       " ['to'],\n",
       " ['ban'],\n",
       " ['sheep'],\n",
       " ['brains'],\n",
       " [','],\n",
       " ['spleens'],\n",
       " ['and'],\n",
       " ['spinal'],\n",
       " ['cords'],\n",
       " ['from'],\n",
       " ['the'],\n",
       " ['human'],\n",
       " ['and'],\n",
       " ['animal'],\n",
       " ['food'],\n",
       " ['chains'],\n",
       " ['was'],\n",
       " ['a'],\n",
       " ['highly'],\n",
       " ['specific'],\n",
       " ['and'],\n",
       " ['precautionary'],\n",
       " ['move'],\n",
       " ['to'],\n",
       " ['protect'],\n",
       " ['human'],\n",
       " ['health'],\n",
       " ['.'],\n",
       " [],\n",
       " ['Fischler', 'B', 'Fischler', '--NME--'],\n",
       " ['proposed'],\n",
       " ['EU-wide', 'B', 'EU-wide', '--NME--'],\n",
       " ['measures'],\n",
       " ['after'],\n",
       " ['reports'],\n",
       " ['from'],\n",
       " ['Britain',\n",
       "  'B',\n",
       "  'Britain',\n",
       "  'United_Kingdom',\n",
       "  'http://en.wikipedia.org/wiki/United_Kingdom',\n",
       "  '31717',\n",
       "  '/m/07ssc'],\n",
       " ['and'],\n",
       " ['France',\n",
       "  'B',\n",
       "  'France',\n",
       "  'France',\n",
       "  'http://en.wikipedia.org/wiki/France',\n",
       "  '5843419',\n",
       "  '/m/0f8l9c'],\n",
       " ['that'],\n",
       " ['under'],\n",
       " ['laboratory'],\n",
       " ['conditions'],\n",
       " ['sheep'],\n",
       " ['could'],\n",
       " ['contract'],\n",
       " ['Bovine', 'B', 'Bovine Spongiform Encephalopathy', '--NME--'],\n",
       " ['Spongiform', 'I', 'Bovine Spongiform Encephalopathy', '--NME--'],\n",
       " ['Encephalopathy', 'I', 'Bovine Spongiform Encephalopathy', '--NME--'],\n",
       " ['('],\n",
       " ['BSE',\n",
       "  'B',\n",
       "  'BSE',\n",
       "  'Bovine_spongiform_encephalopathy',\n",
       "  'http://en.wikipedia.org/wiki/Bovine_spongiform_encephalopathy',\n",
       "  '19344418',\n",
       "  '/m/0jy66'],\n",
       " [')'],\n",
       " ['--'],\n",
       " ['mad'],\n",
       " ['cow'],\n",
       " ['disease'],\n",
       " ['.'],\n",
       " [],\n",
       " ['But'],\n",
       " ['Fischler', 'B', 'Fischler', '--NME--'],\n",
       " ['agreed'],\n",
       " ['to'],\n",
       " ['review'],\n",
       " ['his'],\n",
       " ['proposal'],\n",
       " ['after'],\n",
       " ['the'],\n",
       " ['EU', 'B', 'EU', '--NME--'],\n",
       " [\"'s\"],\n",
       " ['standing'],\n",
       " ['veterinary'],\n",
       " ['committee'],\n",
       " [','],\n",
       " ['mational'],\n",
       " ['animal'],\n",
       " ['health'],\n",
       " ['officials'],\n",
       " [','],\n",
       " ['questioned'],\n",
       " ['if'],\n",
       " ['such'],\n",
       " ['action'],\n",
       " ['was'],\n",
       " ['justified'],\n",
       " ['as'],\n",
       " ['there'],\n",
       " ['was'],\n",
       " ['only'],\n",
       " ['a'],\n",
       " ['slight'],\n",
       " ['risk'],\n",
       " ['to'],\n",
       " ['human'],\n",
       " ['health'],\n",
       " ['.'],\n",
       " [],\n",
       " ['Spanish',\n",
       "  'B',\n",
       "  'Spanish',\n",
       "  'Spain',\n",
       "  'http://en.wikipedia.org/wiki/Spain',\n",
       "  '26667',\n",
       "  '/m/06mkj'],\n",
       " ['Farm'],\n",
       " ['Minister'],\n",
       " ['Loyola',\n",
       "  'B',\n",
       "  'Loyola de Palacio',\n",
       "  'Loyola_de_Palacio',\n",
       "  'http://en.wikipedia.org/wiki/Loyola_de_Palacio',\n",
       "  '6394317',\n",
       "  '/m/0g3rwc'],\n",
       " ['de',\n",
       "  'I',\n",
       "  'Loyola de Palacio',\n",
       "  'Loyola_de_Palacio',\n",
       "  'http://en.wikipedia.org/wiki/Loyola_de_Palacio',\n",
       "  '6394317',\n",
       "  '/m/0g3rwc'],\n",
       " ['Palacio',\n",
       "  'I',\n",
       "  'Loyola de Palacio',\n",
       "  'Loyola_de_Palacio',\n",
       "  'http://en.wikipedia.org/wiki/Loyola_de_Palacio',\n",
       "  '6394317',\n",
       "  '/m/0g3rwc'],\n",
       " ['had'],\n",
       " ['earlier'],\n",
       " ['accused'],\n",
       " ['Fischler', 'B', 'Fischler', '--NME--'],\n",
       " ['at'],\n",
       " ['an'],\n",
       " ['EU', 'B', 'EU', '--NME--'],\n",
       " ['farm'],\n",
       " ['ministers'],\n",
       " [\"'\"],\n",
       " ['meeting'],\n",
       " ['of'],\n",
       " ['causing'],\n",
       " ['unjustified'],\n",
       " ['alarm'],\n",
       " ['through'],\n",
       " [],\n",
       " ['dangerous'],\n",
       " ['generalisation'],\n",
       " ['.'],\n",
       " [],\n",
       " [],\n",
       " ['.'],\n",
       " [],\n",
       " ['Only'],\n",
       " ['France',\n",
       "  'B',\n",
       "  'France',\n",
       "  'France',\n",
       "  'http://en.wikipedia.org/wiki/France',\n",
       "  '5843419',\n",
       "  '/m/0f8l9c'],\n",
       " ['and'],\n",
       " ['Britain',\n",
       "  'B',\n",
       "  'Britain',\n",
       "  'United_Kingdom',\n",
       "  'http://en.wikipedia.org/wiki/United_Kingdom',\n",
       "  '31717',\n",
       "  '/m/07ssc'],\n",
       " ['backed'],\n",
       " ['Fischler', 'B', 'Fischler', '--NME--'],\n",
       " [\"'s\"],\n",
       " ['proposal'],\n",
       " ['.'],\n",
       " [],\n",
       " ['The'],\n",
       " ['EU', 'B', 'EU', '--NME--'],\n",
       " [\"'s\"],\n",
       " ['scientific'],\n",
       " ['veterinary'],\n",
       " ['and'],\n",
       " ['multidisciplinary'],\n",
       " ['committees'],\n",
       " ['are'],\n",
       " ['due'],\n",
       " ['to'],\n",
       " ['re-examine'],\n",
       " ['the'],\n",
       " ['issue'],\n",
       " ['early'],\n",
       " ['next'],\n",
       " ['month'],\n",
       " ['and'],\n",
       " ['make'],\n",
       " ['recommendations'],\n",
       " ['to'],\n",
       " ['the'],\n",
       " ['senior'],\n",
       " ['veterinary'],\n",
       " ['officials'],\n",
       " ['.'],\n",
       " [],\n",
       " ['Sheep'],\n",
       " ['have'],\n",
       " ['long'],\n",
       " ['been'],\n",
       " ['known'],\n",
       " ['to'],\n",
       " ['contract'],\n",
       " ['scrapie'],\n",
       " [','],\n",
       " ['a'],\n",
       " ['brain-wasting'],\n",
       " ['disease'],\n",
       " ['similar'],\n",
       " ['to'],\n",
       " ['BSE',\n",
       "  'B',\n",
       "  'BSE',\n",
       "  'Bovine_spongiform_encephalopathy',\n",
       "  'http://en.wikipedia.org/wiki/Bovine_spongiform_encephalopathy',\n",
       "  '19344418',\n",
       "  '/m/0jy66'],\n",
       " ['which'],\n",
       " ['is'],\n",
       " ['believed'],\n",
       " ['to'],\n",
       " ['have'],\n",
       " ['been'],\n",
       " ['transferred'],\n",
       " ['to'],\n",
       " ['cattle'],\n",
       " ['through'],\n",
       " ['feed'],\n",
       " ['containing'],\n",
       " ['animal'],\n",
       " ['waste'],\n",
       " ['.'],\n",
       " [],\n",
       " ['British',\n",
       "  'B',\n",
       "  'British',\n",
       "  'United_Kingdom',\n",
       "  'http://en.wikipedia.org/wiki/United_Kingdom',\n",
       "  '31717',\n",
       "  '/m/07ssc'],\n",
       " ['farmers'],\n",
       " ['denied'],\n",
       " ['on'],\n",
       " ['Thursday'],\n",
       " ['there'],\n",
       " ['was'],\n",
       " ['any'],\n",
       " ['danger'],\n",
       " ['to'],\n",
       " ['human'],\n",
       " ['health'],\n",
       " ['from'],\n",
       " ['their'],\n",
       " ['sheep'],\n",
       " [','],\n",
       " ['but'],\n",
       " ['expressed'],\n",
       " ['concern'],\n",
       " ['that'],\n",
       " ['German',\n",
       "  'B',\n",
       "  'German',\n",
       "  'Germany',\n",
       "  'http://en.wikipedia.org/wiki/Germany',\n",
       "  '11867',\n",
       "  '/m/0345h'],\n",
       " ['government'],\n",
       " ['advice'],\n",
       " ['to'],\n",
       " ['consumers'],\n",
       " ['to'],\n",
       " ['avoid'],\n",
       " ['British',\n",
       "  'B',\n",
       "  'British',\n",
       "  'United_Kingdom',\n",
       "  'http://en.wikipedia.org/wiki/United_Kingdom',\n",
       "  '31717',\n",
       "  '/m/07ssc'],\n",
       " ['lamb'],\n",
       " ['might'],\n",
       " ['influence'],\n",
       " ['consumers'],\n",
       " ['across'],\n",
       " ['Europe',\n",
       "  'B',\n",
       "  'Europe',\n",
       "  'Europe',\n",
       "  'http://en.wikipedia.org/wiki/Europe',\n",
       "  '9239',\n",
       "  '/m/02j9z'],\n",
       " ['.'],\n",
       " [],\n",
       " [],\n",
       " ['What'],\n",
       " ['we'],\n",
       " ['have'],\n",
       " ['to'],\n",
       " ['be'],\n",
       " ['extremely'],\n",
       " ['careful'],\n",
       " ['of'],\n",
       " ['is'],\n",
       " ['how'],\n",
       " ['other'],\n",
       " ['countries'],\n",
       " ['are'],\n",
       " ['going'],\n",
       " ['to'],\n",
       " ['take'],\n",
       " ['Germany',\n",
       "  'B',\n",
       "  'Germany',\n",
       "  'Germany',\n",
       "  'http://en.wikipedia.org/wiki/Germany',\n",
       "  '11867',\n",
       "  '/m/0345h'],\n",
       " [\"'s\"],\n",
       " ['lead'],\n",
       " [','],\n",
       " [],\n",
       " ['Welsh', 'B', \"Welsh National Farmers ' Union\", '--NME--'],\n",
       " ['National', 'I', \"Welsh National Farmers ' Union\", '--NME--'],\n",
       " ['Farmers', 'I', \"Welsh National Farmers ' Union\", '--NME--'],\n",
       " [\"'\", 'I', \"Welsh National Farmers ' Union\", '--NME--'],\n",
       " ['Union', 'I', \"Welsh National Farmers ' Union\", '--NME--'],\n",
       " ['('],\n",
       " ['NFU', 'B', 'NFU', '--NME--'],\n",
       " [')'],\n",
       " ['chairman'],\n",
       " ['John', 'B', 'John Lloyd Jones', '--NME--'],\n",
       " ['Lloyd', 'I', 'John Lloyd Jones', '--NME--'],\n",
       " ['Jones', 'I', 'John Lloyd Jones', '--NME--'],\n",
       " ['said'],\n",
       " ['on'],\n",
       " ['BBC', 'B', 'BBC radio', '--NME--'],\n",
       " ['radio', 'I', 'BBC radio', '--NME--'],\n",
       " ['.'],\n",
       " [],\n",
       " ['Bonn',\n",
       "  'B',\n",
       "  'Bonn',\n",
       "  'Bonn',\n",
       "  'http://en.wikipedia.org/wiki/Bonn',\n",
       "  '3295',\n",
       "  '/m/0150n'],\n",
       " ['has'],\n",
       " ['led'],\n",
       " ['efforts'],\n",
       " ['to'],\n",
       " ['protect'],\n",
       " ['public'],\n",
       " ['health'],\n",
       " ['after'],\n",
       " ['consumer'],\n",
       " ['confidence'],\n",
       " ['collapsed'],\n",
       " ['in'],\n",
       " ['March'],\n",
       " ['after'],\n",
       " ['a'],\n",
       " ['British',\n",
       "  'B',\n",
       "  'British',\n",
       "  'United_Kingdom',\n",
       "  'http://en.wikipedia.org/wiki/United_Kingdom',\n",
       "  '31717',\n",
       "  '/m/07ssc'],\n",
       " ['report'],\n",
       " ['suggested'],\n",
       " ['humans'],\n",
       " ['could'],\n",
       " ['contract'],\n",
       " ['an'],\n",
       " ['illness'],\n",
       " ['similar'],\n",
       " ['to'],\n",
       " ['mad'],\n",
       " ['cow'],\n",
       " ['disease'],\n",
       " ['by'],\n",
       " ['eating'],\n",
       " ['contaminated'],\n",
       " ['beef'],\n",
       " ['.'],\n",
       " [],\n",
       " ['Germany',\n",
       "  'B',\n",
       "  'Germany',\n",
       "  'Germany',\n",
       "  'http://en.wikipedia.org/wiki/Germany',\n",
       "  '11867',\n",
       "  '/m/0345h'],\n",
       " ['imported'],\n",
       " ['47,600'],\n",
       " ['sheep'],\n",
       " ['from'],\n",
       " ['Britain',\n",
       "  'B',\n",
       "  'Britain',\n",
       "  'United_Kingdom',\n",
       "  'http://en.wikipedia.org/wiki/United_Kingdom',\n",
       "  '31717',\n",
       "  '/m/07ssc'],\n",
       " ['last'],\n",
       " ['year'],\n",
       " [','],\n",
       " ['nearly'],\n",
       " ['half'],\n",
       " ['of'],\n",
       " ['total'],\n",
       " ['imports'],\n",
       " ['.'],\n",
       " [],\n",
       " ['It'],\n",
       " ['brought'],\n",
       " ['in'],\n",
       " ['4,275'],\n",
       " ['tonnes'],\n",
       " ['of'],\n",
       " ['British',\n",
       "  'B',\n",
       "  'British',\n",
       "  'United_Kingdom',\n",
       "  'http://en.wikipedia.org/wiki/United_Kingdom',\n",
       "  '31717',\n",
       "  '/m/07ssc'],\n",
       " ['mutton'],\n",
       " [','],\n",
       " ['some'],\n",
       " ['10'],\n",
       " ['percent'],\n",
       " ['of'],\n",
       " ['overall'],\n",
       " ['imports'],\n",
       " ['.'],\n",
       " []]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_raw = []\n",
    "\n",
    "with open(train_file, encoding='utf-8') as train_tsvfile:\n",
    "    train_reader = csv.reader(train_tsvfile, delimiter='\\t')\n",
    "    for row in train_reader:\n",
    "        train_data_raw.append(row)\n",
    "    \n",
    "train_data_raw[:490] # just show up to the second document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create raw validation and test datasets\n",
    "test_file = 'C:\\\\Users\\\\andre\\\\Documents\\\\Berkeley\\\\w266\\\\w266_project\\\\data\\\\basic_data\\\\test_datasets\\\\AIDA\\\\testa_testb_aggregate_unquoted.txt'\n",
    "\n",
    "validation_data_raw = []\n",
    "test_data_raw = []\n",
    "\n",
    "with open(test_file, encoding='utf-8') as test_tsvfile:\n",
    "    test_reader = csv.reader(test_tsvfile, delimiter='\\t')\n",
    "    for row in test_reader:\n",
    "        validation_data_raw.append(row)\n",
    "\n",
    "for i in range(len(validation_data_raw)):\n",
    "    if validation_data_raw[i] == ['-DOCSTART- (1163testb SOCCER)']: # first test document\n",
    "        test_index = i\n",
    "        \n",
    "test_data_raw = validation_data_raw[test_index:]\n",
    "validation_data_raw = validation_data_raw[:test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikiName_id_mapFile = 'C:\\\\Users\\\\andre\\\\Documents\\\\Berkeley\\\\w266\\\\w266_project\\\\data\\\\basic_data\\\\wiki_name_id_map.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'United Kingdom'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(wikiName_id_mapFile, encoding='utf-8') as wikiMap_tsvfile:\n",
    "    wikiMap_reader = csv.reader(wikiMap_tsvfile, delimiter='\\t')\n",
    "    wikiName_id_map_dict = {int(row[1]): row[0] for row in wikiMap_reader if row[0][0].isalpha() or row[0][0].isnumeric()}\n",
    "\n",
    "wikiName_id_map_dict[31717]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2 - Pre-process the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules that will be used downstream\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import time\n",
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "from random import shuffle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.python.keras.layers import Lambda\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.backend import sparse_categorical_crossentropy\n",
    "from tensorflow.keras.layers import Dense, TimeDistributed\n",
    "\n",
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_bert_path = 'C:\\\\Users\\\\andre\\\\Documents\\\\Berkeley\\\\w266\\\\w266_project\\\\bert'\n",
    "# local path where the bert module has been cloned from git\n",
    "\n",
    "# make sure that the paths are accessible within the notebook\n",
    "sys.path.insert(0,local_bert_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\andre\\Documents\\Berkeley\\w266\\w266_project\\bert\\optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import optimization\n",
    "import run_classifier\n",
    "import tokenization\n",
    "import run_classifier_with_tfhub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the bert model that is cased\n",
    "bert_url = \"https://tfhub.dev/google/bert_cased_L-12_H-768_A-12/1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a BERT tokenizer so that words can be tokenized for BERT input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\andre\\Documents\\Berkeley\\w266\\w266_project\\bert\\tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\andre\\Documents\\Berkeley\\w266\\w266_project\\bert\\tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "graph = tf.get_default_graph()\n",
    "\n",
    "bert_module = hub.Module(bert_url)\n",
    "tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
    "\n",
    "vocab_file, do_lower_case = tf.Session().run([tokenization_info[\"vocab_file\"],\n",
    "                                            tokenization_info[\"do_lower_case\"]])\n",
    "\n",
    "tokenizer = tokenization.FullTokenizer(\n",
    "    vocab_file = vocab_file, do_lower_case = do_lower_case\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'token', '##izer', ',', 'is', 'it', 'even', 'working', '?']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make sure the tokenizer works\n",
    "tokenizer_test = tokenizer.tokenize('This tokenizer, is it even working?')\n",
    "tokenizer_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1188, 22559, 17260, 117, 1110, 1122, 1256, 1684, 136]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_ids_test = tokenizer.convert_tokens_to_ids(tokenizer_test)\n",
    "tokenizer_ids_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'token', '##izer', ',', 'is', 'it', 'even', 'working', '?']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(tokenizer_ids_test)\n",
    "# seems legit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the BERT tokenizer can be used to create tokens from the input text that can be used by the BERT model (more on that later).\n",
    "\n",
    "To simplify things, create a function that creates dictionary objects for document text, NER tags, entity Ids, and wikipedia URLs given row-by-row data (from training or test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "218505"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createUtilityDicts(data_rows):\n",
    "\n",
    "    data_text_dict = {}\n",
    "    data_nerTag_dict = {}\n",
    "    data_entityId_dict = {}\n",
    "    data_wikiUrl_dict = {}\n",
    "\n",
    "    # set the first DOCSTART of the train data as the dict key\n",
    "    dict_key = data_rows[0][0]\n",
    "    # create empty lists for dict values\n",
    "    text_dict_values = []\n",
    "    nerTag_dict_values = []\n",
    "    entityId_dict_values = []\n",
    "    wikiUrl_dict_values = []\n",
    "\n",
    "    for row in data_rows[1:]:\n",
    "\n",
    "        if row == []: #signaling end of sentence\n",
    "            text_dict_values.append('[SEP]')\n",
    "            nerTag_dict_values.append('[nerSEP]')\n",
    "            entityId_dict_values.append('[entSEP]')\n",
    "            wikiUrl_dict_values.append('[urlSEP]')\n",
    "\n",
    "        elif len(row[0]) > 9 and row[0][:10] == '-DOCSTART-': # signaling a new document\n",
    "            data_text_dict.update({dict_key : text_dict_values})\n",
    "            data_nerTag_dict.update({dict_key : nerTag_dict_values})\n",
    "            data_entityId_dict.update({dict_key : entityId_dict_values})\n",
    "            data_wikiUrl_dict.update({dict_key : wikiUrl_dict_values})\n",
    "\n",
    "            # reset key and value objects\n",
    "            dict_key = row[0]\n",
    "            text_dict_values = []\n",
    "            nerTag_dict_values = []\n",
    "            entityId_dict_values = []\n",
    "            wikiUrl_dict_values = []\n",
    "\n",
    "        elif len(row) > 1: # i.e. there is an entity mention\n",
    "            for i in range(len(row)): # skip the possibly variable number of wiki names\n",
    "                if row[i][:4] == 'http' or row[i] == '--NME--':\n",
    "                    wikiUrl = row[i]                \n",
    "                    if row[i][:4] == 'http': # only for cases where there is a wiki url\n",
    "                        entityId = row[i-1] # append the wiki entity name, which precedes the url\n",
    "                    else:\n",
    "                        entityId = '--NME--'\n",
    "            \n",
    "            for i in range(len(tokenizer.tokenize(row[0]))): \n",
    "                text_dict_values.append(tokenizer.tokenize(row[0])[i]) # append all tokenized strings\n",
    "                nerTag_dict_values.append(row[1]) # append same NER tag for each token\n",
    "                wikiUrl_dict_values.append(wikiUrl) # append same Wiki URL for each token\n",
    "                entityId_dict_values.append(entityId) # append same enitity Id for each token\n",
    "\n",
    "        else: # i.e. there is no entity mention\n",
    "            for i in range(len(tokenizer.tokenize(row[0]))): # append all tokenized strings\n",
    "                text_dict_values.append(tokenizer.tokenize(row[0])[i])\n",
    "                if i == 0:\n",
    "                    nerTag_dict_values.append('O')\n",
    "                else:\n",
    "                    nerTag_dict_values.append('nerX')\n",
    "                entityId_dict_values.append(None)\n",
    "                wikiUrl_dict_values.append(None)\n",
    "\n",
    "    return data_text_dict, data_nerTag_dict, data_entityId_dict, data_wikiUrl_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_dict, train_nerTag_dict, train_entityId_dict, train_wikiUrl_dict = createUtilityDicts(train_data_raw)\n",
    "\n",
    "val_text_dict, val_nerTag_dict, val_entityId_dict, val_wikiUrl_dict = createUtilityDicts(validation_data_raw)\n",
    "\n",
    "test_text_dict, test_nerTag_dict, test_entityId_dict, test_wikiUrl_dict = createUtilityDicts(test_data_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check some descriptive statistics on the sentence lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentenceLengths = []\n",
    "\n",
    "for sentences in train_text_dict.values():\n",
    "    i = 0\n",
    "    for token in sentences:\n",
    "        if token != '[SEP]':\n",
    "            i += 1\n",
    "        else:\n",
    "            sentenceLengths.append(i)\n",
    "            i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence mean 16.784703253041968\n",
      "max sentence length 171\n"
     ]
    }
   ],
   "source": [
    "print('sentence mean', np.mean(sentenceLengths))\n",
    "print('max sentence length', max(sentenceLengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.389e+03, 9.000e+02, 9.300e+02, 1.160e+03, 1.899e+03, 1.486e+03,\n",
       "        1.186e+03, 8.170e+02, 7.220e+02, 5.450e+02, 5.910e+02, 5.220e+02,\n",
       "        5.010e+02, 4.660e+02, 4.350e+02, 3.930e+02, 3.750e+02, 3.210e+02,\n",
       "        2.780e+02, 2.520e+02, 1.620e+02, 1.610e+02, 1.460e+02, 1.040e+02,\n",
       "        1.060e+02, 6.400e+01, 5.500e+01, 4.500e+01, 2.200e+01, 1.500e+01,\n",
       "        1.600e+01, 1.100e+01, 6.000e+00, 3.000e+00, 7.000e+00, 4.000e+00,\n",
       "        3.000e+00, 3.000e+00, 1.000e+00, 0.000e+00, 2.000e+00, 0.000e+00,\n",
       "        1.000e+00, 1.000e+00, 1.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "        0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "        0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "        0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "        0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "        0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "        0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "        1.000e+00]),\n",
       " array([  0.        ,   2.01176471,   4.02352941,   6.03529412,\n",
       "          8.04705882,  10.05882353,  12.07058824,  14.08235294,\n",
       "         16.09411765,  18.10588235,  20.11764706,  22.12941176,\n",
       "         24.14117647,  26.15294118,  28.16470588,  30.17647059,\n",
       "         32.18823529,  34.2       ,  36.21176471,  38.22352941,\n",
       "         40.23529412,  42.24705882,  44.25882353,  46.27058824,\n",
       "         48.28235294,  50.29411765,  52.30588235,  54.31764706,\n",
       "         56.32941176,  58.34117647,  60.35294118,  62.36470588,\n",
       "         64.37647059,  66.38823529,  68.4       ,  70.41176471,\n",
       "         72.42352941,  74.43529412,  76.44705882,  78.45882353,\n",
       "         80.47058824,  82.48235294,  84.49411765,  86.50588235,\n",
       "         88.51764706,  90.52941176,  92.54117647,  94.55294118,\n",
       "         96.56470588,  98.57647059, 100.58823529, 102.6       ,\n",
       "        104.61176471, 106.62352941, 108.63529412, 110.64705882,\n",
       "        112.65882353, 114.67058824, 116.68235294, 118.69411765,\n",
       "        120.70588235, 122.71764706, 124.72941176, 126.74117647,\n",
       "        128.75294118, 130.76470588, 132.77647059, 134.78823529,\n",
       "        136.8       , 138.81176471, 140.82352941, 142.83529412,\n",
       "        144.84705882, 146.85882353, 148.87058824, 150.88235294,\n",
       "        152.89411765, 154.90588235, 156.91764706, 158.92941176,\n",
       "        160.94117647, 162.95294118, 164.96470588, 166.97647059,\n",
       "        168.98823529, 171.        ]),\n",
       " <a list of 85 Patch objects>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(sentenceLengths, bins=85) #bins=170)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 - Generate input for the BERT model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because BERT has a limit of 512 tokens per input, I will try to split documents up into the component sentences, using the max sentence length of 50-75 for training. As can be seen by the histogram above, almost all sentences will fall into this range.\n",
    "\n",
    "Additionally, for each sentence we will need to add [CLS] and [PAD] tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will need **Sentences, NER Tokens,** and **entity ids**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createBertTextObjects(text_dict, nerTag_dict, entId_dict, max_sentence_length):\n",
    "    \"\"\"\n",
    "    Add [CLS] and [PAD] tokens to text, NER tag, and entity Id data \n",
    "    [PAD] tokens are added until max_sentence_length\n",
    "    returns bert_sentences, bert_ner_tags, bert_entities, bert_mask_ids\n",
    "    *Concepts borrowed from BERT_NER_v1\n",
    "    \"\"\"\n",
    "    max_sentence_length -= 1 # to account for Python's zero-based indexing\n",
    "    \n",
    "    bert_sentences = []\n",
    "    bert_ner_tags = []\n",
    "    bert_entities = []\n",
    "    bert_mask_ids = []\n",
    "    \n",
    "    # start with [CLS] token\n",
    "    sentence = ['[CLS]'] \n",
    "    ner_tags = ['[nerCLS]']\n",
    "    entity_ids = ['[entCLS]']\n",
    "    \n",
    "    for item in text_dict:\n",
    "    \n",
    "        for i in range(len(text_dict[item])):\n",
    "\n",
    "            if text_dict[item][i] == '[SEP]':\n",
    "                sentence_length = min(max_sentence_length, len(sentence))\n",
    "\n",
    "                # truncate sequences that are longer than the max_sentence_length\n",
    "                sentence = sentence[:max_sentence_length-1]\n",
    "                ner_tags = ner_tags[:max_sentence_length-1]\n",
    "                entity_ids = entity_ids[:max_sentence_length-1]\n",
    "\n",
    "                # mask all [PAD] items\n",
    "                bert_mask_ids.append([1] * (sentence_length + 1) + \n",
    "                                     [0] * (max_sentence_length - sentence_length))\n",
    "\n",
    "                # insert final [SEP] and [PAD] tokens\n",
    "                sentence += [text_dict[item][i]] + ['[PAD]'] * (max_sentence_length - len(sentence))\n",
    "                ner_tags += [nerTag_dict[item][i]] + ['[nerPAD]'] * (max_sentence_length - len(ner_tags))\n",
    "                entity_ids += [entId_dict[item][i]] + ['[entPAD]'] * (max_sentence_length - len(entity_ids))\n",
    "\n",
    "                bert_sentences.append(sentence)\n",
    "                bert_ner_tags.append(ner_tags)\n",
    "                bert_entities.append(entity_ids)\n",
    "\n",
    "                # restart with [CLS] token\n",
    "                sentence = ['[CLS]'] \n",
    "                ner_tags = ['[nerCLS]']\n",
    "                entity_ids = ['[entCLS]']\n",
    "            else:    \n",
    "                sentence += [text_dict[item][i]]\n",
    "                ner_tags += [nerTag_dict[item][i]]\n",
    "                entity_ids += [entId_dict[item][i]]\n",
    "            \n",
    "    bert_mask_ids = np.asarray(bert_mask_ids)\n",
    "            \n",
    "    return bert_sentences, bert_ner_tags, bert_entities, bert_mask_ids\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sentence_length = 50\n",
    "\n",
    "bert_sentences, bert_ner_tags, bert_entities, bert_mask_ids = createBertTextObjects(train_text_dict,\n",
    "                                                                       train_nerTag_dict,\n",
    "                                                                       train_entityId_dict,\n",
    "                                                                       max_sentence_length = max_sentence_length)\n",
    "\n",
    "val_sentences, val_ner_tags, val_entities, val_mask_ids = createBertTextObjects(val_text_dict,\n",
    "                                                                       val_nerTag_dict,\n",
    "                                                                       val_entityId_dict,\n",
    "                                                                       max_sentence_length = max_sentence_length)\n",
    "\n",
    "test_sentences, test_ner_tags, test_entities, test_mask_ids = createBertTextObjects(test_text_dict,\n",
    "                                                                       test_nerTag_dict,\n",
    "                                                                       test_entityId_dict,\n",
    "                                                                       max_sentence_length = max_sentence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our BERT inputs as text, we need to modify all inputs to be numerical values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approach 3 - codes for only relevant tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createBertNumObjects3(bert_sentences, bert_ner_tags):\n",
    "    \"\"\"\n",
    "    Converts BERT text objects created from the createBertTextObjects output into numerical arrays\n",
    "    \"\"\"\n",
    "    \n",
    "    bert_sentence_ids = []\n",
    "    \n",
    "    # convert sentence text to token ids using tokenizer\n",
    "    for sentence in bert_sentences:\n",
    "        bert_sentence_ids.append(tokenizer.convert_tokens_to_ids(sentence))\n",
    "        \n",
    "    bert_sentence_ids = np.asarray(bert_sentence_ids, dtype=np.int32)\n",
    "        \n",
    "    # convert NER tag text to binary 'is-entity' boolean\n",
    "    bert_nerTag_ids = []\n",
    "    for item in bert_ner_tags:\n",
    "        sentenceTags = []\n",
    "        for tag in item:\n",
    "            if tag == 'B': # i.e. tag corresponds with an entity\n",
    "                sentenceTags.append([1])\n",
    "            elif tag == 'I':\n",
    "                sentenceTags.append([2])\n",
    "            else:\n",
    "                sentenceTags.append([0])\n",
    "        bert_nerTag_ids.append(sentenceTags)\n",
    "    \n",
    "    bert_nerTag_ids = np.asarray(bert_nerTag_ids)\n",
    "    \n",
    "    # create a sequence id array (which will just be all zero values)\n",
    "    bert_seq_ids = []\n",
    "\n",
    "    for i in range(len(bert_sentence_ids)):\n",
    "        seq_list = ([0] * len(bert_sentence_ids[i]))\n",
    "        bert_seq_ids.append(seq_list)\n",
    "\n",
    "    bert_seq_ids = np.asarray(bert_seq_ids)\n",
    "    \n",
    "    return bert_sentence_ids, bert_nerTag_ids, bert_seq_ids\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bert_sentence_ids, bert_nerTag_ids, bert_seq_ids = createBertNumObjects3(bert_sentences, bert_ner_tags)\n",
    "\n",
    "val_sentence_ids, val_nerTag_ids, val_seq_ids = createBertNumObjects3(val_sentences, val_ner_tags)\n",
    "\n",
    "test_sentence_ids, test_nerTag_ids, test_seq_ids = createBertNumObjects3(test_sentences, test_ner_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16108, 50)\n",
      "(16108, 50, 1)\n",
      "(16108, 50)\n"
     ]
    }
   ],
   "source": [
    "print(bert_sentence_ids.shape)\n",
    "print(bert_nerTag_ids.shape) # extra dim of 1 for labels\n",
    "print(bert_seq_ids.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get rid of unnecessary documents to free up some memory (going to need it later!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_text_dict\n",
    "del train_nerTag_dict\n",
    "del train_entityId_dict\n",
    "del train_wikiUrl_dict \n",
    "\n",
    "del val_text_dict\n",
    "del val_nerTag_dict\n",
    "del val_entityId_dict\n",
    "del val_wikiUrl_dict \n",
    "\n",
    "del test_text_dict\n",
    "del test_nerTag_dict\n",
    "del test_entityId_dict\n",
    "del test_wikiUrl_dict "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the BERT model with custom accuracy function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.reset_default_graph()\n",
    "\n",
    "def initialize_vars(sess):\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.tables_initializer())\n",
    "    \n",
    "def clear_and_set_sess(sess):\n",
    "    K.clear_session()\n",
    "    K.set_session(sess)\n",
    "\n",
    "#sess = tf.Session()\n",
    "#graph = tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will use a custom accuracy function to show how accurate the model performs with regards to 'B' and 'I' tags only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_entity_accuracy(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate accuracy of predicting 'B' and 'I' lables only\n",
    "    \n",
    "    y_true: actual NER tag codes with the 'O' value (0) masked\n",
    "    y_pred: predicted NER tag codes with the 'O' value (0) masked \n",
    "    \n",
    "    returns accuracy measure\n",
    "    \"\"\"\n",
    "\n",
    "    #get labels and predictions\n",
    "    \n",
    "    y_label = tf.reshape(tf.layers.Flatten()(tf.cast(y_true, tf.int32)),[-1])\n",
    "    mask = (y_label > 0) # evaluates to True for all non 'O'/0 labels\n",
    "    y_label_masked = tf.boolean_mask(y_label, mask)  # mask the labels\n",
    "    \n",
    "    y_flat_pred = tf.reshape(tf.layers.Flatten()(tf.cast(y_pred, tf.float64)),[-1,3])\n",
    "    y_max_pred = tf.cast(tf.math.argmax(input = y_flat_pred, axis=1), tf.int32) # take the max predicted value\n",
    "    y_pred_masked = tf.boolean_mask(y_max_pred, mask) # mask the predictions as well\n",
    "    \n",
    "    return tf.reduce_mean(tf.cast(tf.equal(y_pred_masked, y_label_masked), dtype=tf.float64))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure this works the way we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x0000027D8E9B53C8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x0000027D8E9B53C8>>: AttributeError: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x0000027D8E9B53C8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x0000027D8E9B53C8>>: AttributeError: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x0000027D8E9B53C8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x0000027D8E9B53C8>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:From C:\\Users\\andre\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\andre\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x0000027D8E9B53C8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x0000027D8E9B53C8>>: AttributeError: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x0000027D8E9B53C8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x0000027D8E9B53C8>>: AttributeError: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x0000027D8E9B53C8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x0000027D8E9B53C8>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "y_true = tf.constant([1,0,1,0])\n",
    "y_pred = tf.constant([[0.9,0.05,0.05],[0.9,0.05,0.05],[0.05,0.9,0.05],[0.05,0.9,0.05]])\n",
    "\n",
    "print(is_entity_accuracy(y_true, y_pred).eval(session=sess))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that it is working, we can create the BERT layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code borrowed from: https://towardsdatascience.com/bert-in-keras-with-tensorflow-hub-76bcbc9417b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Create BERT layer, following https://towardsdatascience.com/bert-in-keras-with-tensorflow-hub-76bcbc9417b\n",
    "    init:  initialize layer. Specify various parameters regarding output types and dimensions. Very important is\n",
    "           to set the number of trainable layers.\n",
    "    build: build the layer based on parameters\n",
    "    call:  call the BERT layer within a model\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        n_fine_tune_layers=10,\n",
    "        pooling=\"sequence\",\n",
    "        bert_url = bert_url,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.n_fine_tune_layers = n_fine_tune_layers\n",
    "        self.trainable = True\n",
    "        self.output_size = 768\n",
    "        self.pooling = pooling\n",
    "        self.bert_url = bert_url\n",
    "\n",
    "        super(BertLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.bert = hub.Module(\n",
    "            self.bert_url, trainable=self.trainable, name=f\"{self.name}_module\"\n",
    "        )\n",
    "\n",
    "        # Remove unused layers\n",
    "        trainable_vars = self.bert.variables\n",
    "        trainable_vars = [\n",
    "                var\n",
    "                for var in trainable_vars\n",
    "                if not \"/cls/\" in var.name and not \"/pooler/\" in var.name\n",
    "            ]\n",
    "        trainable_layers = []\n",
    "\n",
    "\n",
    "        # Select how many layers to fine tune\n",
    "        for i in range(self.n_fine_tune_layers):\n",
    "            trainable_layers.append(f\"encoder/layer_{str(11 - i)}\")\n",
    "\n",
    "        # Update trainable vars to contain only the specified layers\n",
    "        trainable_vars = [\n",
    "            var\n",
    "            for var in trainable_vars\n",
    "            if any([l in var.name for l in trainable_layers])\n",
    "        ]\n",
    "\n",
    "        # Add to trainable weights\n",
    "        for var in trainable_vars:\n",
    "            self._trainable_weights.append(var)\n",
    "\n",
    "        for var in self.bert.variables:\n",
    "            if var not in self._trainable_weights:\n",
    "                self._non_trainable_weights.append(var)\n",
    "\n",
    "        super(BertLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        inputs = [K.cast(x, dtype=\"int32\") for x in inputs]\n",
    "        input_ids, input_mask, segment_ids = inputs\n",
    "        bert_inputs = dict(\n",
    "            input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids\n",
    "        )\n",
    "        result = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\n",
    "                \"sequence_output\"\n",
    "            ]\n",
    "\n",
    "        mul_mask = lambda x, m: x * tf.expand_dims(m, axis=-1)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary metrics for the Tensor board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variable_summaries(var):\n",
    "    \"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean', mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        tf.summary.scalar('stddev', stddev)\n",
    "        tf.summary.scalar('max', tf.reduce_max(var))\n",
    "        tf.summary.scalar('min', tf.reduce_min(var))\n",
    "        tf.summary.histogram('histogram', var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And a custom adam optimizer that can be adjusted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_adam = tf.keras.optimizers.Adam(lr=0.0005, beta_1=0.915, beta_2=0.999, epsilon=None, decay=0.05, amsgrad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.experimental.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method BertLayer.call of <__main__.BertLayer object at 0x0000027D920ADCC0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BertLayer.call of <__main__.BertLayer object at 0x0000027D920ADCC0>>: AttributeError: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method BertLayer.call of <__main__.BertLayer object at 0x0000027D920ADCC0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BertLayer.call of <__main__.BertLayer object at 0x0000027D920ADCC0>>: AttributeError: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method BertLayer.call of <__main__.BertLayer object at 0x0000027D920ADCC0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BertLayer.call of <__main__.BertLayer object at 0x0000027D920ADCC0>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\andre\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\andre\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x0000027EC567B7F0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x0000027EC567B7F0>>: AttributeError: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x0000027EC567B7F0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x0000027EC567B7F0>>: AttributeError: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x0000027EC567B7F0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x0000027EC567B7F0>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x0000027EC567B7F0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x0000027EC567B7F0>>: AttributeError: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x0000027EC567B7F0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x0000027EC567B7F0>>: AttributeError: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x0000027EC567B7F0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x0000027EC567B7F0>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "bert_token_ids (InputLayer)     [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert_input_masks (InputLayer)   [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert_seq_ids (InputLayer)       [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert_layer (BertLayer)          (None, None, 768)    108931396   bert_token_ids[0][0]             \n",
      "                                                                 bert_input_masks[0][0]           \n",
      "                                                                 bert_seq_ids[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 128)    98432       bert_layer[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, None, 128)    0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, None, 128)    16512       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, None, 128)    0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "entity_identification (Dense)   (None, None, 3)      387         dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 109,046,727\n",
      "Trainable params: 28,466,819\n",
      "Non-trainable params: 80,579,908\n",
      "__________________________________________________________________________________________________\n",
      "Train on 16108 samples, validate on 3827 samples\n",
      "Epoch 1/8\n",
      "16108/16108 [==============================] - 281s 17ms/sample - loss: 0.0275 - acc: 0.9911 - is_entity_accuracy: 0.9176 - val_loss: 0.0285 - val_acc: 0.9920 - val_is_entity_accuracy: 0.9475\n",
      "Epoch 2/8\n",
      "16108/16108 [==============================] - 304s 19ms/sample - loss: 0.0117 - acc: 0.9964 - is_entity_accuracy: 0.9692 - val_loss: 0.0291 - val_acc: 0.9925 - val_is_entity_accuracy: 0.9561\n",
      "Epoch 3/8\n",
      "16108/16108 [==============================] - 316s 20ms/sample - loss: 0.0082 - acc: 0.9976 - is_entity_accuracy: 0.9809 - val_loss: 0.0318 - val_acc: 0.9924 - val_is_entity_accuracy: 0.9552\n",
      "Epoch 4/8\n",
      "16108/16108 [==============================] - 314s 20ms/sample - loss: 0.0062 - acc: 0.9983 - is_entity_accuracy: 0.9861 - val_loss: 0.0331 - val_acc: 0.9926 - val_is_entity_accuracy: 0.9597\n",
      "Epoch 5/8\n",
      "16108/16108 [==============================] - 314s 20ms/sample - loss: 0.0048 - acc: 0.9987 - is_entity_accuracy: 0.9899 - val_loss: 0.0376 - val_acc: 0.9922 - val_is_entity_accuracy: 0.9584\n",
      "Epoch 6/8\n",
      "16108/16108 [==============================] - 314s 19ms/sample - loss: 0.0038 - acc: 0.9990 - is_entity_accuracy: 0.9926 - val_loss: 0.0399 - val_acc: 0.9922 - val_is_entity_accuracy: 0.9584\n",
      "Epoch 7/8\n",
      "16108/16108 [==============================] - 314s 19ms/sample - loss: 0.0031 - acc: 0.9992 - is_entity_accuracy: 0.9940 - val_loss: 0.0415 - val_acc: 0.9924 - val_is_entity_accuracy: 0.9600\n",
      "Epoch 8/8\n",
      "16108/16108 [==============================] - 315s 20ms/sample - loss: 0.0026 - acc: 0.9994 - is_entity_accuracy: 0.9951 - val_loss: 0.0421 - val_acc: 0.9924 - val_is_entity_accuracy: 0.9563\n"
     ]
    }
   ],
   "source": [
    "# Build model --> sentence inputs -> Bert layer -> Dense layer -> softmax is-entity prediction\n",
    "\n",
    "in_id = tf.keras.layers.Input(shape=(max_sentence_length,), name=\"bert_token_ids\")\n",
    "in_mask = tf.keras.layers.Input(shape=(max_sentence_length,), name=\"bert_input_masks\")\n",
    "in_segment = tf.keras.layers.Input(shape=(max_sentence_length,), name=\"bert_seq_ids\")\n",
    "\n",
    "bert_inputs = [in_id, in_mask, in_segment]\n",
    "\n",
    "# Instantiate the custom Bert Layer defined above\n",
    "bert_output = BertLayer(n_fine_tune_layers=4)(bert_inputs)\n",
    "\n",
    "# Build the rest of the classifier \n",
    "dense = tf.keras.layers.Dense(128, activation='relu', name='dense_1')(bert_output)\n",
    "dense = tf.keras.layers.Dropout(rate=0.3, name='dropout_1')(dense)\n",
    "dense = tf.keras.layers.Dense(128, activation='relu', name='dense_2')(dense)\n",
    "dense = tf.keras.layers.Dropout(rate=0.1, name='dropout_2')(dense)\n",
    "\n",
    "# prediction layer\n",
    "ner_pred = tf.keras.layers.Dense(3, activation='softmax', name='entity_identification')(dense)\n",
    "# take the maximum predicted value\n",
    "#pred = tf.math.argmax(input = tf.reshape(tf.keras.layers.Flatten()(tf.cast(pred, tf.float64)),[-1, 3]), axis=1)\n",
    "\n",
    "\n",
    "initialize_vars(sess)\n",
    "\n",
    "ner_model = tf.keras.models.Model(inputs=bert_inputs, outputs=ner_pred)\n",
    "#ner_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy',\n",
    "#                                                                                is_entity_accuracy])\n",
    "ner_model.compile(loss='sparse_categorical_crossentropy', optimizer=custom_adam, metrics=['accuracy',\n",
    "                                                                                is_entity_accuracy])\n",
    "ner_model.summary()\n",
    "\n",
    "\n",
    "with graph.as_default():\n",
    "    K.set_session(sess)\n",
    "    \n",
    "    ner_model.fit(\n",
    "        [bert_sentence_ids, bert_mask_ids, bert_seq_ids], \n",
    "        bert_nerTag_ids,\n",
    "        validation_data=([test_sentence_ids, test_mask_ids, test_seq_ids], \n",
    "                         test_nerTag_ids),\n",
    "        epochs=8,\n",
    "        batch_size=16\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_data_and_lables(sentence_ids, mask_ids, seq_ids, nerTag_ids):\n",
    "    if sentence_ids.shape[0] == mask_ids.shape[0] == seq_ids.shape[0] == nerTag_ids.shape[0]:\n",
    "        shuffle_index = np.random.permutation(sentence_ids.shape[0])\n",
    "        \n",
    "        sentence_ids_shuf = sentence_ids[shuffle_index]\n",
    "        mask_ids_shuf = mask_ids[shuffle_index]\n",
    "        seq_ids_shuf = seq_ids[shuffle_index]\n",
    "        nerTag_ids_shuf = nerTag_ids[shuffle_index]\n",
    "        \n",
    "        return sentence_ids_shuf, mask_ids_shuf, seq_ids_shuf, nerTag_ids_shuf\n",
    "        \n",
    "    else:\n",
    "        print('Incompatible shapes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_val_inputs = [val_sentence_ids, val_mask_ids, val_seq_ids]\n",
    "\n",
    "result = ner_model.predict(\n",
    "    bert_val_inputs, \n",
    "    batch_size=16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_entity_preds = []\n",
    "\n",
    "# for each sentence in result\n",
    "for i in range(len(result)):\n",
    "    # for each token in the sentence\n",
    "    sentence_preds = []\n",
    "    for j in range(len(result[i])):\n",
    "        # append the index of the max predicted value\n",
    "        index_value = np.where(result[i][j] == np.amax(result[i][j]))[0][0]\n",
    "        sentence_preds.append(index_value)\n",
    "    is_entity_preds.append(sentence_preds) \n",
    "    \n",
    "is_entity_preds = np.asarray(is_entity_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_test_inputs = [test_sentence_ids, test_mask_ids, test_seq_ids]\n",
    "\n",
    "test_result = ner_model.predict(\n",
    "    bert_test_inputs, \n",
    "    batch_size=16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_entity_preds_test = []\n",
    "\n",
    "# for each sentence in result\n",
    "for i in range(len(test_result)):\n",
    "    # for each token in the sentence\n",
    "    sentence_preds = []\n",
    "    for j in range(len(test_result[i])):\n",
    "        # append the index of the max predicted value\n",
    "        index_value = np.where(test_result[i][j] == np.amax(test_result[i][j]))[0][0]\n",
    "        sentence_preds.append(index_value)\n",
    "    is_entity_preds_test.append(sentence_preds) \n",
    "    \n",
    "is_entity_preds_test = np.asarray(is_entity_preds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('[CLS]', 0, array([0])),\n",
       " ('Japan', 1, array([1])),\n",
       " ('began', 0, array([0])),\n",
       " ('the', 0, array([0])),\n",
       " ('defence', 0, array([0])),\n",
       " ('of', 0, array([0])),\n",
       " ('their', 0, array([0])),\n",
       " ('Asian', 1, array([1])),\n",
       " ('Cup', 2, array([2])),\n",
       " ('title', 0, array([0])),\n",
       " ('with', 0, array([0])),\n",
       " ('a', 0, array([0])),\n",
       " ('lucky', 0, array([0])),\n",
       " ('2', 0, array([0])),\n",
       " ('-', 0, array([0])),\n",
       " ('1', 0, array([0])),\n",
       " ('win', 0, array([0])),\n",
       " ('against', 0, array([0])),\n",
       " ('Syria', 1, array([1])),\n",
       " ('in', 0, array([0])),\n",
       " ('a', 0, array([0])),\n",
       " ('Group', 0, array([0])),\n",
       " ('C', 0, array([0])),\n",
       " ('championship', 0, array([0])),\n",
       " ('match', 0, array([0])),\n",
       " ('on', 0, array([0])),\n",
       " ('Friday', 0, array([0])),\n",
       " ('.', 0, array([0])),\n",
       " ('[SEP]', 0, array([0])),\n",
       " ('[PAD]', 0, array([0])),\n",
       " ('[PAD]', 0, array([0])),\n",
       " ('[PAD]', 0, array([0])),\n",
       " ('[PAD]', 0, array([0])),\n",
       " ('[PAD]', 0, array([0])),\n",
       " ('[PAD]', 0, array([0])),\n",
       " ('[PAD]', 0, array([0])),\n",
       " ('[PAD]', 0, array([0])),\n",
       " ('[PAD]', 0, array([0])),\n",
       " ('[PAD]', 0, array([0])),\n",
       " ('[PAD]', 0, array([0])),\n",
       " ('[PAD]', 0, array([0])),\n",
       " ('[PAD]', 0, array([0])),\n",
       " ('[PAD]', 0, array([0])),\n",
       " ('[PAD]', 0, array([0])),\n",
       " ('[PAD]', 0, array([0])),\n",
       " ('[PAD]', 0, array([0])),\n",
       " ('[PAD]', 0, array([0])),\n",
       " ('[PAD]', 0, array([0])),\n",
       " ('[PAD]', 0, array([0])),\n",
       " ('[PAD]', 0, array([0]))]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "view_result = list(zip(test_sentences[3],is_entity_preds_test[3],test_nerTag_ids[3]))\n",
    "view_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output NER prediction arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save('C:\\\\Users\\\\andre\\\\Documents\\\\Berkeley\\\\w266\\\\w266_Project\\\\w266_Project\\\\data\\\\is_entity_preds.npy',\n",
    "#        is_entity_preds)\n",
    "#np.save('C:\\\\Users\\\\andre\\\\Documents\\\\Berkeley\\\\w266\\\\w266_Project\\\\w266_Project\\\\data\\\\is_entity_preds_test.npy',\n",
    "#        is_entity_preds_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And load when needed again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#is_entity_preds = np.load('C:\\\\Users\\\\andre\\\\Documents\\\\Berkeley\\\\w266\\\\w266_Project\\\\w266_Project\\\\data\\\\is_entity_preds.npy')\n",
    "#is_entity_preds_test = np.load('C:\\\\Users\\\\andre\\\\Documents\\\\Berkeley\\\\w266\\\\w266_Project\\\\w266_Project\\\\data\\\\is_entity_preds_test.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cheating..or is it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using wikipedia search functionality, we can greatly reduce the overhead of storing entity vectors and disambiguate entities from a list of candates!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the output from the NER model which identifies the entities, we can then re-assemble the raw text, input that into the wikipedia search, embed the summary for each of the retreived entities, and compute the similarity between the summaries and the mention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing stopwords from the summary improves accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Google Universal Sentence Encoder for mention / wikipedia summary embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below function extracts mentions and the text surrounding the mention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_entity_sequences(sequence_length, nerTag_ids, ner_sentences):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        sequence_length - how many tokens to the left and right of the identified entity\n",
    "        nerTag_ids - array of NER tag categories, i.e. 0 = 'O', 1 = 'B', 2 = 'I'\n",
    "        ner_sentences - list of tokenized sentences corresponding to NER tags \n",
    "        \n",
    "    Outputs: for each identified entity, output tokenized text surrounding the entity of length = sequence_length\n",
    "    \"\"\"\n",
    "\n",
    "    mention_sequences = [] # text sub-sequences for each identified mention\n",
    "    entity_indexes = [] # used to index the original sequence for each identified entity mention\n",
    "    entity_subseq_indexes = [] # used to index the sub-sequence for each identified mention\n",
    "    \n",
    "    # list which gives the entity number within the original sequence\n",
    "    # required for indexing sub-sequences with multiple entities\n",
    "    entity_number_list = []\n",
    "    entity_number = 0\n",
    "    entity_counter = 0\n",
    "    for i in range(len(nerTag_ids)):\n",
    "        if nerTag_ids[i] > [0] and entity_counter == 0:\n",
    "            entity_number += 1\n",
    "            entity_counter += 1\n",
    "            entity_number_list.append(entity_number)\n",
    "        elif nerTag_ids[i] > [0]:\n",
    "            entity_number_list.append(entity_number)\n",
    "        elif nerTag_ids[i] == [0] and entity_counter > 0:\n",
    "            entity_number_list.append(0)\n",
    "            entity_counter = 0\n",
    "        else:\n",
    "            entity_number_list.append(0)   \n",
    "    \n",
    "    zipped_nerTag_ids = list(zip(nerTag_ids, entity_number_list))\n",
    "    \n",
    "    entity_counter = 0 # reset entity counter\n",
    "    total_entities = 0\n",
    "    \n",
    "    for i in range(len(nerTag_ids)):\n",
    "        if nerTag_ids[i] == [0] and entity_counter > 0: # i.e. reached an 'other' tag following identified entities\n",
    "            ent_index_end = i # mark the ending index of the entity mention\n",
    "            \n",
    "            seq_start = max(0, i - entity_counter - sequence_length)\n",
    "            seq_end = min(i+sequence_length, len(nerTag_ids))\n",
    "\n",
    "            if seq_start == 0: # i.e. the seqence starts at beginning of sentence, so use extra tokens\n",
    "                extra_tokens = abs(i - entity_counter - sequence_length)\n",
    "                if seq_end < len(nerTag_ids):\n",
    "                    seq_end = min(seq_end + extra_tokens, len(nerTag_ids))\n",
    "\n",
    "            if seq_end == len(nerTag_ids): # i.e. the sequence ends at the end of the sentence, so use extra tokens\n",
    "                extra_tokens = sequence_length - (len(nerTag_ids) - i + 1)\n",
    "                if seq_start > 0:\n",
    "                    seq_start = max(seq_start - extra_tokens, 0)\n",
    "\n",
    "            total_entities += 1\n",
    "            \n",
    "            mention_sequences.append(ner_sentences[seq_start : seq_end])\n",
    "            entity_indexes.append([ent_index_start, ent_index_end])\n",
    "            \n",
    "            # append the sub-sequence indexing for the entity mention\n",
    "            entity_counter = 0 # reset entity token counter to use in sub-sequence\n",
    "            ent_subseq_idx_end = False\n",
    "            \n",
    "            for j in range(len(zipped_nerTag_ids[seq_start : seq_end])):\n",
    "                if zipped_nerTag_ids[seq_start : seq_end][j][1] == total_entities and entity_counter == 0:\n",
    "                    ent_subseq_idx_start = j\n",
    "                    ent_subseq_idx_end = True # activate variable for end-index value\n",
    "                    entity_counter += 1\n",
    "                        \n",
    "                if zipped_nerTag_ids[seq_start : seq_end][j][0] == [0] and ent_subseq_idx_end:\n",
    "                    ent_subseq_idx_end = j\n",
    "                    entity_subseq_indexes.append([ent_subseq_idx_start,ent_subseq_idx_end])\n",
    "                    entity_counter = 0 # reset entity token counter again for full sequence\n",
    "                    ent_subseq_idx_end = False # switch end-index variable back off\n",
    "\n",
    "\n",
    "            entity_counter = 0 # reset entity token counter again for full sequence\n",
    "\n",
    "        elif nerTag_ids[i] > [0]: # i.e. identified the start of an entity token sequence\n",
    "            if entity_counter == 0: \n",
    "                ent_index_start = i # mark the starting index of the entity mention\n",
    "            \n",
    "            entity_counter += 1\n",
    "            \n",
    "    return mention_sequences, entity_indexes, entity_subseq_indexes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mention_sequences_list = []\n",
    "entity_index_list = []\n",
    "entity_subseq_list = []\n",
    "\n",
    "for i in range(len(val_sentences)):\n",
    "    mention_sequences, ent_idx, ent_subseq_idx = create_entity_sequences(sequence_length = 5, \n",
    "                                                                     nerTag_ids = is_entity_preds[i], \n",
    "                                                                     ner_sentences = val_sentences[i])\n",
    "\n",
    "    mention_sequences_list.extend(mention_sequences)\n",
    "    entity_index_list.append(ent_idx)\n",
    "    entity_subseq_list.extend(ent_subseq_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialize_vars(sess)\n",
    "\n",
    "mention = mention_sequences_list[7][entity_subseq_list[7][0] : entity_subseq_list[7][1]]\n",
    "mention = ' '.join(mention).replace(' ##', '')\n",
    "\n",
    "mention_embedding = embed([mention]).eval(session=sess)\n",
    "\n",
    "entity_summaries = []\n",
    "candidates = wikipedia.search(mention,2)\n",
    "\n",
    "for item in candidates:\n",
    "    summary = wikipedia.summary(item)[:200]\n",
    "    tokenized_summary = word_tokenize(summary)\n",
    "    tokenized_summary = [token for token in tokenized_summary if token not in stopwords]\n",
    "    summary = TreebankWordDetokenizer().detokenize(tokenized_summary)\n",
    "    entity_summaries.append(summary)\n",
    "    \n",
    "entity_embeddings = embed(entity_summaries).eval(session=sess)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the mention and candidate entities embedded, we can compute the cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_entity_nn(mention_arr, entities_arr, entity_list, k=1):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        mention_arr - a vector embedding of mention text\n",
    "        entities_arr - an array of entity summary embeddings\n",
    "        entity_list - the list of text entities\n",
    "        k - how many nearest neighbors to output\n",
    "        \n",
    "    Returns:\n",
    "        a list of the entity and it's cosine similarity value to the mention embedding\n",
    "    \"\"\"\n",
    "    \n",
    "    cos_sim_list = []\n",
    "    for i in range(len(entities_arr)):\n",
    "        cos_sim = np.dot(mention_arr, entities_arr[i]) / (np.linalg.norm(mention_arr) * np.linalg.norm(entities_arr[i]))\n",
    "        cos_sim_list.append(cos_sim[0])\n",
    "        #cos_sim_list.append(cos_sim)\n",
    "\n",
    "    top_index = np.argsort(np.asarray(cos_sim_list))[-k:]\n",
    "\n",
    "    result = []\n",
    "    for item in top_index:\n",
    "        result.append((entity_list[item], cos_sim_list[item]))\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline measure 1 - Use entity mention text to link the entity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_1 = []\n",
    "    \n",
    "for sentence in val_sentences:\n",
    "    sentence_tokens = []\n",
    "    for token in sentence:\n",
    "        if token == '[CLS]':\n",
    "            sentence_tokens.append('[entCLS]')\n",
    "        elif token == '[SEP]':\n",
    "            sentence_tokens.append('[entSEP]')\n",
    "        elif token == '[PAD]':\n",
    "            sentence_tokens.append('[entPAD]')\n",
    "        else:\n",
    "            sentence_tokens.append(None)\n",
    "    baseline_1.append(sentence_tokens)\n",
    "    \n",
    "mention_sequences_list = []\n",
    "entity_index_list = []\n",
    "entity_subseq_list = []\n",
    "    \n",
    "for i in range(len(val_sentences)):\n",
    "    mention_sequences, ent_idx, ent_subseq_idx = create_entity_sequences(sequence_length = 3, \n",
    "                                                                     nerTag_ids = val_nerTag_ids[i], \n",
    "                                                                     ner_sentences = val_sentences[i])\n",
    "\n",
    "    mention_sequences_list.extend(mention_sequences)\n",
    "    entity_index_list.append(ent_idx)\n",
    "    entity_subseq_list.extend(ent_subseq_idx)\n",
    "\n",
    "mention_list = []\n",
    "    \n",
    "for i in range(len(mention_sequences_list)):\n",
    "    mention = mention_sequences_list[i][entity_subseq_list[i][0] : entity_subseq_list[i][1]]\n",
    "    mention = '_'.join(mention).replace('_##', '')\n",
    "    mention_list.append(mention)\n",
    "    \n",
    "candidate_index = 0\n",
    "\n",
    "for i in range(len(baseline_1)):\n",
    "    # use the original sentence indexes to replace values with the top candidate\n",
    "    for index in entity_index_list[i]:\n",
    "        # for each index, replace the output_entities_list value with the top candidate value\n",
    "        for k in range(index[0],index[1]):\n",
    "            baseline_1[i][k] = mention_list[candidate_index]\n",
    "        #print(round((i*100)/len(baseline_1),2),'% complete')            \n",
    "        # move to the next top candidate\n",
    "        candidate_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EL accuracy: 0.82326\n"
     ]
    }
   ],
   "source": [
    "evaluation_entities = val_entities\n",
    "\n",
    "accuracy = 0\n",
    "total = 0\n",
    "\n",
    "for i in range(len(baseline_1)):\n",
    "    for j in range(len(baseline_1[i])):\n",
    "        if baseline_1[i][j] not in ['[entCLS]','[entSEP]','[entPAD]']: # ignore CLS, SEP, and PAD tokens\n",
    "            if baseline_1[i][j] == evaluation_entities[i][j]:\n",
    "                accuracy += 1\n",
    "            total += 1\n",
    "        \n",
    "print('EL accuracy:', round(accuracy/total,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create a function that wraps this all up nicely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entity_linking(nerTag_ids, ner_sentences, mention_sequence_length = 5, wikipedia_summaries=2, wiki_chars=200): \n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        nerTag_ids - array of NER tag categories, i.e. 0 = 'O', 1 = 'B', 2 = 'I'\n",
    "        ner_sentences - list of tokenized sentences corresponding to NER tags \n",
    "        mention_sequence_length - int value for how many tokens to the left and right of the identified entity\n",
    "        wikipedia_summaries - an int for how many wikipedia entity summaries should be pulled\n",
    "        wiki_chars - number of characters to embed from the Wikipedia summaries\n",
    "        \n",
    "    Outputs:\n",
    "        List of entities to be compared with annotated entities\n",
    "    \"\"\"\n",
    "    # create placeholder entity list\n",
    "    output_entity_list = []\n",
    "    \n",
    "    for sentence in ner_sentences:\n",
    "        sentence_tokens = []\n",
    "        for token in sentence:\n",
    "            if token == '[CLS]':\n",
    "                sentence_tokens.append('[entCLS]')\n",
    "            elif token == '[SEP]':\n",
    "                sentence_tokens.append('[entSEP]')\n",
    "            elif token == '[PAD]':\n",
    "                sentence_tokens.append('[entPAD]')\n",
    "            else:\n",
    "                sentence_tokens.append(None)\n",
    "        output_entity_list.append(sentence_tokens)\n",
    "    \n",
    "    # Extract text sequences containing identified entities\n",
    "    mention_sequences_list = []\n",
    "    entity_index_list = []\n",
    "    entity_subseq_list = []\n",
    "\n",
    "    for i in range(len(ner_sentences)):\n",
    "        mention_sequences, ent_idx, ent_subseq_idx = create_entity_sequences(sequence_length = mention_sequence_length, \n",
    "                                                                         nerTag_ids = nerTag_ids[i], \n",
    "                                                                         ner_sentences = ner_sentences[i])\n",
    "\n",
    "        mention_sequences_list.extend(mention_sequences)\n",
    "        entity_index_list.append(ent_idx)\n",
    "        entity_subseq_list.extend(ent_subseq_idx)\n",
    "    \n",
    "    # For each identified entity, pull a list of possible candidates using wikipedia.search\n",
    "    top_candidates_list = []\n",
    "    \n",
    "    for i in range(len(mention_sequences_list)):\n",
    "        mention = mention_sequences_list[i][entity_subseq_list[i][0] : entity_subseq_list[i][1]]\n",
    "        mention = ' '.join(mention).replace(' ##', '')\n",
    "        mention_embedding = embed(mention_sequences_list[i]).eval(session=sess)\n",
    "        \n",
    "        candidates = wikipedia.search(mention, wikipedia_summaries) # how many to search given as input parameter\n",
    "\n",
    "        entity_summaries = []\n",
    "\n",
    "        for item in candidates:\n",
    "            try:\n",
    "                summary = wikipedia.summary(item)[:wiki_chars]\n",
    "\n",
    "            except:\n",
    "                summary = None\n",
    "\n",
    "            if summary:\n",
    "                tokenized_summary = word_tokenize(summary)\n",
    "                tokenized_summary = [token for token in tokenized_summary if token not in stopwords]\n",
    "                summary = TreebankWordDetokenizer().detokenize(tokenized_summary)\n",
    "                entity_summaries.append(summary)\n",
    "\n",
    "        if len(entity_summaries) > 0:\n",
    "            entity_embeddings = embed(entity_summaries).eval(session=sess)\n",
    "\n",
    "            top_candidate = find_entity_nn(mention_embedding, entity_embeddings, candidates, 1)[0][0]\n",
    "\n",
    "            top_candidates_list.append(top_candidate)\n",
    "            print(round((i*100)/len(mention_sequences_list),2),'% complete')\n",
    "\n",
    "        else:\n",
    "            top_candidates_list.append('--NME--')\n",
    "            print(round((i*100)/len(mention_sequences_list),2),'% complete')\n",
    "            \n",
    "    print('\\n\\n')\n",
    "    print('Top Candidate List Complete')\n",
    "            \n",
    "    # add top candidates to placeholder list\n",
    "    print('\\n\\n')\n",
    "    print('Beginning output_entity_list')\n",
    "    candidate_index = 0\n",
    "    for i in range(len(output_entity_list)):\n",
    "        # use the original sentence indexes to replace values with the top candidate\n",
    "        for index in entity_index_list[i]:\n",
    "            # for each index, replace the output_entities_list value with the top candidate value\n",
    "            for k in range(index[0],index[1]):\n",
    "                output_entity_list[i][k] = top_candidates_list[candidate_index]\n",
    "            print(round((i*100)/len(output_entity_list),2),'% complete')            \n",
    "            # move to the next top candidate\n",
    "            candidate_index += 1\n",
    "            \n",
    "    \n",
    "    return output_entity_list\n",
    "    \n",
    "                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning**\n",
    "\n",
    "This takes ~6-8 hours to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 % complete\n",
      "2.0 % complete\n",
      "4.0 % complete\n",
      "6.0 % complete\n",
      "8.0 % complete\n",
      "10.0 % complete\n",
      "12.0 % complete\n",
      "14.0 % complete\n",
      "16.0 % complete\n",
      "18.0 % complete\n",
      "20.0 % complete\n",
      "22.0 % complete\n",
      "24.0 % complete\n",
      "26.0 % complete\n",
      "28.0 % complete\n",
      "30.0 % complete\n",
      "32.0 % complete\n",
      "34.0 % complete\n",
      "36.0 % complete\n",
      "38.0 % complete\n",
      "40.0 % complete\n",
      "42.0 % complete\n",
      "44.0 % complete\n",
      "46.0 % complete\n",
      "48.0 % complete\n",
      "50.0 % complete\n",
      "52.0 % complete\n",
      "54.0 % complete\n",
      "56.0 % complete\n",
      "58.0 % complete\n",
      "60.0 % complete\n",
      "62.0 % complete\n",
      "64.0 % complete\n",
      "66.0 % complete\n",
      "68.0 % complete\n",
      "70.0 % complete\n",
      "72.0 % complete\n",
      "74.0 % complete\n",
      "76.0 % complete\n",
      "78.0 % complete\n",
      "80.0 % complete\n",
      "82.0 % complete\n",
      "84.0 % complete\n",
      "86.0 % complete\n",
      "88.0 % complete\n",
      "90.0 % complete\n",
      "92.0 % complete\n",
      "94.0 % complete\n",
      "96.0 % complete\n",
      "98.0 % complete\n",
      "\n",
      "\n",
      "\n",
      "Top Candidate List Complete\n",
      "\n",
      "\n",
      "\n",
      "Beginning output_entity_list\n",
      "2.0 % complete\n",
      "2.0 % complete\n",
      "2.0 % complete\n",
      "2.0 % complete\n",
      "2.0 % complete\n",
      "2.0 % complete\n",
      "4.0 % complete\n",
      "8.0 % complete\n",
      "10.0 % complete\n",
      "10.0 % complete\n",
      "10.0 % complete\n",
      "12.0 % complete\n",
      "12.0 % complete\n",
      "14.0 % complete\n",
      "16.0 % complete\n",
      "16.0 % complete\n",
      "18.0 % complete\n",
      "20.0 % complete\n",
      "20.0 % complete\n",
      "22.0 % complete\n",
      "28.0 % complete\n",
      "42.0 % complete\n",
      "44.0 % complete\n",
      "46.0 % complete\n",
      "48.0 % complete\n",
      "50.0 % complete\n",
      "52.0 % complete\n",
      "56.0 % complete\n",
      "58.0 % complete\n",
      "58.0 % complete\n",
      "68.0 % complete\n",
      "68.0 % complete\n",
      "70.0 % complete\n",
      "72.0 % complete\n",
      "72.0 % complete\n",
      "72.0 % complete\n",
      "78.0 % complete\n",
      "80.0 % complete\n",
      "82.0 % complete\n",
      "82.0 % complete\n",
      "84.0 % complete\n",
      "86.0 % complete\n",
      "86.0 % complete\n",
      "86.0 % complete\n",
      "92.0 % complete\n",
      "92.0 % complete\n",
      "94.0 % complete\n",
      "96.0 % complete\n",
      "98.0 % complete\n",
      "98.0 % complete\n"
     ]
    }
   ],
   "source": [
    "#initialize_vars(sess)\n",
    "\n",
    "output_entity_list = entity_linking(is_entity_preds_test, \n",
    "                                      test_sentences, \n",
    "                                      mention_sequence_length = 3, \n",
    "                                      wikipedia_summaries=2,\n",
    "                                      wiki_chars=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EL accuracy: 0.84875\n"
     ]
    }
   ],
   "source": [
    "evaluation_entities = test_entities\n",
    "\n",
    "accuracy = 0\n",
    "total = 0\n",
    "\n",
    "for i in range(len(output_entity_list)):\n",
    "    for j in range(len(output_entity_list[i])):\n",
    "        if output_entity_list[i][j] not in ['[entCLS]','[entSEP]','[entPAD]']: # ignore CLS, SEP, and PAD tokens\n",
    "            if output_entity_list[i][j] == evaluation_entities[i][j]:\n",
    "                accuracy += 1\n",
    "            total += 1\n",
    "        \n",
    "print('EL accuracy:', round(accuracy/total,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For the purposes of this project, code ends here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# appendix for entity embeddings in BERT/USE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4 - Create entity embeddings for Wikipedia entities\n",
    "\n",
    "### This will help the task of Candidate Generation for the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the wikiName_id_map_dict created earlier, scrape wikipedia articles for text descriptions of the entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wikipedia.WikipediaPage(title = 'Brussels').summary[:512])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_summary_ids_filepath = 'C:\\\\Users\\\\andre\\\\Documents\\\\Berkeley\\\\w266\\\\w266_Project\\\\w266_Project\\\\data\\\\entity_summary_ids.npy'\n",
    "entity_mask_ids_filepath = 'C:\\\\Users\\\\andre\\\\Documents\\\\Berkeley\\\\w266\\\\w266_Project\\\\w266_Project\\\\data\\\\entity_mask_ids.npy'\n",
    "entity_seq_ids_filepath = 'C:\\\\Users\\\\andre\\\\Documents\\\\Berkeley\\\\w266\\\\w266_Project\\\\w266_Project\\\\data\\\\entity_seq_ids.npy'\n",
    "output_entities_filepath = 'C:\\\\Users\\\\andre\\\\Documents\\\\Berkeley\\\\w266\\\\w266_Project\\\\w266_Project\\\\output_entities.data'\n",
    "entities_ext_filepath = 'C:\\\\Users\\\\andre\\\\Documents\\\\Berkeley\\\\w266\\\\w266_Project\\\\w266_Project\\\\entity_ext.data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For loading pre-created entity objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below cell can be run to open files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_summary_ids = np.load(entity_summary_ids_filepath)\n",
    "entity_mask_ids = np.load(entity_mask_ids_filepath)\n",
    "entity_seq_ids = np.load(entity_seq_ids_filepath)\n",
    "\n",
    "with open(output_entities_filepath, 'rb') as file_in:\n",
    "    output_entities = pickle.load(file_in)\n",
    "\n",
    "with open(entities_ext_filepath, 'rb') as file_in:\n",
    "    entities_ext = pickle.load(file_in)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_summary_ids.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Once run, skip to \"Embed the entities as vectors using Google Universal Sentence Encoder\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For loading entity and summaries the first time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#start_time = time.time()\n",
    "\n",
    "#wikiId_summaryTokens_dict = {}\n",
    "#i = 0\n",
    "#for wikiId in wikiName_id_map_dict:\n",
    "\n",
    "#    if i > 100:\n",
    "#        break # control\n",
    "#    i += 1\n",
    "#    try:\n",
    "#        summary = wikipedia.summary(wikiName_id_map_dict[wikiId])\n",
    "#    except:\n",
    "#        summary = None\n",
    "        \n",
    "#    if summary:\n",
    "#        dict_value = tokenizer.tokenize(summary)[:512] # max_length\n",
    "#        dict_value = tokenizer.convert_tokens_to_ids(dict_value)\n",
    "#        wikiId_summaryTokens_dict.update({wikiId: dict_value})\n",
    "\n",
    "#timing = time.time() - start_time\n",
    "#print('100 Wikipedia page summaries executed in',round(timing,2),'seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that the summary extraction takes so long, I'm going to cheat a bit to encode the entities as BERT 768 dimensional pooled vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = []\n",
    "\n",
    "for item in train_data_raw:\n",
    "    if len(item) > 1 and item[1][:7] != '--NME--': # skip -DOCSTART-, --NME--, and non-entities\n",
    "        for i in range(len(item)):\n",
    "            if item[i][:4] == 'http': # index of the url\n",
    "                stop_index = i\n",
    "                entities.extend(item[2:stop_index])\n",
    "        \n",
    "for item in validation_data_raw:\n",
    "    if len(item) > 1 and item[1][:7] != '--NME--': # skip -DOCSTART-, --NME--, and non-entities    \n",
    "        for i in range(len(item)):\n",
    "            if item[i][:4] == 'http': # index of the url\n",
    "                stop_index = i\n",
    "                entities.extend(item[2:stop_index])\n",
    "        \n",
    "for item in test_data_raw:\n",
    "    if len(item) > 1 and item[1][:7] != '--NME--': # skip -DOCSTART-, --NME--, and non-entities\n",
    "        for i in range(len(item)):\n",
    "            if item[i][:4] == 'http': # index of the url\n",
    "                stop_index = i\n",
    "                entities.extend(item[2:stop_index])\n",
    "        \n",
    "entities = list(set(entities)) # unique entities only\n",
    "\n",
    "# replace unicode characters\n",
    "for i in range(len(entities)):\n",
    "    if '\\\\u' in entities[i]:\n",
    "        entities[i] = entities[i].encode().decode('unicode_escape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's add some noise to our entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#entities_ext = []\n",
    "#start_time = time.time()\n",
    "#i = 1\n",
    "\n",
    "#for entity in entities:\n",
    "#    ent_start_time = time.time()\n",
    "#    print(entity, ' - ', str(round((i/len(entities)*100),3))+'%')\n",
    "#    entities_ext.extend(wikipedia.search(entity,results=5))\n",
    "#    timing = time.time() - ent_start_time\n",
    "#    print('\\t', round(timing,2),'seconds')\n",
    "#    i += 1\n",
    "\n",
    "#timing = time.time() - start_time\n",
    "\n",
    "#print('\\n\\n')\n",
    "#print('Created noise for',len(entities),'entities. Executed in', round((timing/60),2),'minutes \\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can tokenize the wiki summary for each entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createBertEntityObjects(entity_list, num_entities = 10, max_length=512):\n",
    "    \"\"\"\n",
    "    Given an list of entities, output numerical bert token ids, mask ids, and sequence ids\n",
    "    Last output is the text list of each entity that is contained in the vectors\n",
    "    Used for input into the bert entity vectors\n",
    "    Starts at 10 entities for output\n",
    "    \"\"\"\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    max_length -= 2 # account for CLS and SEP tokens\n",
    "    \n",
    "    entity_summary_ids = []\n",
    "    entity_mask_ids = []\n",
    "    entity_seq_ids = []\n",
    "    output_entities = []\n",
    "    \n",
    "    i = 1\n",
    "\n",
    "    for item in entity_list[:num_entities]:\n",
    "        # give run time information as this step can take a long time\n",
    "        item_start_time = time.time()\n",
    "        print(item, ' - ', str(round((i/num_entities*100),3))+'%')\n",
    "        try:\n",
    "            summary = tokenizer.tokenize(wikipedia.summary(item))[:max_length] \n",
    "        except:\n",
    "            summary = None\n",
    "\n",
    "        if summary:\n",
    "            num_pads = max_length - len(summary) \n",
    "            masks = ([1] * (len(summary)+2)) + ([0] * num_pads) # no need to account for CLS and SEP pads\n",
    "            summary = ['[CLS]'] + summary + ['[SEP]'] + (['[PAD]'] * num_pads)\n",
    "            summary = tokenizer.convert_tokens_to_ids(summary)\n",
    "\n",
    "            output_entities.append(item)\n",
    "            entity_summary_ids.append(np.asarray(summary))\n",
    "            entity_mask_ids.append(np.asarray(masks))\n",
    "            entity_seq_ids.append(np.asarray([1] * (max_length + 2))) # no need to account for CLS and SEP pads\n",
    "        \n",
    "        timing = time.time() - item_start_time\n",
    "        print('\\t', round(timing,2),'seconds')\n",
    "        i += 1\n",
    "\n",
    "    entity_summary_ids = np.asarray(entity_summary_ids)\n",
    "    entity_mask_ids = np.asarray(entity_mask_ids)\n",
    "    entity_seq_ids = np.asarray(entity_seq_ids)\n",
    "\n",
    "    timing = time.time() - start_time\n",
    "    print('\\n\\n')\n",
    "    \n",
    "    print(len(entity_summary_ids), 'Wikipedia page summaries executed in',round((timing/60),2),'minutes')\n",
    "    print('entity_summary_ids shape:', entity_summary_ids.shape)\n",
    "    print('entity_mask_ids shape:', entity_mask_ids.shape)\n",
    "    print('entity_seq_ids shape:', entity_seq_ids.shape)\n",
    "    print('Number of text entities:',len(output_entities))\n",
    "    \n",
    "    return entity_summary_ids, entity_mask_ids, entity_seq_ids, output_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(entities_ext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Running this with all extended entities takes ~22 hours**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#num_entities = len(entities_ext)\n",
    "#max_length = 300\n",
    "\n",
    "#entity_summary_ids, entity_mask_ids, entity_seq_ids, output_entities = createBertEntityObjects(entities_ext, \n",
    "#                                                                              num_entities = num_entities,\n",
    "#                                                                              max_length = max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now write the file out so that it can be easily used later on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Only needed when first creating entity objects**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output ndarray objects\n",
    "#entity_files = [entity_summary_ids_filepath, \n",
    "#                entity_mask_ids_filepath, \n",
    "#                entity_seq_ids_filepath]\n",
    "#\n",
    "#entity_data = [entity_summary_ids, \n",
    "#                entity_mask_ids, \n",
    "#                entity_seq_ids]\n",
    "#\n",
    "#for i in range(len(entity_files)):\n",
    "#    np.save(entity_files[i], entity_data[i])\n",
    "#    \n",
    "#\n",
    "# output list objects\n",
    "#entity_files = [output_entities_filepath,\n",
    "#                entities_ext_filepath]\n",
    "#\n",
    "#entity_data = [output_entities,\n",
    "#                entities_ext]\n",
    "#\n",
    "#for i in range(len(entity_files)):\n",
    "#    with open(entity_files[i], 'wb') as file_out:\n",
    "#        pickle.dump(entity_data[i], file_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note - BERT entity embeddings did not work as well as expected, therefore, the Google USE will be used to compute vector similarity between wikipedia summaries and the corresponding mention for the task of entity disambiguation and linking**\n",
    "\n",
    "**Unfortunately this means we have to un-tokenize the text..**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embed the entities as vectors using Google Universal Sentence Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-large/5\")\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_summaries = []\n",
    "for item in test_entity_summary_ids:\n",
    "    summary = ' '.join(tokenizer.convert_ids_to_tokens(item)).replace(' ##','')\n",
    "    entity_summaries.append(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_embeddings = embed(entity_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize_vars(sess)\n",
    "\n",
    "entity_embeddings_arr = entity_embeddings.eval(session=sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embed the mentions as vectors as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list(zip(val_sentences[2],val_ner_tags[2],val_nerTag_ids[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Phil Simmons** is the entity we need to match in this example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need a helper function to output a sequence around the entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_entity_sequences(sequence_length, nerTag_ids, ner_sentences):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        sequence_length - how many tokens to the left and right of the identified entity\n",
    "        nerTag_ids - array of NER tag categories, i.e. 0 = 'O', 1 = 'B', 2 = 'I'\n",
    "        ner_sentences - list of tokenized sentences corresponding to NER tags \n",
    "        \n",
    "    Outputs: for each identified entity, output tokenized text surrounding the entity of length = sequence_length\n",
    "    \"\"\"\n",
    "\n",
    "    mention_sequences = [] # text sub-sequences for each identified mention\n",
    "    entity_indexes = [] # used to index the original sequence for each identified entity mention\n",
    "    entity_subseq_indexes = [] # used to index the sub-sequence for each identified mention\n",
    "    \n",
    "    # list which gives the entity number within the original sequence\n",
    "    # required for indexing sub-sequences with multiple entities\n",
    "    entity_number_list = []\n",
    "    entity_number = 0\n",
    "    entity_counter = 0\n",
    "    for i in range(len(nerTag_ids)):\n",
    "        if nerTag_ids[i] > [0] and entity_counter == 0:\n",
    "            entity_number += 1\n",
    "            entity_counter += 1\n",
    "            entity_number_list.append(entity_number)\n",
    "        elif nerTag_ids[i] > [0]:\n",
    "            entity_number_list.append(entity_number)\n",
    "        elif nerTag_ids[i] == [0] and entity_counter > 0:\n",
    "            entity_number_list.append(0)\n",
    "            entity_counter = 0\n",
    "        else:\n",
    "            entity_number_list.append(0)   \n",
    "    \n",
    "    zipped_nerTag_ids = list(zip(nerTag_ids, entity_number_list))\n",
    "    \n",
    "    entity_counter = 0 # reset entity counter\n",
    "    total_entities = 0\n",
    "    \n",
    "    for i in range(len(nerTag_ids)):\n",
    "        if nerTag_ids[i] == [0] and entity_counter > 0: # i.e. reached an 'other' tag following identified entities\n",
    "            ent_index_end = i # mark the ending index of the entity mention\n",
    "            \n",
    "            seq_start = max(0, i - entity_counter - sequence_length)\n",
    "            seq_end = min(i+sequence_length, len(nerTag_ids))\n",
    "\n",
    "            if seq_start == 0: # i.e. the seqence starts at beginning of sentence, so use extra tokens\n",
    "                extra_tokens = abs(i - entity_counter - sequence_length)\n",
    "                if seq_end < len(nerTag_ids):\n",
    "                    seq_end = min(seq_end + extra_tokens, len(nerTag_ids))\n",
    "\n",
    "            if seq_end == len(nerTag_ids): # i.e. the sequence ends at the end of the sentence, so use extra tokens\n",
    "                extra_tokens = sequence_length - (len(nerTag_ids) - i + 1)\n",
    "                if seq_start > 0:\n",
    "                    seq_start = max(seq_start - extra_tokens, 0)\n",
    "\n",
    "            total_entities += 1\n",
    "            \n",
    "            mention_sequences.append(ner_sentences[seq_start : seq_end])\n",
    "            entity_indexes.append([ent_index_start, ent_index_end])\n",
    "            \n",
    "            # append the sub-sequence indexing for the entity mention\n",
    "            entity_counter = 0 # reset entity token counter to use in sub-sequence\n",
    "            ent_subseq_idx_end = False\n",
    "            \n",
    "            for j in range(len(zipped_nerTag_ids[seq_start : seq_end])):\n",
    "                if zipped_nerTag_ids[seq_start : seq_end][j][1] == total_entities and entity_counter == 0:\n",
    "                    ent_subseq_idx_start = j\n",
    "                    ent_subseq_idx_end = True # activate variable for end-index value\n",
    "                    entity_counter += 1\n",
    "                        \n",
    "                if zipped_nerTag_ids[seq_start : seq_end][j][0] == [0] and ent_subseq_idx_end:\n",
    "                    ent_subseq_idx_end = j\n",
    "                    entity_subseq_indexes.append([ent_subseq_idx_start,ent_subseq_idx_end])\n",
    "                    entity_counter = 0 # reset entity token counter again for full sequence\n",
    "                    ent_subseq_idx_end = False # switch end-index variable back off\n",
    "\n",
    "\n",
    "            entity_counter = 0 # reset entity token counter again for full sequence\n",
    "\n",
    "        elif nerTag_ids[i] > [0]: # i.e. identified the start of an entity token sequence\n",
    "            if entity_counter == 0: \n",
    "                ent_index_start = i # mark the starting index of the entity mention\n",
    "            \n",
    "            entity_counter += 1\n",
    "            \n",
    "    return mention_sequences, entity_indexes, entity_subseq_indexes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mention_sequences, ent_idx, ent_subseq_idx = create_entity_sequences(sequence_length = 5, \n",
    "                                                                     nerTag_ids = val_nerTag_ids[2], \n",
    "                                                                     ner_sentences = val_sentences[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[mention_sequences[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_subseq_idx[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mention_sequences) == len(ent_idx) == len(ent_subseq_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mention_text = []\n",
    "for item in [mention_sequences[1]]:\n",
    "    summary = ' '.join(item).replace(' ##','')\n",
    "    mention_text.append(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mention_embeddings = embed(mention_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mention_embeddings_arr = mention_embeddings.eval(session=sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute cosine similarity between wikipedia summary embeddings and sentence sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_entity_nn(mention_arr, entities_arr, entity_list, k=1):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        mention_arr - a vector embedding of mention text\n",
    "        entities_arr - an array of entity summary embeddings\n",
    "        entity_list - the list of text entities\n",
    "        k - how many nearest neighbors to output\n",
    "        \n",
    "    Returns:\n",
    "        a list of the entity and it's cosine similarity value to the mention embedding\n",
    "    \"\"\"\n",
    "    \n",
    "    cos_sim_list = []\n",
    "    for i in range(len(entities_arr)):\n",
    "        cos_sim = np.dot(mention_arr, entities_arr[i]) / (np.linalg.norm(mention_arr) * np.linalg.norm(entities_arr[i]))\n",
    "        cos_sim_list.append(cos_sim[0])\n",
    "        #cos_sim_list.append(cos_sim)\n",
    "\n",
    "    top_index = np.argsort(np.asarray(cos_sim_list))[-k:]\n",
    "\n",
    "    result = []\n",
    "    for item in top_index:\n",
    "        result.append((entity_list[item], cos_sim_list[item]))\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "find_entity_nn(mention_embeddings_arr, entity_embeddings_arr, test_output_entities, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia.summary('Lendl Simmons')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove english stopwords for increased accuracy?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords = set(stopwords.words('english'))\n",
    "# remove bert tokens as well\n",
    "#stopwords.add('[cls]') \n",
    "#stopwords.add('[sep]')\n",
    "#stopwords.add('[pad]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_summaries = []\n",
    "for item in test_entity_summary_ids:\n",
    "    tokenized_summary = tokenizer.convert_ids_to_tokens(item)\n",
    "    tokenized_summary = [token for token in tokenized_summary if token.lower() not in stopwords]\n",
    "    summary = ' '.join(tokenized_summary).replace(' ##','')\n",
    "    entity_summaries.append(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_embeddings = embed(entity_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_embeddings_arr = entity_embeddings.eval(session=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_entity_nn(mention_embeddings_arr, entity_embeddings_arr, test_output_entities, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting closer!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with a baseline..\n",
    "\n",
    "The simplest thing to do might be to just take the entity mention and slap it on the back of a wikipedia article. Let's try that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(entity_summaries)):\n",
    "    if entity_summaries[i] == 'London':\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's find the most similar wikipedia summary vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_summaries = []\n",
    "for item in entity_summary_ids:\n",
    "    tokenized_summary = tokenizer.convert_ids_to_tokens(item)\n",
    "    tokenized_summary = [token for token in tokenized_summary if token.lower() not in stopwords]\n",
    "    summary = ' '.join(tokenized_summary).replace(' ##','')\n",
    "    entity_summaries.append(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize_vars(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mention_sequences_list = []\n",
    "entity_index_list = []\n",
    "entity_subseq_list = []\n",
    "\n",
    "for i in range(len(val_sentences)):\n",
    "    mention_sequences, ent_idx, ent_subseq_idx = create_entity_sequences(sequence_length = 5, \n",
    "                                                                     nerTag_ids = val_nerTag_ids[i], \n",
    "                                                                     ner_sentences = val_sentences[i])\n",
    "    \n",
    "    mention_sequences_list.extend(mention_sequences)\n",
    "    entity_index_list.extend(ent_idx)\n",
    "    entity_subseq_list.extend(ent_subseq_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mention_text = []\n",
    "for item in mention_sequences_list:\n",
    "    summary = ' '.join(item).replace(' ##','')\n",
    "    mention_text.append(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mention_embeddings_arr = embed(mention_text[0:10]).eval(session=sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, the memory space required to store all entity summaries as a single array is too much. So we can split it up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_entity_nn1(mention_arr, entities_arr, entity_list, k=1):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        mention_arr - a vector embedding of mention text\n",
    "        entities_arr - an array of entity summary embeddings\n",
    "        entity_list - the list of text entities\n",
    "        k - how many nearest neighbors to output\n",
    "        \n",
    "    Returns:\n",
    "        a list of the entity and it's cosine similarity value to the mention embedding\n",
    "    \"\"\"\n",
    "    \n",
    "    cos_sim_list = []\n",
    "    for i in range(len(entities_arr)):\n",
    "        cos_sim = np.dot(mention_arr, entities_arr[i]) / (np.linalg.norm(mention_arr) * np.linalg.norm(entities_arr[i]))\n",
    "        #cos_sim_list.append(cos_sim[0])\n",
    "        cos_sim_list.append(cos_sim)\n",
    "\n",
    "    top_index = np.argsort(np.asarray(cos_sim_list))[-k:]\n",
    "\n",
    "    result = []\n",
    "    for item in top_index:\n",
    "        result.append((entity_list[item], cos_sim_list[item]))\n",
    "        \n",
    "    result = np.asarray(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_indexes = [item for item in range(0,57501,2500)]\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "mention_similarities = []\n",
    "\n",
    "for i in range(len(split_indexes)-1):\n",
    "    item_start_time = time.time()\n",
    "    \n",
    "    entity_embeddings = embed(entity_summaries[split_indexes[i] : split_indexes[i+1]])\n",
    "    output_entities_split = output_entities[split_indexes[i] : split_indexes[i+1]]\n",
    "    entity_embeddings_arr = entity_embeddings.eval(session=sess)\n",
    "    split_mention_similarities = []\n",
    "    \n",
    "    for vector in mention_embeddings_arr:\n",
    "        split_mention_similarities.append(find_entity_nn1(vector, \n",
    "                                                         entity_embeddings_arr, \n",
    "                                                         output_entities_split, 3)) # top 3 candidates\n",
    "        \n",
    "    mention_similarities.append(np.asarray(split_mention_similarities))\n",
    "    split_time = time.time()\n",
    "    timing = time.time() - item_start_time\n",
    "    print(round((100 * (i+1)) / len(split_indexes),2),'% complete. Executed in',round(timing,2),'seconds.')\n",
    "\n",
    "mention_similarities = np.asarray(mention_similarities)    \n",
    "timing = time.time() - start_time\n",
    "\n",
    "print('Completed in', round((timing/60),2),'minutes.')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 mentions - Completed in 3.91 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mention_text[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for item in mention_similarities:\n",
    "    print(item[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mention_text[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not as accurate for the work..which brings about the final section"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
